{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def test_loss(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out2 = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "        loss = []\n",
    "        for iter in range(out2.shape[1]):\n",
    "            loss.append(MSE(out2[:, iter], y_in[:, iter]))\n",
    "        return np.mean(loss)\n",
    "\n",
    "def pred_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out1_temp = np.matmul(weights1.T, x_in[:,:])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        return out2\n",
    "\n",
    "def acc_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "    pred = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "    sum_acc = 0\n",
    "    for iter in range(pred.shape[1]):\n",
    "#         print(iter)\n",
    "        sum_acc+=(np.argmax(pred[:, iter])==np.argmax(y_in[:, iter]))\n",
    "    return sum_acc/y_in.shape[1]\n",
    "\n",
    "def MSE(yHat, y):\n",
    "    return np.sum(np.multiply(yHat - y, yHat - y)) / y.size\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-5):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.mean(np.multiply(np.log(predictions),targets)) \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 10)\n"
     ]
    }
   ],
   "source": [
    "##Load and segregate the data\n",
    "letters = genfromtxt('energy_eff2.csv', delimiter=',')\n",
    "# np.random.shuffle(letters)\n",
    "print(letters.shape)\n",
    "x_in = letters[:, 0:8]\n",
    "x_mean = np.mean(x_in, axis = 0)\n",
    "x_std = np.std(x_in, axis = 0)\n",
    "y_in = letters[:, 8:10]\n",
    "y_mean = np.mean(y_in, axis=0)\n",
    "y_std = np.std(y_in, axis = 0)\n",
    "# y_in = (y_in-y_mean)/y_std\n",
    "x_in = (x_in-x_mean)/x_std\n",
    "# print(np.std(y_in, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_in[0:int(x_in.shape[0]*0.7), :]\n",
    "y_train = y_in[0:int(x_in.shape[0]*0.7), :]\n",
    "\n",
    "x_test = x_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]\n",
    "y_test = y_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "alpha = 0.9\n",
    "\n",
    "weights1 = np.random.normal(size=(x_in.shape[1], hidden_size))\n",
    "weights2 = np.random.normal(size=(hidden_size, y_in.shape[1]))\n",
    "print(weights1.shape)\n",
    "print(weights2.shape)\n",
    "biases1 = np.random.normal(size=(hidden_size, 1))\n",
    "biases2 = np.random.normal(size=(y_in.shape[1], 1))\n",
    "\n",
    "x_train = np.matrix(x_train)\n",
    "x_train = x_train.T\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = np.matrix(x_test)\n",
    "x_test = x_test.T\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  9.770793089895845\n",
      "train_loss :  10.17822563823167\n",
      "epoch_num :  1\n",
      "test_loss :  5.256961084877043\n",
      "train_loss :  4.962052897066543\n",
      "epoch_num :  6\n",
      "test_loss :  4.2755279459281725\n",
      "train_loss :  3.8290317733310486\n",
      "epoch_num :  11\n",
      "test_loss :  3.903810946304401\n",
      "train_loss :  3.486808228415732\n",
      "epoch_num :  16\n",
      "test_loss :  3.677259315827885\n",
      "train_loss :  3.2825917615065867\n",
      "epoch_num :  21\n",
      "test_loss :  3.54304634321656\n",
      "train_loss :  3.1258645696089755\n",
      "epoch_num :  26\n",
      "test_loss :  3.476512918124157\n",
      "train_loss :  2.9962379703347035\n",
      "epoch_num :  31\n",
      "test_loss :  3.4089351438650524\n",
      "train_loss :  2.8680299890198193\n",
      "epoch_num :  36\n",
      "test_loss :  3.3203260719086316\n",
      "train_loss :  2.746663053186962\n",
      "epoch_num :  41\n",
      "test_loss :  3.237021711853218\n",
      "train_loss :  2.656403222498233\n",
      "epoch_num :  46\n",
      "test_loss :  3.1651114567295897\n",
      "train_loss :  2.59525817643966\n",
      "epoch_num :  51\n",
      "test_loss :  3.1014590871594825\n",
      "train_loss :  2.552199773482904\n",
      "epoch_num :  56\n",
      "test_loss :  3.0415641359957677\n",
      "train_loss :  2.517805088040573\n",
      "epoch_num :  61\n",
      "test_loss :  2.9784778582358316\n",
      "train_loss :  2.4848471127268907\n",
      "epoch_num :  66\n",
      "test_loss :  2.905466452858163\n",
      "train_loss :  2.4494637885788255\n",
      "epoch_num :  71\n",
      "test_loss :  2.8425238665737873\n",
      "train_loss :  2.417776388991832\n",
      "epoch_num :  76\n",
      "test_loss :  2.8007938122875493\n",
      "train_loss :  2.3957403278219083\n",
      "epoch_num :  81\n",
      "test_loss :  2.7755318133122597\n",
      "train_loss :  2.3818670213669666\n",
      "epoch_num :  86\n",
      "test_loss :  2.7611710893895633\n",
      "train_loss :  2.3721102913279197\n",
      "epoch_num :  91\n",
      "test_loss :  2.7514603209730075\n",
      "train_loss :  2.3632272526940286\n",
      "epoch_num :  96\n",
      "test_loss :  2.7428340870660786\n",
      "train_loss :  2.3538445387505265\n",
      "epoch_num :  101\n",
      "test_loss :  2.7342772345843622\n",
      "train_loss :  2.3436738606437473\n",
      "epoch_num :  106\n",
      "test_loss :  2.7255222796850966\n",
      "train_loss :  2.332686584775526\n",
      "epoch_num :  111\n",
      "test_loss :  2.716170425145553\n",
      "train_loss :  2.320842171970288\n",
      "epoch_num :  116\n",
      "test_loss :  2.7051853654428983\n",
      "train_loss :  2.307607850592595\n",
      "epoch_num :  121\n",
      "test_loss :  2.689887796197796\n",
      "train_loss :  2.290557742178608\n",
      "epoch_num :  126\n",
      "test_loss :  2.6664113856646847\n",
      "train_loss :  2.2636536666934477\n",
      "epoch_num :  131\n",
      "test_loss :  2.6479958995830803\n",
      "train_loss :  2.2349558113248484\n",
      "epoch_num :  136\n",
      "test_loss :  2.644261664093811\n",
      "train_loss :  2.2194590872263644\n",
      "epoch_num :  141\n",
      "test_loss :  2.6425964471184673\n",
      "train_loss :  2.20943180432672\n",
      "epoch_num :  146\n",
      "test_loss :  2.6398375245025973\n",
      "train_loss :  2.200962979641538\n",
      "epoch_num :  151\n",
      "test_loss :  2.636197512506465\n",
      "train_loss :  2.1928707703841726\n",
      "epoch_num :  156\n",
      "test_loss :  2.631845801205979\n",
      "train_loss :  2.184723499720147\n",
      "epoch_num :  161\n",
      "test_loss :  2.6261986822673506\n",
      "train_loss :  2.1762082060355055\n",
      "epoch_num :  166\n",
      "test_loss :  2.618033027257029\n",
      "train_loss :  2.1670474784583633\n",
      "epoch_num :  171\n",
      "test_loss :  2.606417335732612\n",
      "train_loss :  2.157311957089414\n",
      "epoch_num :  176\n",
      "test_loss :  2.5899990867128064\n",
      "train_loss :  2.147161493127934\n",
      "epoch_num :  181\n",
      "test_loss :  2.559910834438284\n",
      "train_loss :  2.134209250452443\n",
      "epoch_num :  186\n",
      "test_loss :  2.487179674627129\n",
      "train_loss :  2.1044705281874148\n",
      "epoch_num :  191\n",
      "test_loss :  2.3608116495874367\n",
      "train_loss :  2.0473404268086033\n",
      "epoch_num :  196\n",
      "test_loss :  1.923665162814795\n",
      "train_loss :  1.7774444929214452\n",
      "epoch_num :  201\n",
      "test_loss :  1.6700610690690103\n",
      "train_loss :  1.6240514188602821\n",
      "epoch_num :  206\n",
      "test_loss :  1.5969052567613713\n",
      "train_loss :  1.5869452474949381\n",
      "epoch_num :  211\n",
      "test_loss :  1.5751876757947447\n",
      "train_loss :  1.5743761554081397\n",
      "epoch_num :  216\n",
      "test_loss :  1.571175665988273\n",
      "train_loss :  1.5702091587652918\n",
      "epoch_num :  221\n",
      "test_loss :  1.5739794386583046\n",
      "train_loss :  1.569964573604145\n",
      "epoch_num :  226\n",
      "test_loss :  1.5786810058578185\n",
      "train_loss :  1.571511981813862\n",
      "epoch_num :  231\n",
      "test_loss :  1.5827431053341592\n",
      "train_loss :  1.5732664692316138\n",
      "epoch_num :  236\n",
      "test_loss :  1.5851966228897953\n",
      "train_loss :  1.5742602727294446\n",
      "epoch_num :  241\n",
      "test_loss :  1.5858588812591987\n",
      "train_loss :  1.5741017459449669\n",
      "epoch_num :  246\n",
      "test_loss :  1.584734030723217\n",
      "train_loss :  1.5726833702644123\n",
      "epoch_num :  251\n",
      "test_loss :  1.5817601269751063\n",
      "train_loss :  1.5699216396235118\n",
      "epoch_num :  256\n",
      "test_loss :  1.5766296833001416\n",
      "train_loss :  1.5654893001548886\n",
      "epoch_num :  261\n",
      "test_loss :  1.5686009775483598\n",
      "train_loss :  1.55849716930974\n",
      "epoch_num :  266\n",
      "test_loss :  1.5582901258833086\n",
      "train_loss :  1.5489590716219788\n",
      "epoch_num :  271\n",
      "test_loss :  1.5496883717288252\n",
      "train_loss :  1.5408987860055963\n",
      "epoch_num :  276\n",
      "test_loss :  1.5418609313322091\n",
      "train_loss :  1.5350095027396002\n",
      "epoch_num :  281\n",
      "test_loss :  1.5338583537551318\n",
      "train_loss :  1.5299014849367616\n",
      "epoch_num :  286\n",
      "test_loss :  1.5261763175692287\n",
      "train_loss :  1.525282210085547\n",
      "epoch_num :  291\n",
      "test_loss :  1.5190559665902643\n",
      "train_loss :  1.52113712724699\n",
      "epoch_num :  296\n",
      "test_loss :  1.5125041097586553\n",
      "train_loss :  1.517344596460855\n",
      "epoch_num :  301\n",
      "test_loss :  1.5064626768538387\n",
      "train_loss :  1.5137288176555328\n",
      "epoch_num :  306\n",
      "test_loss :  1.5008640631504087\n",
      "train_loss :  1.5101537082612153\n",
      "epoch_num :  311\n",
      "test_loss :  1.4955868406308777\n",
      "train_loss :  1.5064967700026513\n",
      "epoch_num :  316\n",
      "test_loss :  1.4903515111491774\n",
      "train_loss :  1.5025160258111447\n",
      "epoch_num :  321\n",
      "test_loss :  1.4842222503262263\n",
      "train_loss :  1.4973229031752056\n",
      "epoch_num :  326\n",
      "test_loss :  1.4746553288088393\n",
      "train_loss :  1.4873415626989805\n",
      "epoch_num :  331\n",
      "test_loss :  1.4695599404021134\n",
      "train_loss :  1.4771564023744637\n",
      "epoch_num :  336\n",
      "test_loss :  1.4689605766273925\n",
      "train_loss :  1.4711031042049274\n",
      "epoch_num :  341\n",
      "test_loss :  1.4711186051131997\n",
      "train_loss :  1.4687902844677485\n",
      "epoch_num :  346\n",
      "test_loss :  1.4737931110577112\n",
      "train_loss :  1.4680460120193752\n",
      "epoch_num :  351\n",
      "test_loss :  1.4761979533369494\n",
      "train_loss :  1.4678653364694616\n",
      "epoch_num :  356\n",
      "test_loss :  1.4780324260604085\n",
      "train_loss :  1.4677368779508801\n",
      "epoch_num :  361\n",
      "test_loss :  1.4792597764396302\n",
      "train_loss :  1.4673866727666516\n",
      "epoch_num :  366\n",
      "test_loss :  1.479921313063331\n",
      "train_loss :  1.4666967618165527\n",
      "epoch_num :  371\n",
      "test_loss :  1.480073620930727\n",
      "train_loss :  1.4656499215364884\n",
      "epoch_num :  376\n",
      "test_loss :  1.479788337054311\n",
      "train_loss :  1.464281757178745\n",
      "epoch_num :  381\n",
      "test_loss :  1.4791481765649184\n",
      "train_loss :  1.4626476221458462\n",
      "epoch_num :  386\n",
      "test_loss :  1.4782353126597845\n",
      "train_loss :  1.4608047769079673\n",
      "epoch_num :  391\n",
      "test_loss :  1.4771222057825981\n",
      "train_loss :  1.4588045190206673\n",
      "epoch_num :  396\n",
      "test_loss :  1.4758677853616315\n",
      "train_loss :  1.4566896221774652\n",
      "epoch_num :  401\n",
      "test_loss :  1.4745175038640483\n",
      "train_loss :  1.454494346071554\n",
      "epoch_num :  406\n",
      "test_loss :  1.473105257751279\n",
      "train_loss :  1.4522455373629435\n",
      "epoch_num :  411\n",
      "test_loss :  1.471655826669245\n",
      "train_loss :  1.4499640329503805\n",
      "epoch_num :  416\n",
      "test_loss :  1.4701871481820648\n",
      "train_loss :  1.4476659813513675\n",
      "epoch_num :  421\n",
      "test_loss :  1.468712174626443\n",
      "train_loss :  1.4453639364879827\n",
      "epoch_num :  426\n",
      "test_loss :  1.4672402818263148\n",
      "train_loss :  1.4430677041012394\n",
      "epoch_num :  431\n",
      "test_loss :  1.4657782924468468\n",
      "train_loss :  1.4407849748136239\n",
      "epoch_num :  436\n",
      "test_loss :  1.4643312009968563\n",
      "train_loss :  1.4385217919438404\n",
      "epoch_num :  441\n",
      "test_loss :  1.4629026812146404\n",
      "train_loss :  1.4362828983612113\n",
      "epoch_num :  446\n",
      "test_loss :  1.4614954402627356\n",
      "train_loss :  1.434071997136056\n",
      "epoch_num :  451\n",
      "test_loss :  1.4601114672261726\n",
      "train_loss :  1.4318919511247865\n",
      "epoch_num :  456\n",
      "test_loss :  1.4587522093273688\n",
      "train_loss :  1.429744938893698\n",
      "epoch_num :  461\n",
      "test_loss :  1.4574186987151552\n",
      "train_loss :  1.4276325787997406\n",
      "epoch_num :  466\n",
      "test_loss :  1.4561116452266674\n",
      "train_loss :  1.4255560292467224\n",
      "epoch_num :  471\n",
      "test_loss :  1.4548315054379077\n",
      "train_loss :  1.423516070635093\n",
      "epoch_num :  476\n",
      "test_loss :  1.4535785349289407\n",
      "train_loss :  1.421513172899773\n",
      "epoch_num :  481\n",
      "test_loss :  1.452352828455136\n",
      "train_loss :  1.4195475514740294\n",
      "epoch_num :  486\n",
      "test_loss :  1.4511543512466532\n",
      "train_loss :  1.417619213817563\n",
      "epoch_num :  491\n",
      "test_loss :  1.449982963689174\n",
      "train_loss :  1.4157279981700515\n",
      "epoch_num :  496\n",
      "test_loss :  1.44883844099222\n",
      "train_loss :  1.4138736058538774\n",
      "epoch_num :  501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  1.4477204890134912\n",
      "train_loss :  1.4120556282013594\n",
      "epoch_num :  506\n",
      "test_loss :  1.4466287571046532\n",
      "train_loss :  1.4102735689915546\n",
      "epoch_num :  511\n",
      "test_loss :  1.4455628486298702\n",
      "train_loss :  1.4085268631316141\n",
      "epoch_num :  516\n",
      "test_loss :  1.4445223296535306\n",
      "train_loss :  1.4068148921964316\n",
      "epoch_num :  521\n",
      "test_loss :  1.443506736178506\n",
      "train_loss :  1.4051369973399768\n",
      "epoch_num :  526\n",
      "test_loss :  1.4425155802302423\n",
      "train_loss :  1.4034924900089367\n",
      "epoch_num :  531\n",
      "test_loss :  1.4415483550154093\n",
      "train_loss :  1.4018806608190637\n",
      "epoch_num :  536\n",
      "test_loss :  1.4406045393329934\n",
      "train_loss :  1.400300786896444\n",
      "epoch_num :  541\n",
      "test_loss :  1.4396836013756427\n",
      "train_loss :  1.398752137936099\n",
      "epoch_num :  546\n",
      "test_loss :  1.4387850020284207\n",
      "train_loss :  1.397233981189164\n",
      "epoch_num :  551\n",
      "test_loss :  1.4379081977477157\n",
      "train_loss :  1.3957455855546752\n",
      "epoch_num :  556\n",
      "test_loss :  1.4370526430846913\n",
      "train_loss :  1.3942862249230106\n",
      "epoch_num :  561\n",
      "test_loss :  1.4362177929031632\n",
      "train_loss :  1.392855180893286\n",
      "epoch_num :  566\n",
      "test_loss :  1.4354031043309823\n",
      "train_loss :  1.3914517449668569\n",
      "epoch_num :  571\n",
      "test_loss :  1.434608038475733\n",
      "train_loss :  1.390075220301795\n",
      "epoch_num :  576\n",
      "test_loss :  1.433832061929522\n",
      "train_loss :  1.3887249230993723\n",
      "epoch_num :  581\n",
      "test_loss :  1.4330746480829846\n",
      "train_loss :  1.3874001836816205\n",
      "epoch_num :  586\n",
      "test_loss :  1.4323352782656629\n",
      "train_loss :  1.386100347309585\n",
      "epoch_num :  591\n",
      "test_loss :  1.431613442727048\n",
      "train_loss :  1.3848247747835674\n",
      "epoch_num :  596\n",
      "test_loss :  1.4309086414716292\n",
      "train_loss :  1.3835728428604164\n",
      "epoch_num :  601\n",
      "test_loss :  1.4302203849593285\n",
      "train_loss :  1.3823439445169976\n",
      "epoch_num :  606\n",
      "test_loss :  1.429548194682355\n",
      "train_loss :  1.3811374890847299\n",
      "epoch_num :  611\n",
      "test_loss :  1.4288916036287138\n",
      "train_loss :  1.3799529022761792\n",
      "epoch_num :  616\n",
      "test_loss :  1.4282501566419048\n",
      "train_loss :  1.3787896261214547\n",
      "epoch_num :  621\n",
      "test_loss :  1.4276234106862484\n",
      "train_loss :  1.377647118829742\n",
      "epoch_num :  626\n",
      "test_loss :  1.427010935026543\n",
      "train_loss :  1.376524854588825\n",
      "epoch_num :  631\n",
      "test_loss :  1.4264123113305194\n",
      "train_loss :  1.3754223233137284\n",
      "epoch_num :  636\n",
      "test_loss :  1.4258271337022181\n",
      "train_loss :  1.3743390303541143\n",
      "epoch_num :  641\n",
      "test_loss :  1.4252550086538127\n",
      "train_loss :  1.3732744961684298\n",
      "epoch_num :  646\n",
      "test_loss :  1.424695555023232\n",
      "train_loss :  1.3722282559720032\n",
      "epoch_num :  651\n",
      "test_loss :  1.4241484038441492\n",
      "train_loss :  1.37119985936496\n",
      "epoch_num :  656\n",
      "test_loss :  1.4236131981749096\n",
      "train_loss :  1.3701888699453066\n",
      "epoch_num :  661\n",
      "test_loss :  1.4230895928921012\n",
      "train_loss :  1.3691948649114951\n",
      "epoch_num :  666\n",
      "test_loss :  1.4225772544542574\n",
      "train_loss :  1.3682174346583287\n",
      "epoch_num :  671\n",
      "test_loss :  1.4220758606406376\n",
      "train_loss :  1.3672561823694087\n",
      "epoch_num :  676\n",
      "test_loss :  1.4215851002697493\n",
      "train_loss :  1.3663107236090024\n",
      "epoch_num :  681\n",
      "test_loss :  1.4211046729015366\n",
      "train_loss :  1.365380685915477\n",
      "epoch_num :  686\n",
      "test_loss :  1.4206342885271392\n",
      "train_loss :  1.364465708398445\n",
      "epoch_num :  691\n",
      "test_loss :  1.4201736672494933\n",
      "train_loss :  1.3635654413411757\n",
      "epoch_num :  696\n",
      "test_loss :  1.4197225389576982\n",
      "train_loss :  1.3626795458096401\n",
      "epoch_num :  701\n",
      "test_loss :  1.4192806429978493\n",
      "train_loss :  1.361807693269333\n",
      "epoch_num :  706\n",
      "test_loss :  1.4188477278426428\n",
      "train_loss :  1.3609495652107657\n",
      "epoch_num :  711\n",
      "test_loss :  1.4184235507617309\n",
      "train_loss :  1.3601048527843467\n",
      "epoch_num :  716\n",
      "test_loss :  1.4180078774946243\n",
      "train_loss :  1.359273256445199\n",
      "epoch_num :  721\n",
      "test_loss :  1.4176004819276182\n",
      "train_loss :  1.3584544856083376\n",
      "epoch_num :  726\n",
      "test_loss :  1.4172011457760525\n",
      "train_loss :  1.3576482583145333\n",
      "epoch_num :  731\n",
      "test_loss :  1.4168096582729162\n",
      "train_loss :  1.3568543009069771\n",
      "epoch_num :  736\n",
      "test_loss :  1.4164258158648015\n",
      "train_loss :  1.3560723477189613\n",
      "epoch_num :  741\n",
      "test_loss :  1.4160494219157782\n",
      "train_loss :  1.355302140772483\n",
      "epoch_num :  746\n",
      "test_loss :  1.4156802864198963\n",
      "train_loss :  1.3545434294878256\n",
      "epoch_num :  751\n",
      "test_loss :  1.4153182257226677\n",
      "train_loss :  1.3537959704039648\n",
      "epoch_num :  756\n",
      "test_loss :  1.414963062251964\n",
      "train_loss :  1.3530595269097179\n",
      "epoch_num :  761\n",
      "test_loss :  1.4146146242584015\n",
      "train_loss :  1.352333868985392\n",
      "epoch_num :  766\n",
      "test_loss :  1.4142727455655144\n",
      "train_loss :  1.3516187729547875\n",
      "epoch_num :  771\n",
      "test_loss :  1.4139372653296245\n",
      "train_loss :  1.350914021247249\n",
      "epoch_num :  776\n",
      "test_loss :  1.413608027809509\n",
      "train_loss :  1.3502194021695921\n",
      "epoch_num :  781\n",
      "test_loss :  1.4132848821457538\n",
      "train_loss :  1.349534709687579\n",
      "epoch_num :  786\n",
      "test_loss :  1.4129676821496058\n",
      "train_loss :  1.3488597432166327\n",
      "epoch_num :  791\n",
      "test_loss :  1.4126562861013443\n",
      "train_loss :  1.348194307421607\n",
      "epoch_num :  796\n",
      "test_loss :  1.4123505565577001\n",
      "train_loss :  1.347538212025138\n",
      "epoch_num :  801\n",
      "test_loss :  1.4120503601683836\n",
      "train_loss :  1.3468912716244699\n",
      "epoch_num :  806\n",
      "test_loss :  1.411755567501296\n",
      "train_loss :  1.3462533055162913\n",
      "epoch_num :  811\n",
      "test_loss :  1.411466052876185\n",
      "train_loss :  1.3456241375293638\n",
      "epoch_num :  816\n",
      "test_loss :  1.411181694206468\n",
      "train_loss :  1.3450035958645883\n",
      "epoch_num :  821\n",
      "test_loss :  1.4109023728489924\n",
      "train_loss :  1.344391512942259\n",
      "epoch_num :  826\n",
      "test_loss :  1.410627973461316\n",
      "train_loss :  1.3437877252561634\n",
      "epoch_num :  831\n",
      "test_loss :  1.410358383866324\n",
      "train_loss :  1.3431920732342697\n",
      "epoch_num :  836\n",
      "test_loss :  1.4100934949237562\n",
      "train_loss :  1.342604401105678\n",
      "epoch_num :  841\n",
      "test_loss :  1.4098332004084715\n",
      "train_loss :  1.3420245567736104\n",
      "epoch_num :  846\n",
      "test_loss :  1.4095773968950058\n",
      "train_loss :  1.3414523916941303\n",
      "epoch_num :  851\n",
      "test_loss :  1.4093259836482617\n",
      "train_loss :  1.3408877607603176\n",
      "epoch_num :  856\n",
      "test_loss :  1.4090788625199462\n",
      "train_loss :  1.340330522191702\n",
      "epoch_num :  861\n",
      "test_loss :  1.408835937850423\n",
      "train_loss :  1.3397805374286125\n",
      "epoch_num :  866\n",
      "test_loss :  1.4085971163758881\n",
      "train_loss :  1.3392376710313125\n",
      "epoch_num :  871\n",
      "test_loss :  1.4083623071404132\n",
      "train_loss :  1.338701790583616\n",
      "epoch_num :  876\n",
      "test_loss :  1.4081314214126457\n",
      "train_loss :  1.3381727666007865\n",
      "epoch_num :  881\n",
      "test_loss :  1.4079043726069225\n",
      "train_loss :  1.337650472441474\n",
      "epoch_num :  886\n",
      "test_loss :  1.407681076208609\n",
      "train_loss :  1.3371347842235708\n",
      "epoch_num :  891\n",
      "test_loss :  1.4074614497033142\n",
      "train_loss :  1.3366255807437057\n",
      "epoch_num :  896\n",
      "test_loss :  1.407245412509824\n",
      "train_loss :  1.3361227434001837\n",
      "epoch_num :  901\n",
      "test_loss :  1.4070328859165606\n",
      "train_loss :  1.3356261561192952\n",
      "epoch_num :  906\n",
      "test_loss :  1.4068237930212804\n",
      "train_loss :  1.3351357052846782\n",
      "epoch_num :  911\n",
      "test_loss :  1.406618058673911\n",
      "train_loss :  1.334651279669708\n",
      "epoch_num :  916\n",
      "test_loss :  1.4064156094222615\n",
      "train_loss :  1.3341727703726507\n",
      "epoch_num :  921\n",
      "test_loss :  1.406216373460475\n",
      "train_loss :  1.333700070754495\n",
      "epoch_num :  926\n",
      "test_loss :  1.4060202805800364\n",
      "train_loss :  1.3332330763792948\n",
      "epoch_num :  931\n",
      "test_loss :  1.4058272621232035\n",
      "train_loss :  1.3327716849569258\n",
      "epoch_num :  936\n",
      "test_loss :  1.4056372509386545\n",
      "train_loss :  1.3323157962880365\n",
      "epoch_num :  941\n",
      "test_loss :  1.4054501813392701\n",
      "train_loss :  1.331865312211213\n",
      "epoch_num :  946\n",
      "test_loss :  1.4052659890619195\n",
      "train_loss :  1.3314201365521245\n",
      "epoch_num :  951\n",
      "test_loss :  1.405084611229026\n",
      "train_loss :  1.3309801750745687\n",
      "epoch_num :  956\n",
      "test_loss :  1.4049059863119369\n",
      "train_loss :  1.330545335433369\n",
      "epoch_num :  961\n",
      "test_loss :  1.4047300540958978\n",
      "train_loss :  1.3301155271289757\n",
      "epoch_num :  966\n",
      "test_loss :  1.4045567556465177\n",
      "train_loss :  1.3296906614636714\n",
      "epoch_num :  971\n",
      "test_loss :  1.4043860332777194\n",
      "train_loss :  1.3292706514993569\n",
      "epoch_num :  976\n",
      "test_loss :  1.4042178305209505\n",
      "train_loss :  1.328855412016757\n",
      "epoch_num :  981\n",
      "test_loss :  1.4040520920956805\n",
      "train_loss :  1.3284448594760248\n",
      "epoch_num :  986\n",
      "test_loss :  1.403888763881081\n",
      "train_loss :  1.32803891197867\n",
      "epoch_num :  991\n",
      "test_loss :  1.4037277928887586\n",
      "train_loss :  1.327637489230702\n",
      "epoch_num :  996\n"
     ]
    }
   ],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "biases1_momentum = 0\n",
    "biases2_momentum = 0\n",
    "weights1_momentum = 0\n",
    "weights2_momentum = 0\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "    if epoch_num%5==1:\n",
    "        print(\"test_loss : \", test_losses[-1])\n",
    "        print(\"train_loss : \", losses[-1])\n",
    "        print(\"epoch_num : \", epoch_num)\n",
    "    for sample_num in range(x_train.shape[1]):\n",
    "#         print(\"sample_num : \", sample_num)\n",
    "        #Forward propagation\n",
    "        out1_temp = np.matmul(weights1.T, x_train[:,sample_num])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        #Back-propogation\n",
    "        error_out = out2-y_train[:, sample_num]\n",
    "        err_2 = np.matrix(error_out)\n",
    "        grad_2 = np.matmul(out1, err_2.T)\n",
    "        err_1 = np.multiply(np.matmul(weights2, err_2), np.multiply(out1, np.ones(out1.shape)-out1))\n",
    "        grad_1 = np.multiply(x_train[:, sample_num], err_1.T)\n",
    "        #Updating the weights\n",
    "        \n",
    "        biases1_momentum = alpha*biases1_momentum-learning_rate*err_1\n",
    "        biases2_momentum = alpha*biases2_momentum-learning_rate*err_2\n",
    "        weights1_momentum = alpha*weights1_momentum-learning_rate*grad_1\n",
    "        weights2_momentum = alpha*weights2_momentum-learning_rate*grad_2\n",
    "        \n",
    "        biases1 = biases1 + biases1_momentum\n",
    "        biases2 = biases2 + biases2_momentum\n",
    "        weights1 = weights1 + weights1_momentum\n",
    "        weights2 = weights2 + weights2_momentum\n",
    "        #Updating the accuracy and loss\n",
    "        #End of for loop\n",
    "#     print(\"accuracy : \", np.mean(acc_arr))\n",
    "    test_losses.append(test_loss(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_test.append(acc_out(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_train.append(acc_out(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    losses.append(test_loss(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot(211)\n",
    "plt.plot(losses[1:], label = 'train_loss')\n",
    "plt.plot(test_losses[1:], label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.multiply(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test, pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26.19758036 12.33799571 10.37585935 27.16407459 28.73197313]\n",
      " [29.94478177 15.33841201 13.72729958 26.51809348 30.15112411]]\n",
      "[[26.48 12.16 10.35 29.71 29.47]\n",
      " [30.91 15.18 13.65 28.02 29.77]]\n"
     ]
    }
   ],
   "source": [
    "ind = 101\n",
    "print(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)[:,5:10])\n",
    "print(y_test[:,5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
