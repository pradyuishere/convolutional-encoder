{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def test_loss(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out2 = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "        loss = []\n",
    "        for iter in range(out2.shape[1]):\n",
    "            loss.append(MSE(out2[:, iter], y_in[:, iter]))\n",
    "        return np.mean(loss)\n",
    "\n",
    "def pred_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out1_temp = np.matmul(weights1.T, x_in[:,:])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        return out2\n",
    "\n",
    "def acc_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "    pred = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "    sum_acc = 0\n",
    "    for iter in range(pred.shape[1]):\n",
    "#         print(iter)\n",
    "        sum_acc+=(np.argmax(pred[:, iter])==np.argmax(y_in[:, iter]))\n",
    "    return sum_acc/y_in.shape[1]\n",
    "\n",
    "def MSE(yHat, y):\n",
    "    return np.sum(np.multiply(yHat - y, yHat - y)) / y.size\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-5):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.mean(np.multiply(np.log(predictions),targets)) \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 10)\n"
     ]
    }
   ],
   "source": [
    "##Load and segregate the data\n",
    "letters = genfromtxt('energy_eff2.csv', delimiter=',')\n",
    "# np.random.shuffle(letters)\n",
    "print(letters.shape)\n",
    "x_in = letters[:, 0:8]\n",
    "x_mean = np.mean(x_in, axis = 0)\n",
    "x_std = np.std(x_in, axis = 0)\n",
    "y_in = letters[:, 8:10]\n",
    "y_mean = np.mean(y_in, axis=0)\n",
    "y_std = np.std(y_in, axis = 0)\n",
    "# y_in = (y_in-y_mean)/y_std\n",
    "x_in = (x_in-x_mean)/x_std\n",
    "# print(np.std(y_in, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_in[0:int(x_in.shape[0]*0.7), :]\n",
    "y_train = y_in[0:int(x_in.shape[0]*0.7), :]\n",
    "\n",
    "x_test = x_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]\n",
    "y_test = y_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "alpha = 0.9\n",
    "\n",
    "weights1 = np.random.normal(size=(x_in.shape[1], hidden_size))\n",
    "weights2 = np.random.normal(size=(hidden_size, y_in.shape[1]))\n",
    "print(weights1.shape)\n",
    "print(weights2.shape)\n",
    "biases1 = np.random.normal(size=(hidden_size, 1))\n",
    "biases2 = np.random.normal(size=(y_in.shape[1], 1))\n",
    "\n",
    "x_train = np.matrix(x_train)\n",
    "x_train = x_train.T\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = np.matrix(x_test)\n",
    "x_test = x_test.T\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  11.297377199928311\n",
      "train_loss :  11.297377199928311\n",
      "epoch_num :  1\n",
      "test_loss :  6.445178612535452\n",
      "train_loss :  6.445178612535452\n",
      "epoch_num :  6\n",
      "test_loss :  4.4715884498219465\n",
      "train_loss :  4.4715884498219465\n",
      "epoch_num :  11\n",
      "test_loss :  3.4664869829147533\n",
      "train_loss :  3.4664869829147533\n",
      "epoch_num :  16\n",
      "test_loss :  3.1920531075390013\n",
      "train_loss :  3.1920531075390013\n",
      "epoch_num :  21\n",
      "test_loss :  2.9753561793072807\n",
      "train_loss :  2.9753561793072807\n",
      "epoch_num :  26\n",
      "test_loss :  2.7850057477258043\n",
      "train_loss :  2.7850057477258043\n",
      "epoch_num :  31\n",
      "test_loss :  2.5413881514864287\n",
      "train_loss :  2.5413881514864287\n",
      "epoch_num :  36\n",
      "test_loss :  2.288936285515035\n",
      "train_loss :  2.288936285515035\n",
      "epoch_num :  41\n",
      "test_loss :  2.134519294884335\n",
      "train_loss :  2.134519294884335\n",
      "epoch_num :  46\n",
      "test_loss :  2.0442463616565973\n",
      "train_loss :  2.0442463616565973\n",
      "epoch_num :  51\n",
      "test_loss :  1.9987680943639303\n",
      "train_loss :  1.9987680943639303\n",
      "epoch_num :  56\n",
      "test_loss :  1.9634179458207974\n",
      "train_loss :  1.9634179458207974\n",
      "epoch_num :  61\n",
      "test_loss :  1.9265593184325596\n",
      "train_loss :  1.9265593184325596\n",
      "epoch_num :  66\n",
      "test_loss :  1.89218251380702\n",
      "train_loss :  1.89218251380702\n",
      "epoch_num :  71\n",
      "test_loss :  1.8755264955871467\n",
      "train_loss :  1.8755264955871467\n",
      "epoch_num :  76\n",
      "test_loss :  1.8664733515904801\n",
      "train_loss :  1.8664733515904801\n",
      "epoch_num :  81\n",
      "test_loss :  1.8595852005989035\n",
      "train_loss :  1.8595852005989035\n",
      "epoch_num :  86\n",
      "test_loss :  1.8548066713707256\n",
      "train_loss :  1.8548066713707256\n",
      "epoch_num :  91\n",
      "test_loss :  1.852113772615684\n",
      "train_loss :  1.852113772615684\n",
      "epoch_num :  96\n",
      "test_loss :  1.8511779572579126\n",
      "train_loss :  1.8511779572579126\n",
      "epoch_num :  101\n",
      "test_loss :  1.851618714280469\n",
      "train_loss :  1.851618714280469\n",
      "epoch_num :  106\n",
      "test_loss :  1.8531064820694252\n",
      "train_loss :  1.8531064820694252\n",
      "epoch_num :  111\n",
      "test_loss :  1.8553661695448203\n",
      "train_loss :  1.8553661695448203\n",
      "epoch_num :  116\n",
      "test_loss :  1.8581429274957941\n",
      "train_loss :  1.8581429274957941\n",
      "epoch_num :  121\n",
      "test_loss :  1.8611466588294736\n",
      "train_loss :  1.8611466588294736\n",
      "epoch_num :  126\n",
      "test_loss :  1.8639691393980733\n",
      "train_loss :  1.8639691393980733\n",
      "epoch_num :  131\n",
      "test_loss :  1.8660017549784669\n",
      "train_loss :  1.8660017549784669\n",
      "epoch_num :  136\n",
      "test_loss :  1.8665102677780445\n",
      "train_loss :  1.8665102677780445\n",
      "epoch_num :  141\n",
      "test_loss :  1.8650361804975182\n",
      "train_loss :  1.8650361804975182\n",
      "epoch_num :  146\n",
      "test_loss :  1.8618535931768556\n",
      "train_loss :  1.8618535931768556\n",
      "epoch_num :  151\n",
      "test_loss :  1.8578157801912158\n",
      "train_loss :  1.8578157801912158\n",
      "epoch_num :  156\n",
      "test_loss :  1.8536301979085055\n",
      "train_loss :  1.8536301979085055\n",
      "epoch_num :  161\n",
      "test_loss :  1.8495017911806537\n",
      "train_loss :  1.8495017911806537\n",
      "epoch_num :  166\n",
      "test_loss :  1.8453437205531646\n",
      "train_loss :  1.8453437205531646\n",
      "epoch_num :  171\n",
      "test_loss :  1.841026561585789\n",
      "train_loss :  1.841026561585789\n",
      "epoch_num :  176\n",
      "test_loss :  1.8364734928573652\n",
      "train_loss :  1.8364734928573652\n",
      "epoch_num :  181\n",
      "test_loss :  1.8316785806865123\n",
      "train_loss :  1.8316785806865123\n",
      "epoch_num :  186\n",
      "test_loss :  1.8267042703165475\n",
      "train_loss :  1.8267042703165475\n",
      "epoch_num :  191\n",
      "test_loss :  1.8216664607426991\n",
      "train_loss :  1.8216664607426991\n",
      "epoch_num :  196\n",
      "test_loss :  1.8167066746380405\n",
      "train_loss :  1.8167066746380405\n",
      "epoch_num :  201\n",
      "test_loss :  1.8119594273251673\n",
      "train_loss :  1.8119594273251673\n",
      "epoch_num :  206\n",
      "test_loss :  1.8075275447721064\n",
      "train_loss :  1.8075275447721064\n",
      "epoch_num :  211\n",
      "test_loss :  1.8034723739534542\n",
      "train_loss :  1.8034723739534542\n",
      "epoch_num :  216\n",
      "test_loss :  1.7998168247657087\n",
      "train_loss :  1.7998168247657087\n",
      "epoch_num :  221\n",
      "test_loss :  1.796554892285991\n",
      "train_loss :  1.796554892285991\n",
      "epoch_num :  226\n",
      "test_loss :  1.7936621741382162\n",
      "train_loss :  1.7936621741382162\n",
      "epoch_num :  231\n",
      "test_loss :  1.7911045331001758\n",
      "train_loss :  1.7911045331001758\n",
      "epoch_num :  236\n",
      "test_loss :  1.7888441441242289\n",
      "train_loss :  1.7888441441242289\n",
      "epoch_num :  241\n",
      "test_loss :  1.7868432247667612\n",
      "train_loss :  1.7868432247667612\n",
      "epoch_num :  246\n",
      "test_loss :  1.7850660816550756\n",
      "train_loss :  1.7850660816550756\n",
      "epoch_num :  251\n",
      "test_loss :  1.7834800788646887\n",
      "train_loss :  1.7834800788646887\n",
      "epoch_num :  256\n",
      "test_loss :  1.7820559815227968\n",
      "train_loss :  1.7820559815227968\n",
      "epoch_num :  261\n",
      "test_loss :  1.7807679671618613\n",
      "train_loss :  1.7807679671618613\n",
      "epoch_num :  266\n",
      "test_loss :  1.779593473625186\n",
      "train_loss :  1.779593473625186\n",
      "epoch_num :  271\n",
      "test_loss :  1.7785129726690345\n",
      "train_loss :  1.7785129726690345\n",
      "epoch_num :  276\n",
      "test_loss :  1.777509713571848\n",
      "train_loss :  1.777509713571848\n",
      "epoch_num :  281\n",
      "test_loss :  1.776569458446804\n",
      "train_loss :  1.776569458446804\n",
      "epoch_num :  286\n",
      "test_loss :  1.7756802204167244\n",
      "train_loss :  1.7756802204167244\n",
      "epoch_num :  291\n",
      "test_loss :  1.7748320109945332\n",
      "train_loss :  1.7748320109945332\n",
      "epoch_num :  296\n",
      "test_loss :  1.7740166005997202\n",
      "train_loss :  1.7740166005997202\n",
      "epoch_num :  301\n",
      "test_loss :  1.7732272946693453\n",
      "train_loss :  1.7732272946693453\n",
      "epoch_num :  306\n",
      "test_loss :  1.7724587267455887\n",
      "train_loss :  1.7724587267455887\n",
      "epoch_num :  311\n",
      "test_loss :  1.7717066690779226\n",
      "train_loss :  1.7717066690779226\n",
      "epoch_num :  316\n",
      "test_loss :  1.770967860625197\n",
      "train_loss :  1.770967860625197\n",
      "epoch_num :  321\n",
      "test_loss :  1.770239851867704\n",
      "train_loss :  1.770239851867704\n",
      "epoch_num :  326\n",
      "test_loss :  1.7695208655272248\n",
      "train_loss :  1.7695208655272248\n",
      "epoch_num :  331\n",
      "test_loss :  1.768809672126124\n",
      "train_loss :  1.768809672126124\n",
      "epoch_num :  336\n",
      "test_loss :  1.7681054792718156\n",
      "train_loss :  1.7681054792718156\n",
      "epoch_num :  341\n",
      "test_loss :  1.7674078336063221\n",
      "train_loss :  1.7674078336063221\n",
      "epoch_num :  346\n",
      "test_loss :  1.7667165344844222\n",
      "train_loss :  1.7667165344844222\n",
      "epoch_num :  351\n",
      "test_loss :  1.766031558607769\n",
      "train_loss :  1.766031558607769\n",
      "epoch_num :  356\n",
      "test_loss :  1.7653529950158902\n",
      "train_loss :  1.7653529950158902\n",
      "epoch_num :  361\n",
      "test_loss :  1.7646809899880564\n",
      "train_loss :  1.7646809899880564\n",
      "epoch_num :  366\n",
      "test_loss :  1.7640157015171871\n",
      "train_loss :  1.7640157015171871\n",
      "epoch_num :  371\n",
      "test_loss :  1.763357263060299\n",
      "train_loss :  1.763357263060299\n",
      "epoch_num :  376\n",
      "test_loss :  1.762705756241885\n",
      "train_loss :  1.762705756241885\n",
      "epoch_num :  381\n",
      "test_loss :  1.7620611920902556\n",
      "train_loss :  1.7620611920902556\n",
      "epoch_num :  386\n",
      "test_loss :  1.7614235002371568\n",
      "train_loss :  1.7614235002371568\n",
      "epoch_num :  391\n",
      "test_loss :  1.7607925253310626\n",
      "train_loss :  1.7607925253310626\n",
      "epoch_num :  396\n",
      "test_loss :  1.7601680297321995\n",
      "train_loss :  1.7601680297321995\n",
      "epoch_num :  401\n",
      "test_loss :  1.759549701400293\n",
      "train_loss :  1.759549701400293\n",
      "epoch_num :  406\n",
      "test_loss :  1.7589371657782096\n",
      "train_loss :  1.7589371657782096\n",
      "epoch_num :  411\n",
      "test_loss :  1.7583300004313098\n",
      "train_loss :  1.7583300004313098\n",
      "epoch_num :  416\n",
      "test_loss :  1.7577277512293648\n",
      "train_loss :  1.7577277512293648\n",
      "epoch_num :  421\n",
      "test_loss :  1.7571299489522607\n",
      "train_loss :  1.7571299489522607\n",
      "epoch_num :  426\n",
      "test_loss :  1.756536125349692\n",
      "train_loss :  1.756536125349692\n",
      "epoch_num :  431\n",
      "test_loss :  1.7559458278732751\n",
      "train_loss :  1.7559458278732751\n",
      "epoch_num :  436\n",
      "test_loss :  1.7553586325062942\n",
      "train_loss :  1.7553586325062942\n",
      "epoch_num :  441\n",
      "test_loss :  1.7547741543243542\n",
      "train_loss :  1.7547741543243542\n",
      "epoch_num :  446\n",
      "test_loss :  1.7541920556131907\n",
      "train_loss :  1.7541920556131907\n",
      "epoch_num :  451\n",
      "test_loss :  1.753612051536873\n",
      "train_loss :  1.753612051536873\n",
      "epoch_num :  456\n",
      "test_loss :  1.7530339134830903\n",
      "train_loss :  1.7530339134830903\n",
      "epoch_num :  461\n",
      "test_loss :  1.752457470310229\n",
      "train_loss :  1.752457470310229\n",
      "epoch_num :  466\n",
      "test_loss :  1.7518826077842764\n",
      "train_loss :  1.7518826077842764\n",
      "epoch_num :  471\n",
      "test_loss :  1.7513092665258252\n",
      "train_loss :  1.7513092665258252\n",
      "epoch_num :  476\n",
      "test_loss :  1.7507374387941272\n",
      "train_loss :  1.7507374387941272\n",
      "epoch_num :  481\n",
      "test_loss :  1.7501671644217685\n",
      "train_loss :  1.7501671644217685\n",
      "epoch_num :  486\n",
      "test_loss :  1.7495985261865505\n",
      "train_loss :  1.7495985261865505\n",
      "epoch_num :  491\n",
      "test_loss :  1.7490316448709946\n",
      "train_loss :  1.7490316448709946\n",
      "epoch_num :  496\n",
      "test_loss :  1.7484666742200823\n",
      "train_loss :  1.7484666742200823\n",
      "epoch_num :  501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  1.7479037959668213\n",
      "train_loss :  1.7479037959668213\n",
      "epoch_num :  506\n",
      "test_loss :  1.7473432150563901\n",
      "train_loss :  1.7473432150563901\n",
      "epoch_num :  511\n",
      "test_loss :  1.7467851551640385\n",
      "train_loss :  1.7467851551640385\n",
      "epoch_num :  516\n",
      "test_loss :  1.7462298545709467\n",
      "train_loss :  1.7462298545709467\n",
      "epoch_num :  521\n",
      "test_loss :  1.7456775624360623\n",
      "train_loss :  1.7456775624360623\n",
      "epoch_num :  526\n",
      "test_loss :  1.7451285354806274\n",
      "train_loss :  1.7451285354806274\n",
      "epoch_num :  531\n",
      "test_loss :  1.7445830350853582\n",
      "train_loss :  1.7445830350853582\n",
      "epoch_num :  536\n",
      "test_loss :  1.7440413247874693\n",
      "train_loss :  1.7440413247874693\n",
      "epoch_num :  541\n",
      "test_loss :  1.743503668155667\n",
      "train_loss :  1.743503668155667\n",
      "epoch_num :  546\n",
      "test_loss :  1.742970327014874\n",
      "train_loss :  1.742970327014874\n",
      "epoch_num :  551\n",
      "test_loss :  1.7424415599888075\n",
      "train_loss :  1.7424415599888075\n",
      "epoch_num :  556\n",
      "test_loss :  1.7419176213262497\n",
      "train_loss :  1.7419176213262497\n",
      "epoch_num :  561\n",
      "test_loss :  1.7413987599769092\n",
      "train_loss :  1.7413987599769092\n",
      "epoch_num :  566\n",
      "test_loss :  1.7408852188825008\n",
      "train_loss :  1.7408852188825008\n",
      "epoch_num :  571\n",
      "test_loss :  1.7403772344508073\n",
      "train_loss :  1.7403772344508073\n",
      "epoch_num :  576\n",
      "test_loss :  1.7398750361816677\n",
      "train_loss :  1.7398750361816677\n",
      "epoch_num :  581\n",
      "test_loss :  1.7393788464160533\n",
      "train_loss :  1.7393788464160533\n",
      "epoch_num :  586\n",
      "test_loss :  1.7388888801817752\n",
      "train_loss :  1.7388888801817752\n",
      "epoch_num :  591\n",
      "test_loss :  1.7384053451112937\n",
      "train_loss :  1.7384053451112937\n",
      "epoch_num :  596\n",
      "test_loss :  1.7379284414093654\n",
      "train_loss :  1.7379284414093654\n",
      "epoch_num :  601\n",
      "test_loss :  1.7374583618504384\n",
      "train_loss :  1.7374583618504384\n",
      "epoch_num :  606\n",
      "test_loss :  1.7369952917872264\n",
      "train_loss :  1.7369952917872264\n",
      "epoch_num :  611\n",
      "test_loss :  1.7365394091540165\n",
      "train_loss :  1.7365394091540165\n",
      "epoch_num :  616\n",
      "test_loss :  1.7360908844493952\n",
      "train_loss :  1.7360908844493952\n",
      "epoch_num :  621\n",
      "test_loss :  1.7356498806847258\n",
      "train_loss :  1.7356498806847258\n",
      "epoch_num :  626\n",
      "test_loss :  1.7352165532856172\n",
      "train_loss :  1.7352165532856172\n",
      "epoch_num :  631\n",
      "test_loss :  1.7347910499347372\n",
      "train_loss :  1.7347910499347372\n",
      "epoch_num :  636\n",
      "test_loss :  1.734373510345242\n",
      "train_loss :  1.734373510345242\n",
      "epoch_num :  641\n",
      "test_loss :  1.7339640659547082\n",
      "train_loss :  1.7339640659547082\n",
      "epoch_num :  646\n",
      "test_loss :  1.7335628395301281\n",
      "train_loss :  1.7335628395301281\n",
      "epoch_num :  651\n",
      "test_loss :  1.7331699446751345\n",
      "train_loss :  1.7331699446751345\n",
      "epoch_num :  656\n",
      "test_loss :  1.732785485231217\n",
      "train_loss :  1.732785485231217\n",
      "epoch_num :  661\n",
      "test_loss :  1.7324095545650433\n",
      "train_loss :  1.7324095545650433\n",
      "epoch_num :  666\n",
      "test_loss :  1.7320422347348405\n",
      "train_loss :  1.7320422347348405\n",
      "epoch_num :  671\n",
      "test_loss :  1.7316835955293555\n",
      "train_loss :  1.7316835955293555\n",
      "epoch_num :  676\n",
      "test_loss :  1.731333693373635\n",
      "train_loss :  1.731333693373635\n",
      "epoch_num :  681\n",
      "test_loss :  1.7309925700971844\n",
      "train_loss :  1.7309925700971844\n",
      "epoch_num :  686\n",
      "test_loss :  1.73066025156105\n",
      "train_loss :  1.73066025156105\n",
      "epoch_num :  691\n",
      "test_loss :  1.7303367461421653\n",
      "train_loss :  1.7303367461421653\n",
      "epoch_num :  696\n",
      "test_loss :  1.7300220430753568\n",
      "train_loss :  1.7300220430753568\n",
      "epoch_num :  701\n",
      "test_loss :  1.7297161106556769\n",
      "train_loss :  1.7297161106556769\n",
      "epoch_num :  706\n",
      "test_loss :  1.72941889430717\n",
      "train_loss :  1.72941889430717\n",
      "epoch_num :  711\n",
      "test_loss :  1.7291303145273995\n",
      "train_loss :  1.7291303145273995\n",
      "epoch_num :  716\n",
      "test_loss :  1.7288502647217894\n",
      "train_loss :  1.7288502647217894\n",
      "epoch_num :  721\n",
      "test_loss :  1.728578608946902\n",
      "train_loss :  1.728578608946902\n",
      "epoch_num :  726\n",
      "test_loss :  1.728315179587278\n",
      "train_loss :  1.728315179587278\n",
      "epoch_num :  731\n",
      "test_loss :  1.7280597749976219\n",
      "train_loss :  1.7280597749976219\n",
      "epoch_num :  736\n",
      "test_loss :  1.7278121571492244\n",
      "train_loss :  1.7278121571492244\n",
      "epoch_num :  741\n",
      "test_loss :  1.727572049327259\n",
      "train_loss :  1.727572049327259\n",
      "epoch_num :  746\n",
      "test_loss :  1.7273391339345523\n",
      "train_loss :  1.7273391339345523\n",
      "epoch_num :  751\n",
      "test_loss :  1.727113050465556\n",
      "train_loss :  1.727113050465556\n",
      "epoch_num :  756\n",
      "test_loss :  1.7268933937230764\n",
      "train_loss :  1.7268933937230764\n",
      "epoch_num :  761\n",
      "test_loss :  1.7266797123582005\n",
      "train_loss :  1.7266797123582005\n",
      "epoch_num :  766\n",
      "test_loss :  1.7264715078206312\n",
      "train_loss :  1.7264715078206312\n",
      "epoch_num :  771\n",
      "test_loss :  1.7262682338121953\n",
      "train_loss :  1.7262682338121953\n",
      "epoch_num :  776\n",
      "test_loss :  1.7260692963388105\n",
      "train_loss :  1.7260692963388105\n",
      "epoch_num :  781\n",
      "test_loss :  1.7258740544563356\n",
      "train_loss :  1.7258740544563356\n",
      "epoch_num :  786\n",
      "test_loss :  1.7256818218017254\n",
      "train_loss :  1.7256818218017254\n",
      "epoch_num :  791\n",
      "test_loss :  1.7254918689927146\n",
      "train_loss :  1.7254918689927146\n",
      "epoch_num :  796\n",
      "test_loss :  1.7253034269662348\n",
      "train_loss :  1.7253034269662348\n",
      "epoch_num :  801\n",
      "test_loss :  1.7251156913075942\n",
      "train_loss :  1.7251156913075942\n",
      "epoch_num :  806\n",
      "test_loss :  1.7249278275990008\n",
      "train_loss :  1.7249278275990008\n",
      "epoch_num :  811\n",
      "test_loss :  1.7247389777880286\n",
      "train_loss :  1.7247389777880286\n",
      "epoch_num :  816\n",
      "test_loss :  1.7245482675442485\n",
      "train_loss :  1.7245482675442485\n",
      "epoch_num :  821\n",
      "test_loss :  1.7243548145367293\n",
      "train_loss :  1.7243548145367293\n",
      "epoch_num :  826\n",
      "test_loss :  1.724157737527997\n",
      "train_loss :  1.724157737527997\n",
      "epoch_num :  831\n",
      "test_loss :  1.7239561661429033\n",
      "train_loss :  1.7239561661429033\n",
      "epoch_num :  836\n",
      "test_loss :  1.7237492511351598\n",
      "train_loss :  1.7237492511351598\n",
      "epoch_num :  841\n",
      "test_loss :  1.7235361749429186\n",
      "train_loss :  1.7235361749429186\n",
      "epoch_num :  846\n",
      "test_loss :  1.7233161622986144\n",
      "train_loss :  1.7233161622986144\n",
      "epoch_num :  851\n",
      "test_loss :  1.7230884906395971\n",
      "train_loss :  1.7230884906395971\n",
      "epoch_num :  856\n",
      "test_loss :  1.7228525000565773\n",
      "train_loss :  1.7228525000565773\n",
      "epoch_num :  861\n",
      "test_loss :  1.7226076025165495\n",
      "train_loss :  1.7226076025165495\n",
      "epoch_num :  866\n",
      "test_loss :  1.7223532901077347\n",
      "train_loss :  1.7223532901077347\n",
      "epoch_num :  871\n",
      "test_loss :  1.7220891420746491\n",
      "train_loss :  1.7220891420746491\n",
      "epoch_num :  876\n",
      "test_loss :  1.721814830442469\n",
      "train_loss :  1.721814830442469\n",
      "epoch_num :  881\n",
      "test_loss :  1.7215301240693786\n",
      "train_loss :  1.7215301240693786\n",
      "epoch_num :  886\n",
      "test_loss :  1.7212348910126918\n",
      "train_loss :  1.7212348910126918\n",
      "epoch_num :  891\n",
      "test_loss :  1.7209290991469415\n",
      "train_loss :  1.7209290991469415\n",
      "epoch_num :  896\n",
      "test_loss :  1.7206128150272095\n",
      "train_loss :  1.7206128150272095\n",
      "epoch_num :  901\n",
      "test_loss :  1.7202862010469335\n",
      "train_loss :  1.7202862010469335\n",
      "epoch_num :  906\n",
      "test_loss :  1.7199495109925933\n",
      "train_loss :  1.7199495109925933\n",
      "epoch_num :  911\n",
      "test_loss :  1.7196030841462535\n",
      "train_loss :  1.7196030841462535\n",
      "epoch_num :  916\n",
      "test_loss :  1.7192473381282398\n",
      "train_loss :  1.7192473381282398\n",
      "epoch_num :  921\n",
      "test_loss :  1.718882760704457\n",
      "train_loss :  1.718882760704457\n",
      "epoch_num :  926\n",
      "test_loss :  1.7185099008044296\n",
      "train_loss :  1.7185099008044296\n",
      "epoch_num :  931\n",
      "test_loss :  1.7181293590072342\n",
      "train_loss :  1.7181293590072342\n",
      "epoch_num :  936\n",
      "test_loss :  1.7177417777519774\n",
      "train_loss :  1.7177417777519774\n",
      "epoch_num :  941\n",
      "test_loss :  1.7173478315191522\n",
      "train_loss :  1.7173478315191522\n",
      "epoch_num :  946\n",
      "test_loss :  1.716948217209778\n",
      "train_loss :  1.716948217209778\n",
      "epoch_num :  951\n",
      "test_loss :  1.7165436449228633\n",
      "train_loss :  1.7165436449228633\n",
      "epoch_num :  956\n",
      "test_loss :  1.7161348292997747\n",
      "train_loss :  1.7161348292997747\n",
      "epoch_num :  961\n",
      "test_loss :  1.7157224815697238\n",
      "train_loss :  1.7157224815697238\n",
      "epoch_num :  966\n",
      "test_loss :  1.7153073023947838\n",
      "train_loss :  1.7153073023947838\n",
      "epoch_num :  971\n",
      "test_loss :  1.7148899755780185\n",
      "train_loss :  1.7148899755780185\n",
      "epoch_num :  976\n",
      "test_loss :  1.7144711626660452\n",
      "train_loss :  1.7144711626660452\n",
      "epoch_num :  981\n",
      "test_loss :  1.7140514984480089\n",
      "train_loss :  1.7140514984480089\n",
      "epoch_num :  986\n",
      "test_loss :  1.7136315873287835\n",
      "train_loss :  1.7136315873287835\n",
      "epoch_num :  991\n",
      "test_loss :  1.7132120005335902\n",
      "train_loss :  1.7132120005335902\n",
      "epoch_num :  996\n"
     ]
    }
   ],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "biases1_momentum = 0\n",
    "biases2_momentum = 0\n",
    "weights1_momentum = 0\n",
    "weights2_momentum = 0\n",
    "\n",
    "weights1_main = weights1\n",
    "weights2_main = weights2\n",
    "biases1_main = biases1\n",
    "biases2_main = biases2\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "    if epoch_num%5==1:\n",
    "        print(\"test_loss : \", test_losses[-1])\n",
    "        print(\"train_loss : \", losses[-1])\n",
    "        print(\"epoch_num : \", epoch_num)\n",
    "    for sample_num in range(x_train.shape[1]):\n",
    "#         print(\"sample_num : \", sample_num)\n",
    "        #Forward propagation\n",
    "        out1_temp = np.matmul(weights1.T, x_train[:,sample_num])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        #Back-propogation\n",
    "        error_out = out2-y_train[:, sample_num]\n",
    "        err_2 = np.matrix(error_out)\n",
    "        grad_2 = np.matmul(out1, err_2.T)\n",
    "        err_1 = np.multiply(np.matmul(weights2, err_2), np.multiply(out1, np.ones(out1.shape)-out1))\n",
    "        grad_1 = np.multiply(x_train[:, sample_num], err_1.T)\n",
    "        #Updating the weights\n",
    "        \n",
    "        weights1 = weights1_main\n",
    "        weights2 = weights2_main\n",
    "        biases1 = biases1_main\n",
    "        biases2 = biases2_main\n",
    "        \n",
    "        biases1_momentum = alpha*biases1_momentum-learning_rate*err_1\n",
    "        biases2_momentum = alpha*biases2_momentum-learning_rate*err_2\n",
    "        weights1_momentum = alpha*weights1_momentum-learning_rate*grad_1\n",
    "        weights2_momentum = alpha*weights2_momentum-learning_rate*grad_2\n",
    "        \n",
    "        biases1_main = biases1_main + biases1_momentum\n",
    "        biases2_main = biases2_main + biases2_momentum\n",
    "        weights1_main = weights1_main + weights1_momentum\n",
    "        weights2_main = weights2_main + weights2_momentum\n",
    "        \n",
    "        biases1 = biases1 - alpha*biases1_momentum\n",
    "        biases2 = biases2 - alpha*biases2_momentum\n",
    "        weights1 = weights1 - alpha*weights1_momentum\n",
    "        weights2 = weights2 - alpha*weights2_momentum\n",
    "        #Updating the accuracy and loss\n",
    "        #End of for loop\n",
    "#     print(\"accuracy : \", np.mean(acc_arr))\n",
    "    test_losses.append(test_loss(x_test, y_test, weights1_main, weights2_main, biases1_main, biases2_main))\n",
    "#     accuracies_test.append(acc_out(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_train.append(acc_out(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    losses.append(test_loss(x_test, y_test, weights1_main, weights2_main, biases1_main, biases2_main))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot(211)\n",
    "plt.plot(losses[1:], label = 'train_loss')\n",
    "plt.plot(test_losses[1:], label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.multiply(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test, pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.29799157 19.78050094 38.77947015 18.98843546 23.69254541]\n",
      " [14.68892553 26.79134396 40.98893813 22.37333856 26.6203414 ]]\n",
      "[[11.7  19.95 38.57 19.36 24.29]\n",
      " [13.88 21.97 43.86 22.73 25.95]]\n"
     ]
    }
   ],
   "source": [
    "ind = 101\n",
    "print(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)[:,5:10])\n",
    "print(y_test[:,5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.712876926038536\n"
     ]
    }
   ],
   "source": [
    "print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
