{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def test_loss(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out2 = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "        loss = []\n",
    "        for iter in range(out2.shape[1]):\n",
    "            loss.append(MSE(out2[:, iter], y_in[:, iter]))\n",
    "        return np.mean(loss)\n",
    "\n",
    "def pred_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out1_temp = np.matmul(weights1.T, x_in[:,:])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        return out2\n",
    "\n",
    "def acc_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "    pred = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "    sum_acc = 0\n",
    "    for iter in range(pred.shape[1]):\n",
    "#         print(iter)\n",
    "        sum_acc+=(np.argmax(pred[:, iter])==np.argmax(y_in[:, iter]))\n",
    "    return sum_acc/y_in.shape[1]\n",
    "\n",
    "def MSE(yHat, y):\n",
    "    return np.sum(np.multiply(yHat - y, yHat - y)) / y.size\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-5):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.mean(np.multiply(np.log(predictions),targets)) \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 10)\n"
     ]
    }
   ],
   "source": [
    "##Load and segregate the data\n",
    "letters = genfromtxt('energy_eff2.csv', delimiter=',')\n",
    "# np.random.shuffle(letters)\n",
    "print(letters.shape)\n",
    "x_in = letters[:, 0:8]\n",
    "x_mean = np.mean(x_in, axis = 0)\n",
    "x_std = np.std(x_in, axis = 0)\n",
    "y_in = letters[:, 8:10]\n",
    "y_mean = np.mean(y_in, axis=0)\n",
    "y_std = np.std(y_in, axis = 0)\n",
    "# y_in = (y_in-y_mean)/y_std\n",
    "x_in = (x_in-x_mean)/x_std\n",
    "# print(np.std(y_in, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_in[0:int(x_in.shape[0]*0.7), :]\n",
    "y_train = y_in[0:int(x_in.shape[0]*0.7), :]\n",
    "\n",
    "x_test = x_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]\n",
    "y_test = y_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "alpha = 0.9\n",
    "\n",
    "weights1 = np.random.normal(size=(x_in.shape[1], hidden_size))\n",
    "weights2 = np.random.normal(size=(hidden_size, y_in.shape[1]))\n",
    "print(weights1.shape)\n",
    "print(weights2.shape)\n",
    "biases1 = np.random.normal(size=(hidden_size, 1))\n",
    "biases2 = np.random.normal(size=(y_in.shape[1], 1))\n",
    "\n",
    "x_train = np.matrix(x_train)\n",
    "x_train = x_train.T\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = np.matrix(x_test)\n",
    "x_test = x_test.T\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T\n",
    "alpha = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  9.265306916403842\n",
      "train_loss :  9.265306916403842\n",
      "epoch_num :  1\n",
      "test_loss :  5.178338364699458\n",
      "train_loss :  5.178338364699458\n",
      "epoch_num :  6\n",
      "test_loss :  3.969876689506447\n",
      "train_loss :  3.969876689506447\n",
      "epoch_num :  11\n",
      "test_loss :  3.3088979079611582\n",
      "train_loss :  3.3088979079611582\n",
      "epoch_num :  16\n",
      "test_loss :  2.9802172075134457\n",
      "train_loss :  2.9802172075134457\n",
      "epoch_num :  21\n",
      "test_loss :  2.8513232509595294\n",
      "train_loss :  2.8513232509595294\n",
      "epoch_num :  26\n",
      "test_loss :  2.7949726300019164\n",
      "train_loss :  2.7949726300019164\n",
      "epoch_num :  31\n",
      "test_loss :  2.628172927731942\n",
      "train_loss :  2.628172927731942\n",
      "epoch_num :  36\n",
      "test_loss :  2.3835166546375524\n",
      "train_loss :  2.3835166546375524\n",
      "epoch_num :  41\n",
      "test_loss :  2.272003173548736\n",
      "train_loss :  2.272003173548736\n",
      "epoch_num :  46\n",
      "test_loss :  2.216141685234246\n",
      "train_loss :  2.216141685234246\n",
      "epoch_num :  51\n",
      "test_loss :  2.184937740967844\n",
      "train_loss :  2.184937740967844\n",
      "epoch_num :  56\n",
      "test_loss :  2.165483401689426\n",
      "train_loss :  2.165483401689426\n",
      "epoch_num :  61\n",
      "test_loss :  2.151963360012488\n",
      "train_loss :  2.151963360012488\n",
      "epoch_num :  66\n",
      "test_loss :  2.138830182619504\n",
      "train_loss :  2.138830182619504\n",
      "epoch_num :  71\n",
      "test_loss :  2.1222819301092533\n",
      "train_loss :  2.1222819301092533\n",
      "epoch_num :  76\n",
      "test_loss :  2.100385171472323\n",
      "train_loss :  2.100385171472323\n",
      "epoch_num :  81\n",
      "test_loss :  2.0701505857587064\n",
      "train_loss :  2.0701505857587064\n",
      "epoch_num :  86\n",
      "test_loss :  2.0349627359105145\n",
      "train_loss :  2.0349627359105145\n",
      "epoch_num :  91\n",
      "test_loss :  1.987143462573547\n",
      "train_loss :  1.987143462573547\n",
      "epoch_num :  96\n",
      "test_loss :  1.9389600494758596\n",
      "train_loss :  1.9389600494758596\n",
      "epoch_num :  101\n",
      "test_loss :  1.899589233476638\n",
      "train_loss :  1.899589233476638\n",
      "epoch_num :  106\n",
      "test_loss :  1.793274364533545\n",
      "train_loss :  1.793274364533545\n",
      "epoch_num :  111\n",
      "test_loss :  1.6583158581179167\n",
      "train_loss :  1.6583158581179167\n",
      "epoch_num :  116\n",
      "test_loss :  1.5702491835093968\n",
      "train_loss :  1.5702491835093968\n",
      "epoch_num :  121\n",
      "test_loss :  1.512841326193521\n",
      "train_loss :  1.512841326193521\n",
      "epoch_num :  126\n",
      "test_loss :  1.4763813454617343\n",
      "train_loss :  1.4763813454617343\n",
      "epoch_num :  131\n",
      "test_loss :  1.4539582960797695\n",
      "train_loss :  1.4539582960797695\n",
      "epoch_num :  136\n",
      "test_loss :  1.4407187268007622\n",
      "train_loss :  1.4407187268007622\n",
      "epoch_num :  141\n",
      "test_loss :  1.4333000453071214\n",
      "train_loss :  1.4333000453071214\n",
      "epoch_num :  146\n",
      "test_loss :  1.4294220180181774\n",
      "train_loss :  1.4294220180181774\n",
      "epoch_num :  151\n",
      "test_loss :  1.4275587761854711\n",
      "train_loss :  1.4275587761854711\n",
      "epoch_num :  156\n",
      "test_loss :  1.4267283665023591\n",
      "train_loss :  1.4267283665023591\n",
      "epoch_num :  161\n",
      "test_loss :  1.4263474730308026\n",
      "train_loss :  1.4263474730308026\n",
      "epoch_num :  166\n",
      "test_loss :  1.4261073388010141\n",
      "train_loss :  1.4261073388010141\n",
      "epoch_num :  171\n",
      "test_loss :  1.425870984636071\n",
      "train_loss :  1.425870984636071\n",
      "epoch_num :  176\n",
      "test_loss :  1.4255995322215322\n",
      "train_loss :  1.4255995322215322\n",
      "epoch_num :  181\n",
      "test_loss :  1.4253048267557706\n",
      "train_loss :  1.4253048267557706\n",
      "epoch_num :  186\n",
      "test_loss :  1.425019788252906\n",
      "train_loss :  1.425019788252906\n",
      "epoch_num :  191\n",
      "test_loss :  1.42477930199906\n",
      "train_loss :  1.42477930199906\n",
      "epoch_num :  196\n",
      "test_loss :  1.4246082156351554\n",
      "train_loss :  1.4246082156351554\n",
      "epoch_num :  201\n",
      "test_loss :  1.42451562645785\n",
      "train_loss :  1.42451562645785\n",
      "epoch_num :  206\n",
      "test_loss :  1.4244949192041068\n",
      "train_loss :  1.4244949192041068\n",
      "epoch_num :  211\n",
      "test_loss :  1.424527805459388\n",
      "train_loss :  1.424527805459388\n",
      "epoch_num :  216\n",
      "test_loss :  1.4245897244262973\n",
      "train_loss :  1.4245897244262973\n",
      "epoch_num :  221\n",
      "test_loss :  1.4246543486492507\n",
      "train_loss :  1.4246543486492507\n",
      "epoch_num :  226\n",
      "test_loss :  1.4246961248861345\n",
      "train_loss :  1.4246961248861345\n",
      "epoch_num :  231\n",
      "test_loss :  1.424690852718945\n",
      "train_loss :  1.424690852718945\n",
      "epoch_num :  236\n",
      "test_loss :  1.4246148244255927\n",
      "train_loss :  1.4246148244255927\n",
      "epoch_num :  241\n",
      "test_loss :  1.4244431036649243\n",
      "train_loss :  1.4244431036649243\n",
      "epoch_num :  246\n",
      "test_loss :  1.4241473717512478\n",
      "train_loss :  1.4241473717512478\n",
      "epoch_num :  251\n",
      "test_loss :  1.4236936462304104\n",
      "train_loss :  1.4236936462304104\n",
      "epoch_num :  256\n",
      "test_loss :  1.4230402477444448\n",
      "train_loss :  1.4230402477444448\n",
      "epoch_num :  261\n",
      "test_loss :  1.4221368352818877\n",
      "train_loss :  1.4221368352818877\n",
      "epoch_num :  266\n",
      "test_loss :  1.4209263380009929\n",
      "train_loss :  1.4209263380009929\n",
      "epoch_num :  271\n",
      "test_loss :  1.4193531257656793\n",
      "train_loss :  1.4193531257656793\n",
      "epoch_num :  276\n",
      "test_loss :  1.417381702502887\n",
      "train_loss :  1.417381702502887\n",
      "epoch_num :  281\n",
      "test_loss :  1.4150271136413588\n",
      "train_loss :  1.4150271136413588\n",
      "epoch_num :  286\n",
      "test_loss :  1.4123842349164188\n",
      "train_loss :  1.4123842349164188\n",
      "epoch_num :  291\n",
      "test_loss :  1.4096193155104284\n",
      "train_loss :  1.4096193155104284\n",
      "epoch_num :  296\n",
      "test_loss :  1.4068996038218156\n",
      "train_loss :  1.4068996038218156\n",
      "epoch_num :  301\n",
      "test_loss :  1.4043188535212034\n",
      "train_loss :  1.4043188535212034\n",
      "epoch_num :  306\n",
      "test_loss :  1.4018943559667236\n",
      "train_loss :  1.4018943559667236\n",
      "epoch_num :  311\n",
      "test_loss :  1.3996103327247098\n",
      "train_loss :  1.3996103327247098\n",
      "epoch_num :  316\n",
      "test_loss :  1.3974486598189095\n",
      "train_loss :  1.3974486598189095\n",
      "epoch_num :  321\n",
      "test_loss :  1.395397192526735\n",
      "train_loss :  1.395397192526735\n",
      "epoch_num :  326\n",
      "test_loss :  1.3934493469805744\n",
      "train_loss :  1.3934493469805744\n",
      "epoch_num :  331\n",
      "test_loss :  1.3916024748625522\n",
      "train_loss :  1.3916024748625522\n",
      "epoch_num :  336\n",
      "test_loss :  1.3898563996734248\n",
      "train_loss :  1.3898563996734248\n",
      "epoch_num :  341\n",
      "test_loss :  1.3882121168830417\n",
      "train_loss :  1.3882121168830417\n",
      "epoch_num :  346\n",
      "test_loss :  1.386670722443554\n",
      "train_loss :  1.386670722443554\n",
      "epoch_num :  351\n",
      "test_loss :  1.3852326636202998\n",
      "train_loss :  1.3852326636202998\n",
      "epoch_num :  356\n",
      "test_loss :  1.3838973265650047\n",
      "train_loss :  1.3838973265650047\n",
      "epoch_num :  361\n",
      "test_loss :  1.3826629029154474\n",
      "train_loss :  1.3826629029154474\n",
      "epoch_num :  366\n",
      "test_loss :  1.3815264496821684\n",
      "train_loss :  1.3815264496821684\n",
      "epoch_num :  371\n",
      "test_loss :  1.380484061220523\n",
      "train_loss :  1.380484061220523\n",
      "epoch_num :  376\n",
      "test_loss :  1.3795310906161515\n",
      "train_loss :  1.3795310906161515\n",
      "epoch_num :  381\n",
      "test_loss :  1.3786623783039218\n",
      "train_loss :  1.3786623783039218\n",
      "epoch_num :  386\n",
      "test_loss :  1.3778724628905406\n",
      "train_loss :  1.3778724628905406\n",
      "epoch_num :  391\n",
      "test_loss :  1.3771557616332486\n",
      "train_loss :  1.3771557616332486\n",
      "epoch_num :  396\n",
      "test_loss :  1.3765067162754836\n",
      "train_loss :  1.3765067162754836\n",
      "epoch_num :  401\n",
      "test_loss :  1.375919904926822\n",
      "train_loss :  1.375919904926822\n",
      "epoch_num :  406\n",
      "test_loss :  1.3753901233676091\n",
      "train_loss :  1.3753901233676091\n",
      "epoch_num :  411\n",
      "test_loss :  1.3749124403444848\n",
      "train_loss :  1.3749124403444848\n",
      "epoch_num :  416\n",
      "test_loss :  1.3744822316785739\n",
      "train_loss :  1.3744822316785739\n",
      "epoch_num :  421\n",
      "test_loss :  1.3740951977332636\n",
      "train_loss :  1.3740951977332636\n",
      "epoch_num :  426\n",
      "test_loss :  1.3737473682475692\n",
      "train_loss :  1.3737473682475692\n",
      "epoch_num :  431\n",
      "test_loss :  1.3734350979006396\n",
      "train_loss :  1.3734350979006396\n",
      "epoch_num :  436\n",
      "test_loss :  1.3731550553329972\n",
      "train_loss :  1.3731550553329972\n",
      "epoch_num :  441\n",
      "test_loss :  1.3729042077649696\n",
      "train_loss :  1.3729042077649696\n",
      "epoch_num :  446\n",
      "test_loss :  1.3726798028484661\n",
      "train_loss :  1.3726798028484661\n",
      "epoch_num :  451\n",
      "test_loss :  1.3724793489722056\n",
      "train_loss :  1.3724793489722056\n",
      "epoch_num :  456\n",
      "test_loss :  1.37230059490992\n",
      "train_loss :  1.37230059490992\n",
      "epoch_num :  461\n",
      "test_loss :  1.3721415094473304\n",
      "train_loss :  1.3721415094473304\n",
      "epoch_num :  466\n",
      "test_loss :  1.3720002614356208\n",
      "train_loss :  1.3720002614356208\n",
      "epoch_num :  471\n",
      "test_loss :  1.3718752005841106\n",
      "train_loss :  1.3718752005841106\n",
      "epoch_num :  476\n",
      "test_loss :  1.3717648392118833\n",
      "train_loss :  1.3717648392118833\n",
      "epoch_num :  481\n",
      "test_loss :  1.3716678351147509\n",
      "train_loss :  1.3716678351147509\n",
      "epoch_num :  486\n",
      "test_loss :  1.371582975661667\n",
      "train_loss :  1.371582975661667\n",
      "epoch_num :  491\n",
      "test_loss :  1.37150916320405\n",
      "train_loss :  1.37150916320405\n",
      "epoch_num :  496\n",
      "test_loss :  1.3714454018560578\n",
      "train_loss :  1.3714454018560578\n",
      "epoch_num :  501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  1.37139078567932\n",
      "train_loss :  1.37139078567932\n",
      "epoch_num :  506\n",
      "test_loss :  1.3713444882785346\n",
      "train_loss :  1.3713444882785346\n",
      "epoch_num :  511\n",
      "test_loss :  1.3713057537841513\n",
      "train_loss :  1.3713057537841513\n",
      "epoch_num :  516\n",
      "test_loss :  1.3712738891663652\n",
      "train_loss :  1.3712738891663652\n",
      "epoch_num :  521\n",
      "test_loss :  1.3712482577920633\n",
      "train_loss :  1.3712482577920633\n",
      "epoch_num :  526\n",
      "test_loss :  1.3712282741068142\n",
      "train_loss :  1.3712282741068142\n",
      "epoch_num :  531\n",
      "test_loss :  1.371213399299926\n",
      "train_loss :  1.371213399299926\n",
      "epoch_num :  536\n",
      "test_loss :  1.371203137794483\n",
      "train_loss :  1.371203137794483\n",
      "epoch_num :  541\n",
      "test_loss :  1.3711970343975501\n",
      "train_loss :  1.3711970343975501\n",
      "epoch_num :  546\n",
      "test_loss :  1.3711946719485681\n",
      "train_loss :  1.3711946719485681\n",
      "epoch_num :  551\n",
      "test_loss :  1.3711956693159673\n",
      "train_loss :  1.3711956693159673\n",
      "epoch_num :  556\n",
      "test_loss :  1.3711996796108834\n",
      "train_loss :  1.3711996796108834\n",
      "epoch_num :  561\n",
      "test_loss :  1.3712063885107768\n",
      "train_loss :  1.3712063885107768\n",
      "epoch_num :  566\n",
      "test_loss :  1.3712155126119543\n",
      "train_loss :  1.3712155126119543\n",
      "epoch_num :  571\n",
      "test_loss :  1.3712267977560995\n",
      "train_loss :  1.3712267977560995\n",
      "epoch_num :  576\n",
      "test_loss :  1.3712400173001174\n",
      "train_loss :  1.3712400173001174\n",
      "epoch_num :  581\n",
      "test_loss :  1.3712549703190553\n",
      "train_loss :  1.3712549703190553\n",
      "epoch_num :  586\n",
      "test_loss :  1.371271479748338\n",
      "train_loss :  1.371271479748338\n",
      "epoch_num :  591\n",
      "test_loss :  1.3712893904830943\n",
      "train_loss :  1.3712893904830943\n",
      "epoch_num :  596\n",
      "test_loss :  1.3713085674597414\n",
      "train_loss :  1.3713085674597414\n",
      "epoch_num :  601\n",
      "test_loss :  1.3713288937482964\n",
      "train_loss :  1.3713288937482964\n",
      "epoch_num :  606\n",
      "test_loss :  1.3713502686841796\n",
      "train_loss :  1.3713502686841796\n",
      "epoch_num :  611\n",
      "test_loss :  1.3713726060659952\n",
      "train_loss :  1.3713726060659952\n",
      "epoch_num :  616\n",
      "test_loss :  1.3713958324421296\n",
      "train_loss :  1.3713958324421296\n",
      "epoch_num :  621\n",
      "test_loss :  1.3714198855041793\n",
      "train_loss :  1.3714198855041793\n",
      "epoch_num :  626\n",
      "test_loss :  1.3714447126002831\n",
      "train_loss :  1.3714447126002831\n",
      "epoch_num :  631\n",
      "test_loss :  1.3714702693764853\n",
      "train_loss :  1.3714702693764853\n",
      "epoch_num :  636\n",
      "test_loss :  1.3714965185496513\n",
      "train_loss :  1.3714965185496513\n",
      "epoch_num :  641\n",
      "test_loss :  1.3715234288117084\n",
      "train_loss :  1.3715234288117084\n",
      "epoch_num :  646\n",
      "test_loss :  1.3715509738615408\n",
      "train_loss :  1.3715509738615408\n",
      "epoch_num :  651\n",
      "test_loss :  1.3715791315585684\n",
      "train_loss :  1.3715791315585684\n",
      "epoch_num :  656\n",
      "test_loss :  1.3716078831900775\n",
      "train_loss :  1.3716078831900775\n",
      "epoch_num :  661\n",
      "test_loss :  1.3716372128432417\n",
      "train_loss :  1.3716372128432417\n",
      "epoch_num :  666\n",
      "test_loss :  1.3716671068721054\n",
      "train_loss :  1.3716671068721054\n",
      "epoch_num :  671\n",
      "test_loss :  1.3716975534495663\n",
      "train_loss :  1.3716975534495663\n",
      "epoch_num :  676\n",
      "test_loss :  1.3717285421945111\n",
      "train_loss :  1.3717285421945111\n",
      "epoch_num :  681\n",
      "test_loss :  1.3717600638646699\n",
      "train_loss :  1.3717600638646699\n",
      "epoch_num :  686\n",
      "test_loss :  1.3717921101063357\n",
      "train_loss :  1.3717921101063357\n",
      "epoch_num :  691\n",
      "test_loss :  1.371824673252756\n",
      "train_loss :  1.371824673252756\n",
      "epoch_num :  696\n",
      "test_loss :  1.37185774616376\n",
      "train_loss :  1.37185774616376\n",
      "epoch_num :  701\n",
      "test_loss :  1.3718913221000115\n",
      "train_loss :  1.3718913221000115\n",
      "epoch_num :  706\n",
      "test_loss :  1.3719253946260628\n",
      "train_loss :  1.3719253946260628\n",
      "epoch_num :  711\n",
      "test_loss :  1.3719599575370214\n",
      "train_loss :  1.3719599575370214\n",
      "epoch_num :  716\n",
      "test_loss :  1.3719950048044844\n",
      "train_loss :  1.3719950048044844\n",
      "epoch_num :  721\n",
      "test_loss :  1.3720305305379135\n",
      "train_loss :  1.3720305305379135\n",
      "epoch_num :  726\n",
      "test_loss :  1.372066528958291\n",
      "train_loss :  1.372066528958291\n",
      "epoch_num :  731\n",
      "test_loss :  1.3721029943813796\n",
      "train_loss :  1.3721029943813796\n",
      "epoch_num :  736\n",
      "test_loss :  1.3721399212083203\n",
      "train_loss :  1.3721399212083203\n",
      "epoch_num :  741\n",
      "test_loss :  1.3721773039217822\n",
      "train_loss :  1.3721773039217822\n",
      "epoch_num :  746\n",
      "test_loss :  1.3722151370861624\n",
      "train_loss :  1.3722151370861624\n",
      "epoch_num :  751\n",
      "test_loss :  1.372253415350583\n",
      "train_loss :  1.372253415350583\n",
      "epoch_num :  756\n",
      "test_loss :  1.3722921334538158\n",
      "train_loss :  1.3722921334538158\n",
      "epoch_num :  761\n",
      "test_loss :  1.3723312862302988\n",
      "train_loss :  1.3723312862302988\n",
      "epoch_num :  766\n",
      "test_loss :  1.3723708686167237\n",
      "train_loss :  1.3723708686167237\n",
      "epoch_num :  771\n",
      "test_loss :  1.3724108756586695\n",
      "train_loss :  1.3724108756586695\n",
      "epoch_num :  776\n",
      "test_loss :  1.3724513025170606\n",
      "train_loss :  1.3724513025170606\n",
      "epoch_num :  781\n",
      "test_loss :  1.3724921444741192\n",
      "train_loss :  1.3724921444741192\n",
      "epoch_num :  786\n",
      "test_loss :  1.3725333969386844\n",
      "train_loss :  1.3725333969386844\n",
      "epoch_num :  791\n",
      "test_loss :  1.3725750554507978\n",
      "train_loss :  1.3725750554507978\n",
      "epoch_num :  796\n",
      "test_loss :  1.3726171156854792\n",
      "train_loss :  1.3726171156854792\n",
      "epoch_num :  801\n",
      "test_loss :  1.372659573455697\n",
      "train_loss :  1.372659573455697\n",
      "epoch_num :  806\n",
      "test_loss :  1.372702424714478\n",
      "train_loss :  1.372702424714478\n",
      "epoch_num :  811\n",
      "test_loss :  1.3727456655562513\n",
      "train_loss :  1.3727456655562513\n",
      "epoch_num :  816\n",
      "test_loss :  1.3727892922174179\n",
      "train_loss :  1.3727892922174179\n",
      "epoch_num :  821\n",
      "test_loss :  1.3728333010762062\n",
      "train_loss :  1.3728333010762062\n",
      "epoch_num :  826\n",
      "test_loss :  1.3728776886518523\n",
      "train_loss :  1.3728776886518523\n",
      "epoch_num :  831\n",
      "test_loss :  1.372922451603186\n",
      "train_loss :  1.372922451603186\n",
      "epoch_num :  836\n",
      "test_loss :  1.3729675867266715\n",
      "train_loss :  1.3729675867266715\n",
      "epoch_num :  841\n",
      "test_loss :  1.373013090953954\n",
      "train_loss :  1.373013090953954\n",
      "epoch_num :  846\n",
      "test_loss :  1.3730589613489874\n",
      "train_loss :  1.3730589613489874\n",
      "epoch_num :  851\n",
      "test_loss :  1.373105195104761\n",
      "train_loss :  1.373105195104761\n",
      "epoch_num :  856\n",
      "test_loss :  1.3731517895397232\n",
      "train_loss :  1.3731517895397232\n",
      "epoch_num :  861\n",
      "test_loss :  1.3731987420939458\n",
      "train_loss :  1.3731987420939458\n",
      "epoch_num :  866\n",
      "test_loss :  1.3732460503250288\n",
      "train_loss :  1.3732460503250288\n",
      "epoch_num :  871\n",
      "test_loss :  1.3732937119038497\n",
      "train_loss :  1.3732937119038497\n",
      "epoch_num :  876\n",
      "test_loss :  1.373341724610159\n",
      "train_loss :  1.373341724610159\n",
      "epoch_num :  881\n",
      "test_loss :  1.3733900863280606\n",
      "train_loss :  1.3733900863280606\n",
      "epoch_num :  886\n",
      "test_loss :  1.3734387950414466\n",
      "train_loss :  1.3734387950414466\n",
      "epoch_num :  891\n",
      "test_loss :  1.373487848829337\n",
      "train_loss :  1.373487848829337\n",
      "epoch_num :  896\n",
      "test_loss :  1.3735372458612605\n",
      "train_loss :  1.3735372458612605\n",
      "epoch_num :  901\n",
      "test_loss :  1.3735869843926034\n",
      "train_loss :  1.3735869843926034\n",
      "epoch_num :  906\n",
      "test_loss :  1.3736370627599692\n",
      "train_loss :  1.3736370627599692\n",
      "epoch_num :  911\n",
      "test_loss :  1.3736874793766467\n",
      "train_loss :  1.3736874793766467\n",
      "epoch_num :  916\n",
      "test_loss :  1.3737382327280716\n",
      "train_loss :  1.3737382327280716\n",
      "epoch_num :  921\n",
      "test_loss :  1.3737893213673866\n",
      "train_loss :  1.3737893213673866\n",
      "epoch_num :  926\n",
      "test_loss :  1.3738407439111147\n",
      "train_loss :  1.3738407439111147\n",
      "epoch_num :  931\n",
      "test_loss :  1.3738924990348884\n",
      "train_loss :  1.3738924990348884\n",
      "epoch_num :  936\n",
      "test_loss :  1.373944585469306\n",
      "train_loss :  1.373944585469306\n",
      "epoch_num :  941\n",
      "test_loss :  1.3739970019958967\n",
      "train_loss :  1.3739970019958967\n",
      "epoch_num :  946\n",
      "test_loss :  1.3740497474432058\n",
      "train_loss :  1.3740497474432058\n",
      "epoch_num :  951\n",
      "test_loss :  1.3741028206830024\n",
      "train_loss :  1.3741028206830024\n",
      "epoch_num :  956\n",
      "test_loss :  1.3741562206266003\n",
      "train_loss :  1.3741562206266003\n",
      "epoch_num :  961\n",
      "test_loss :  1.374209946221337\n",
      "train_loss :  1.374209946221337\n",
      "epoch_num :  966\n",
      "test_loss :  1.3742639964471344\n",
      "train_loss :  1.3742639964471344\n",
      "epoch_num :  971\n",
      "test_loss :  1.374318370313252\n",
      "train_loss :  1.374318370313252\n",
      "epoch_num :  976\n",
      "test_loss :  1.37437306685511\n",
      "train_loss :  1.37437306685511\n",
      "epoch_num :  981\n",
      "test_loss :  1.3744280851312787\n",
      "train_loss :  1.3744280851312787\n",
      "epoch_num :  986\n",
      "test_loss :  1.3744834242206072\n",
      "train_loss :  1.3744834242206072\n",
      "epoch_num :  991\n",
      "test_loss :  1.374539083219414\n",
      "train_loss :  1.374539083219414\n",
      "epoch_num :  996\n"
     ]
    }
   ],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "biases1_momentum = 0\n",
    "biases2_momentum = 0\n",
    "weights1_momentum = 0\n",
    "weights2_momentum = 0\n",
    "\n",
    "weights1_main = weights1\n",
    "weights2_main = weights2\n",
    "biases1_main = biases1\n",
    "biases2_main = biases2\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "    if epoch_num%5==1:\n",
    "        print(\"test_loss : \", test_losses[-1])\n",
    "        print(\"train_loss : \", losses[-1])\n",
    "        print(\"epoch_num : \", epoch_num)\n",
    "    for sample_num in range(x_train.shape[1]):\n",
    "#         print(\"sample_num : \", sample_num)\n",
    "        #Forward propagation\n",
    "        out1_temp = np.matmul(weights1.T, x_train[:,sample_num])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        #Back-propogation\n",
    "        error_out = out2-y_train[:, sample_num]\n",
    "        err_2 = np.matrix(error_out)\n",
    "        grad_2 = np.matmul(out1, err_2.T)\n",
    "        err_1 = np.multiply(np.matmul(weights2, err_2), np.multiply(out1, np.ones(out1.shape)-out1))\n",
    "        grad_1 = np.multiply(x_train[:, sample_num], err_1.T)\n",
    "        #Updating the weights\n",
    "        \n",
    "        weights1 = weights1_main\n",
    "        weights2 = weights2_main\n",
    "        biases1 = biases1_main\n",
    "        biases2 = biases2_main\n",
    "        \n",
    "        biases1_momentum = alpha*biases1_momentum-learning_rate*err_1\n",
    "        biases2_momentum = alpha*biases2_momentum-learning_rate*err_2\n",
    "        weights1_momentum = alpha*weights1_momentum-learning_rate*grad_1\n",
    "        weights2_momentum = alpha*weights2_momentum-learning_rate*grad_2\n",
    "        \n",
    "        biases1_main = biases1_main + biases1_momentum\n",
    "        biases2_main = biases2_main + biases2_momentum\n",
    "        weights1_main = weights1_main + weights1_momentum\n",
    "        weights2_main = weights2_main + weights2_momentum\n",
    "        \n",
    "        biases1 = biases1 - alpha*biases1_momentum\n",
    "        biases2 = biases2 - alpha*biases2_momentum\n",
    "        weights1 = weights1 - alpha*weights1_momentum\n",
    "        weights2 = weights2 - alpha*weights2_momentum\n",
    "        #Updating the accuracy and loss\n",
    "        #End of for loop\n",
    "#     print(\"accuracy : \", np.mean(acc_arr))\n",
    "    test_losses.append(test_loss(x_test, y_test, weights1_main, weights2_main, biases1_main, biases2_main))\n",
    "#     accuracies_test.append(acc_out(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_train.append(acc_out(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    losses.append(test_loss(x_test, y_test, weights1_main, weights2_main, biases1_main, biases2_main))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot(211)\n",
    "plt.plot(losses[1:], label = 'train_loss')\n",
    "plt.plot(test_losses[1:], label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.multiply(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test, pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26.52604597 12.12002362 10.21535427 28.12771296 28.83626326]\n",
      " [29.05095559 15.09555882 13.53557174 26.47001319 30.33176895]]\n",
      "[[26.48 12.16 10.35 29.71 29.47]\n",
      " [30.91 15.18 13.65 28.02 29.77]]\n"
     ]
    }
   ],
   "source": [
    "ind = 101\n",
    "print(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)[:,5:10])\n",
    "print(y_test[:,5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
