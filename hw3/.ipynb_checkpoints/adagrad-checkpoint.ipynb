{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def test_loss(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out2 = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "        loss = []\n",
    "        for iter in range(out2.shape[1]):\n",
    "            loss.append(MSE(out2[:, iter], y_in[:, iter]))\n",
    "        return np.mean(loss)\n",
    "\n",
    "def pred_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out1_temp = np.matmul(weights1.T, x_in[:,:])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        return out2\n",
    "\n",
    "def acc_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "    pred = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "    sum_acc = 0\n",
    "    for iter in range(pred.shape[1]):\n",
    "#         print(iter)\n",
    "        sum_acc+=(np.argmax(pred[:, iter])==np.argmax(y_in[:, iter]))\n",
    "    return sum_acc/y_in.shape[1]\n",
    "\n",
    "def MSE(yHat, y):\n",
    "    return np.sum(np.multiply(yHat - y, yHat - y)) / y.size\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-5):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.mean(np.multiply(np.log(predictions),targets)) \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 10)\n"
     ]
    }
   ],
   "source": [
    "##Load and segregate the data\n",
    "letters = genfromtxt('energy_eff2.csv', delimiter=',')\n",
    "# np.random.shuffle(letters)\n",
    "print(letters.shape)\n",
    "x_in = letters[:, 0:8]\n",
    "x_mean = np.mean(x_in, axis = 0)\n",
    "x_std = np.std(x_in, axis = 0)\n",
    "y_in = letters[:, 8:10]\n",
    "y_mean = np.mean(y_in, axis=0)\n",
    "y_std = np.std(y_in, axis = 0)\n",
    "# y_in = (y_in-y_mean)/y_std\n",
    "x_in = (x_in-x_mean)/x_std\n",
    "# print(np.std(y_in, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_in[0:int(x_in.shape[0]*0.7), :]\n",
    "y_train = y_in[0:int(x_in.shape[0]*0.7), :]\n",
    "\n",
    "x_test = x_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]\n",
    "y_test = y_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1000\n",
    "alpha = 1\n",
    "epsilon = 1e-12\n",
    "\n",
    "weights1 = np.random.normal(size=(x_in.shape[1], hidden_size))\n",
    "weights2 = np.random.normal(size=(hidden_size, y_in.shape[1]))\n",
    "print(weights1.shape)\n",
    "print(weights2.shape)\n",
    "biases1 = np.random.normal(size=(hidden_size, 1))\n",
    "biases2 = np.random.normal(size=(y_in.shape[1], 1))\n",
    "\n",
    "x_train = np.matrix(x_train)\n",
    "x_train = x_train.T\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = np.matrix(x_test)\n",
    "x_test = x_test.T\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  35.95366356966176\n",
      "train_loss :  45.160173814916924\n",
      "epoch_num :  1\n",
      "test_loss :  13.365240387631614\n",
      "train_loss :  14.331372766832978\n",
      "epoch_num :  6\n",
      "test_loss :  10.247962067288828\n",
      "train_loss :  10.746337178777653\n",
      "epoch_num :  11\n",
      "test_loss :  9.14988884220135\n",
      "train_loss :  9.527611301332925\n",
      "epoch_num :  16\n",
      "test_loss :  8.425214160492795\n",
      "train_loss :  8.827677756054022\n",
      "epoch_num :  21\n",
      "test_loss :  8.001448922992365\n",
      "train_loss :  8.389400984318264\n",
      "epoch_num :  26\n",
      "test_loss :  7.695820830524173\n",
      "train_loss :  8.079485979773102\n",
      "epoch_num :  31\n",
      "test_loss :  7.438932608860341\n",
      "train_loss :  7.829870876122728\n",
      "epoch_num :  36\n",
      "test_loss :  7.2080926155183755\n",
      "train_loss :  7.609311090325788\n",
      "epoch_num :  41\n",
      "test_loss :  6.995110536837758\n",
      "train_loss :  7.4031653726746915\n",
      "epoch_num :  46\n",
      "test_loss :  6.797175962888281\n",
      "train_loss :  7.203081900062022\n",
      "epoch_num :  51\n",
      "test_loss :  6.6107398085501625\n",
      "train_loss :  7.000798154185627\n",
      "epoch_num :  56\n",
      "test_loss :  6.421363066349397\n",
      "train_loss :  6.782596496531381\n",
      "epoch_num :  61\n",
      "test_loss :  6.225048302303124\n",
      "train_loss :  6.5551348514251035\n",
      "epoch_num :  66\n",
      "test_loss :  6.058693954379914\n",
      "train_loss :  6.355971363326268\n",
      "epoch_num :  71\n",
      "test_loss :  5.919696708655255\n",
      "train_loss :  6.186359673725142\n",
      "epoch_num :  76\n",
      "test_loss :  5.797468889273326\n",
      "train_loss :  6.037164522122282\n",
      "epoch_num :  81\n",
      "test_loss :  5.688625574053302\n",
      "train_loss :  5.903680716932805\n",
      "epoch_num :  86\n",
      "test_loss :  5.591031576326064\n",
      "train_loss :  5.7831018365829925\n",
      "epoch_num :  91\n",
      "test_loss :  5.50251704930019\n",
      "train_loss :  5.67320839203827\n",
      "epoch_num :  96\n",
      "test_loss :  5.421146553281977\n",
      "train_loss :  5.5721520601591825\n",
      "epoch_num :  101\n",
      "test_loss :  5.345375076042104\n",
      "train_loss :  5.478411285167967\n",
      "epoch_num :  106\n",
      "test_loss :  5.274026688298197\n",
      "train_loss :  5.390736188322467\n",
      "epoch_num :  111\n",
      "test_loss :  5.206223585261243\n",
      "train_loss :  5.308094282562006\n",
      "epoch_num :  116\n",
      "test_loss :  5.1413157407868395\n",
      "train_loss :  5.229627589047413\n",
      "epoch_num :  121\n",
      "test_loss :  5.0788234010550894\n",
      "train_loss :  5.154624373531652\n",
      "epoch_num :  126\n",
      "test_loss :  5.018395729060767\n",
      "train_loss :  5.0825080019479\n",
      "epoch_num :  131\n",
      "test_loss :  4.959787438240835\n",
      "train_loss :  5.0128439255095545\n",
      "epoch_num :  136\n",
      "test_loss :  4.9028538656659935\n",
      "train_loss :  4.945359394957987\n",
      "epoch_num :  141\n",
      "test_loss :  4.847560327017748\n",
      "train_loss :  4.879958796774108\n",
      "epoch_num :  146\n",
      "test_loss :  4.793992002356185\n",
      "train_loss :  4.816708786936729\n",
      "epoch_num :  151\n",
      "test_loss :  4.742340057192976\n",
      "train_loss :  4.755777212087371\n",
      "epoch_num :  156\n",
      "test_loss :  4.692843696080688\n",
      "train_loss :  4.697340686506517\n",
      "epoch_num :  161\n",
      "test_loss :  4.645699413444156\n",
      "train_loss :  4.64150164860173\n",
      "epoch_num :  166\n",
      "test_loss :  4.600984145302868\n",
      "train_loss :  4.588248951014693\n",
      "epoch_num :  171\n",
      "test_loss :  4.558632754501261\n",
      "train_loss :  4.53746323848094\n",
      "epoch_num :  176\n",
      "test_loss :  4.518467224226799\n",
      "train_loss :  4.488944859354431\n",
      "epoch_num :  181\n",
      "test_loss :  4.480246901658367\n",
      "train_loss :  4.442450167536687\n",
      "epoch_num :  186\n",
      "test_loss :  4.443713896691661\n",
      "train_loss :  4.397733297443031\n",
      "epoch_num :  191\n",
      "test_loss :  4.408623529890919\n",
      "train_loss :  4.3545802393078175\n",
      "epoch_num :  196\n",
      "test_loss :  4.374760875947816\n",
      "train_loss :  4.3128222769346225\n",
      "epoch_num :  201\n",
      "test_loss :  4.341947751881946\n",
      "train_loss :  4.272332425340109\n",
      "epoch_num :  206\n",
      "test_loss :  4.310043633188842\n",
      "train_loss :  4.233015251178668\n",
      "epoch_num :  211\n",
      "test_loss :  4.278942780174167\n",
      "train_loss :  4.194796643761138\n",
      "epoch_num :  216\n",
      "test_loss :  4.248569131476455\n",
      "train_loss :  4.1576157706519705\n",
      "epoch_num :  221\n",
      "test_loss :  4.218870281473174\n",
      "train_loss :  4.121419412168243\n",
      "epoch_num :  226\n",
      "test_loss :  4.189811893318709\n",
      "train_loss :  4.086158419239592\n",
      "epoch_num :  231\n",
      "test_loss :  4.161373965811556\n",
      "train_loss :  4.0517865471502725\n",
      "epoch_num :  236\n",
      "test_loss :  4.1335502467147\n",
      "train_loss :  4.0182631728527705\n",
      "epoch_num :  241\n",
      "test_loss :  4.106350625827874\n",
      "train_loss :  3.9855624444564923\n",
      "epoch_num :  246\n",
      "test_loss :  4.079800606203574\n",
      "train_loss :  3.953687174619229\n",
      "epoch_num :  251\n",
      "test_loss :  4.053925031227324\n",
      "train_loss :  3.922669078669309\n",
      "epoch_num :  256\n",
      "test_loss :  4.02872116707492\n",
      "train_loss :  3.8925325540153\n",
      "epoch_num :  261\n",
      "test_loss :  4.004159524678451\n",
      "train_loss :  3.8632559643315316\n",
      "epoch_num :  266\n",
      "test_loss :  3.9802092585404307\n",
      "train_loss :  3.834784737890751\n",
      "epoch_num :  271\n",
      "test_loss :  3.9568459248048646\n",
      "train_loss :  3.807064649647104\n",
      "epoch_num :  276\n",
      "test_loss :  3.9340456404926543\n",
      "train_loss :  3.780053284597038\n",
      "epoch_num :  281\n",
      "test_loss :  3.9117820422705876\n",
      "train_loss :  3.7537170773202657\n",
      "epoch_num :  286\n",
      "test_loss :  3.8900264633125468\n",
      "train_loss :  3.728027215949074\n",
      "epoch_num :  291\n",
      "test_loss :  3.8687488221780955\n",
      "train_loss :  3.702957390401615\n",
      "epoch_num :  296\n",
      "test_loss :  3.847918378177314\n",
      "train_loss :  3.678482770014648\n",
      "epoch_num :  301\n",
      "test_loss :  3.827504228894609\n",
      "train_loss :  3.6545795340687195\n",
      "epoch_num :  306\n",
      "test_loss :  3.8074755887211804\n",
      "train_loss :  3.6312246281954805\n",
      "epoch_num :  311\n",
      "test_loss :  3.7878019116658477\n",
      "train_loss :  3.6083956136482906\n",
      "epoch_num :  316\n",
      "test_loss :  3.768452912165303\n",
      "train_loss :  3.586070556705759\n",
      "epoch_num :  321\n",
      "test_loss :  3.7493985217759738\n",
      "train_loss :  3.564227936473249\n",
      "epoch_num :  326\n",
      "test_loss :  3.730608806326198\n",
      "train_loss :  3.5428465613783637\n",
      "epoch_num :  331\n",
      "test_loss :  3.7120538591399006\n",
      "train_loss :  3.5219054895141815\n",
      "epoch_num :  336\n",
      "test_loss :  3.693703680714464\n",
      "train_loss :  3.501383950077903\n",
      "epoch_num :  341\n",
      "test_loss :  3.67552805274998\n",
      "train_loss :  3.4812612641609286\n",
      "epoch_num :  346\n",
      "test_loss :  3.657496413922682\n",
      "train_loss :  3.461516763740188\n",
      "epoch_num :  351\n",
      "test_loss :  3.6395777458346545\n",
      "train_loss :  3.4421297081878\n",
      "epoch_num :  356\n",
      "test_loss :  3.6217404799791746\n",
      "train_loss :  3.4230791980920943\n",
      "epoch_num :  361\n",
      "test_loss :  3.6039524403435013\n",
      "train_loss :  3.404344086734305\n",
      "epoch_num :  366\n",
      "test_loss :  3.5861808415424106\n",
      "train_loss :  3.3859028902121096\n",
      "epoch_num :  371\n",
      "test_loss :  3.5683923692783037\n",
      "train_loss :  3.3677336979041246\n",
      "epoch_num :  376\n",
      "test_loss :  3.5505533785079466\n",
      "train_loss :  3.3498140855821976\n",
      "epoch_num :  381\n",
      "test_loss :  3.5326302547445723\n",
      "train_loss :  3.3321210336504787\n",
      "epoch_num :  386\n",
      "test_loss :  3.5145899946780674\n",
      "train_loss :  3.314630852001155\n",
      "epoch_num :  391\n",
      "test_loss :  3.496401072061346\n",
      "train_loss :  3.297319109456843\n",
      "epoch_num :  396\n",
      "test_loss :  3.4780346604852785\n",
      "train_loss :  3.2801605572792463\n",
      "epoch_num :  401\n",
      "test_loss :  3.4594662812892274\n",
      "train_loss :  3.2631290186495208\n",
      "epoch_num :  406\n",
      "test_loss :  3.440677925498821\n",
      "train_loss :  3.2461971828208416\n",
      "epoch_num :  411\n",
      "test_loss :  3.4216606551428446\n",
      "train_loss :  3.2293361840233663\n",
      "epoch_num :  416\n",
      "test_loss :  3.4024176140552918\n",
      "train_loss :  3.21251474764932\n",
      "epoch_num :  421\n",
      "test_loss :  3.3829672673195783\n",
      "train_loss :  3.195697533138337\n",
      "epoch_num :  426\n",
      "test_loss :  3.3633465419269624\n",
      "train_loss :  3.1788420786480036\n",
      "epoch_num :  431\n",
      "test_loss :  3.343613351962007\n",
      "train_loss :  3.161893457376499\n",
      "epoch_num :  436\n",
      "test_loss :  3.3238477029742954\n",
      "train_loss :  3.144775458194648\n",
      "epoch_num :  441\n",
      "test_loss :  3.3041499729819437\n",
      "train_loss :  3.1273771166143804\n",
      "epoch_num :  446\n",
      "test_loss :  3.2846335377529203\n",
      "train_loss :  3.1095348285781683\n",
      "epoch_num :  451\n",
      "test_loss :  3.2654058374787023\n",
      "train_loss :  3.0910161910844454\n",
      "epoch_num :  456\n",
      "test_loss :  3.2465277826491326\n",
      "train_loss :  3.0715286978645744\n",
      "epoch_num :  461\n",
      "test_loss :  3.227945077853462\n",
      "train_loss :  3.050808552528364\n",
      "epoch_num :  466\n",
      "test_loss :  3.2094260188207038\n",
      "train_loss :  3.0288558849664597\n",
      "epoch_num :  471\n",
      "test_loss :  3.1906316223357134\n",
      "train_loss :  3.0062390517815794\n",
      "epoch_num :  476\n",
      "test_loss :  3.171414403327664\n",
      "train_loss :  2.9840677669372115\n",
      "epoch_num :  481\n",
      "test_loss :  3.1520749515549924\n",
      "train_loss :  2.9633828644796445\n",
      "epoch_num :  486\n",
      "test_loss :  3.133185816985728\n",
      "train_loss :  2.944610447329544\n",
      "epoch_num :  491\n",
      "test_loss :  3.115204030929196\n",
      "train_loss :  2.9276560203396973\n",
      "epoch_num :  496\n",
      "test_loss :  3.0983058608061995\n",
      "train_loss :  2.912236478644695\n",
      "epoch_num :  501\n",
      "test_loss :  3.0824504236900228\n",
      "train_loss :  2.898059988505774\n",
      "epoch_num :  506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  3.0674964267950684\n",
      "train_loss :  2.8848761922015127\n",
      "epoch_num :  511\n",
      "test_loss :  3.05328410784299\n",
      "train_loss :  2.872483676652881\n",
      "epoch_num :  516\n"
     ]
    }
   ],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "biases1_squares = 0\n",
    "biases2_squares = 0\n",
    "weights1_squares = 0\n",
    "weights2_squares = 0\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "    if epoch_num%5==1:\n",
    "        print(\"test_loss : \", test_losses[-1])\n",
    "        print(\"train_loss : \", losses[-1])\n",
    "        print(\"epoch_num : \", epoch_num)\n",
    "    for sample_num in range(x_train.shape[1]):\n",
    "#         print(\"sample_num : \", sample_num)\n",
    "        #Forward propagation\n",
    "        out1_temp = np.matmul(weights1.T, x_train[:,sample_num])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        #Back-propogation\n",
    "        error_out = out2-y_train[:, sample_num]\n",
    "        err_2 = np.matrix(error_out)\n",
    "        grad_2 = np.matmul(out1, err_2.T)\n",
    "        err_1 = np.multiply(np.matmul(weights2, err_2), np.multiply(out1, np.ones(out1.shape)-out1))\n",
    "        grad_1 = np.multiply(x_train[:, sample_num], err_1.T)\n",
    "        #Updating the weights\n",
    "        \n",
    "        biases1_squares = alpha*biases1_squares+np.multiply(err_1, err_1)\n",
    "        biases2_squares = alpha*biases2_squares+np.multiply(err_2, err_2)\n",
    "        weights1_squares = alpha*weights1_squares+np.multiply(grad_1, grad_1)\n",
    "        weights2_squares = alpha*weights2_squares+np.multiply(grad_2, grad_2)\n",
    "        \n",
    "        biases1 = biases1 - np.multiply(learning_rate/(np.sqrt(biases1_squares)+epsilon), err_1)\n",
    "        biases2 = biases2 - np.multiply(learning_rate/(np.sqrt(biases2_squares)+epsilon), err_2)\n",
    "        weights1 = weights1 - np.multiply(learning_rate/(np.sqrt(weights1_squares)+epsilon), grad_1)\n",
    "        weights2 = weights2 - np.multiply(learning_rate/(np.sqrt(weights2_squares)+epsilon), grad_2)\n",
    "        #Updating the accuracy and loss\n",
    "        #End of for loop\n",
    "#     print(\"accuracy : \", np.mean(acc_arr))\n",
    "    test_losses.append(test_loss(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_test.append(acc_out(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_train.append(acc_out(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    losses.append(test_loss(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot(211)\n",
    "plt.plot(losses[1:], label = 'train_loss')\n",
    "plt.plot(test_losses[1:], label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.multiply(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test, pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 101\n",
    "print(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)[:,5:10])\n",
    "print(y_test[:,5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
