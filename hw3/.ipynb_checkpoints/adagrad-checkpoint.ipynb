{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def test_loss(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out2 = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "        loss = []\n",
    "        for iter in range(out2.shape[1]):\n",
    "            loss.append(MSE(out2[:, iter], y_in[:, iter]))\n",
    "        return np.mean(loss)\n",
    "\n",
    "def pred_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out1_temp = np.matmul(weights1.T, x_in[:,:])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        return out2\n",
    "\n",
    "def acc_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "    pred = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "    sum_acc = 0\n",
    "    for iter in range(pred.shape[1]):\n",
    "#         print(iter)\n",
    "        sum_acc+=(np.argmax(pred[:, iter])==np.argmax(y_in[:, iter]))\n",
    "    return sum_acc/y_in.shape[1]\n",
    "\n",
    "def MSE(yHat, y):\n",
    "    return np.sum(np.multiply(yHat - y, yHat - y)) / y.size\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-5):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.mean(np.multiply(np.log(predictions),targets)) \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 10)\n"
     ]
    }
   ],
   "source": [
    "##Load and segregate the data\n",
    "letters = genfromtxt('energy_eff2.csv', delimiter=',')\n",
    "# np.random.shuffle(letters)\n",
    "print(letters.shape)\n",
    "x_in = letters[:, 0:8]\n",
    "x_mean = np.mean(x_in, axis = 0)\n",
    "x_std = np.std(x_in, axis = 0)\n",
    "y_in = letters[:, 8:10]\n",
    "y_mean = np.mean(y_in, axis=0)\n",
    "y_std = np.std(y_in, axis = 0)\n",
    "# y_in = (y_in-y_mean)/y_std\n",
    "x_in = (x_in-x_mean)/x_std\n",
    "# print(np.std(y_in, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_in[0:int(x_in.shape[0]*0.7), :]\n",
    "y_train = y_in[0:int(x_in.shape[0]*0.7), :]\n",
    "\n",
    "x_test = x_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]\n",
    "y_test = y_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1000\n",
    "alpha = 1\n",
    "epsilon = 1e-8\n",
    "\n",
    "weights1 = np.random.normal(size=(x_in.shape[1], hidden_size))\n",
    "weights2 = np.random.normal(size=(hidden_size, y_in.shape[1]))\n",
    "print(weights1.shape)\n",
    "print(weights2.shape)\n",
    "biases1 = np.random.normal(size=(hidden_size, 1))\n",
    "biases2 = np.random.normal(size=(y_in.shape[1], 1))\n",
    "\n",
    "x_train = np.matrix(x_train)\n",
    "x_train = x_train.T\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = np.matrix(x_test)\n",
    "x_test = x_test.T\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  29.125063620781944\n",
      "train_loss :  27.886172762573445\n",
      "epoch_num :  1\n",
      "test_loss :  13.293495272580405\n",
      "train_loss :  11.246863474341463\n",
      "epoch_num :  6\n",
      "test_loss :  11.450141090810217\n",
      "train_loss :  10.063754665691027\n",
      "epoch_num :  11\n",
      "test_loss :  9.920946088659829\n",
      "train_loss :  9.033729667999914\n",
      "epoch_num :  16\n",
      "test_loss :  9.247341393171068\n",
      "train_loss :  8.38881338951685\n",
      "epoch_num :  21\n",
      "test_loss :  8.756773995721142\n",
      "train_loss :  7.95385863127667\n",
      "epoch_num :  26\n",
      "test_loss :  8.297064515783758\n",
      "train_loss :  7.562994206252157\n",
      "epoch_num :  31\n",
      "test_loss :  7.853212749634959\n",
      "train_loss :  7.199578645168721\n",
      "epoch_num :  36\n",
      "test_loss :  7.431388182590002\n",
      "train_loss :  6.862610227442156\n",
      "epoch_num :  41\n",
      "test_loss :  7.047483170652594\n",
      "train_loss :  6.552954465581658\n",
      "epoch_num :  46\n",
      "test_loss :  6.716611579959346\n",
      "train_loss :  6.268672183688968\n",
      "epoch_num :  51\n",
      "test_loss :  6.450758948803103\n",
      "train_loss :  6.011945023720741\n",
      "epoch_num :  56\n",
      "test_loss :  6.24386768690312\n",
      "train_loss :  5.78530857436308\n",
      "epoch_num :  61\n",
      "test_loss :  6.07551188835194\n",
      "train_loss :  5.5832799363665995\n",
      "epoch_num :  66\n",
      "test_loss :  5.927063734128532\n",
      "train_loss :  5.395757161234313\n",
      "epoch_num :  71\n",
      "test_loss :  5.7854670762024325\n",
      "train_loss :  5.213677667293076\n",
      "epoch_num :  76\n",
      "test_loss :  5.640073319250204\n",
      "train_loss :  5.031011204899824\n",
      "epoch_num :  81\n",
      "test_loss :  5.481796585071685\n",
      "train_loss :  4.8461512532534545\n",
      "epoch_num :  86\n",
      "test_loss :  5.309025675606001\n",
      "train_loss :  4.663775930071306\n",
      "epoch_num :  91\n",
      "test_loss :  5.133621375430158\n",
      "train_loss :  4.49363062993985\n",
      "epoch_num :  96\n",
      "test_loss :  4.971344408387904\n",
      "train_loss :  4.3430985343567405\n",
      "epoch_num :  101\n",
      "test_loss :  4.828826734948798\n",
      "train_loss :  4.212583987344852\n",
      "epoch_num :  106\n",
      "test_loss :  4.704532641727656\n",
      "train_loss :  4.0985575990912775\n",
      "epoch_num :  111\n",
      "test_loss :  4.594649163515529\n",
      "train_loss :  3.9972513648494186\n",
      "epoch_num :  116\n",
      "test_loss :  4.495837268002737\n",
      "train_loss :  3.9058361441075276\n",
      "epoch_num :  121\n",
      "test_loss :  4.405675235616344\n",
      "train_loss :  3.822355626161412\n",
      "epoch_num :  126\n",
      "test_loss :  4.322470997651968\n",
      "train_loss :  3.7454501522417027\n",
      "epoch_num :  131\n",
      "test_loss :  4.245031065890029\n",
      "train_loss :  3.6741433784839734\n",
      "epoch_num :  136\n",
      "test_loss :  4.172493177035159\n",
      "train_loss :  3.6077072345427936\n",
      "epoch_num :  141\n",
      "test_loss :  4.104217512503521\n",
      "train_loss :  3.5455793797421333\n",
      "epoch_num :  146\n",
      "test_loss :  4.039717815127378\n",
      "train_loss :  3.487312116700826\n",
      "epoch_num :  151\n",
      "test_loss :  3.9786177556390387\n",
      "train_loss :  3.432539877607989\n",
      "epoch_num :  156\n",
      "test_loss :  3.9206227566433984\n",
      "train_loss :  3.3809577936363966\n",
      "epoch_num :  161\n",
      "test_loss :  3.865500945738574\n",
      "train_loss :  3.3323069838275567\n",
      "epoch_num :  166\n",
      "test_loss :  3.8130692435608498\n",
      "train_loss :  3.2863640035416406\n",
      "epoch_num :  171\n",
      "test_loss :  3.763182213439615\n",
      "train_loss :  3.2429329647872254\n",
      "epoch_num :  176\n",
      "test_loss :  3.715722453559477\n",
      "train_loss :  3.2018394893482958\n",
      "epoch_num :  181\n",
      "test_loss :  3.670592103075882\n",
      "train_loss :  3.162926036356623\n",
      "epoch_num :  186\n",
      "test_loss :  3.6277054979403607\n",
      "train_loss :  3.126048346451158\n",
      "epoch_num :  191\n",
      "test_loss :  3.5869831892960575\n",
      "train_loss :  3.0910728275230137\n",
      "epoch_num :  196\n",
      "test_loss :  3.548347499790175\n",
      "train_loss :  3.0578747247686136\n",
      "epoch_num :  201\n",
      "test_loss :  3.51171963991376\n",
      "train_loss :  3.0263369122055845\n",
      "epoch_num :  206\n",
      "test_loss :  3.4770182333724193\n",
      "train_loss :  2.996349140765867\n",
      "epoch_num :  211\n",
      "test_loss :  3.4441589733292943\n",
      "train_loss :  2.967807590528431\n",
      "epoch_num :  216\n",
      "test_loss :  3.4130550762539618\n",
      "train_loss :  2.940614600649028\n",
      "epoch_num :  221\n",
      "test_loss :  3.3836182110072826\n",
      "train_loss :  2.914678483388173\n",
      "epoch_num :  226\n",
      "test_loss :  3.3557596349909784\n",
      "train_loss :  2.8899133611080874\n",
      "epoch_num :  231\n",
      "test_loss :  3.329391341544131\n",
      "train_loss :  2.866238992284353\n",
      "epoch_num :  236\n",
      "test_loss :  3.304427093995848\n",
      "train_loss :  2.843580572430686\n",
      "epoch_num :  241\n",
      "test_loss :  3.2807832809171495\n",
      "train_loss :  2.8218685085491764\n",
      "epoch_num :  246\n",
      "test_loss :  3.2583795703020466\n",
      "train_loss :  2.801038172655539\n",
      "epoch_num :  251\n",
      "test_loss :  3.237139368382085\n",
      "train_loss :  2.7810296427825087\n",
      "epoch_num :  256\n",
      "test_loss :  3.2169901044717237\n",
      "train_loss :  2.7617874401567577\n",
      "epoch_num :  261\n",
      "test_loss :  3.1978633701316412\n",
      "train_loss :  2.743260270128757\n",
      "epoch_num :  266\n",
      "test_loss :  3.1796949421437875\n",
      "train_loss :  2.7254007726808034\n",
      "epoch_num :  271\n",
      "test_loss :  3.1624247167338244\n",
      "train_loss :  2.7081652864083487\n",
      "epoch_num :  276\n",
      "test_loss :  3.1459965788432114\n",
      "train_loss :  2.691513628013091\n",
      "epoch_num :  281\n",
      "test_loss :  3.1303582261234064\n",
      "train_loss :  2.6754088876830893\n",
      "epoch_num :  286\n",
      "test_loss :  3.1154609633297166\n",
      "train_loss :  2.659817239319939\n",
      "epoch_num :  291\n",
      "test_loss :  3.101259479254722\n",
      "train_loss :  2.64470776343409\n",
      "epoch_num :  296\n",
      "test_loss :  3.0877116153932973\n",
      "train_loss :  2.6300522796914767\n",
      "epoch_num :  301\n",
      "test_loss :  3.0747781331898656\n",
      "train_loss :  2.6158251855865604\n",
      "epoch_num :  306\n",
      "test_loss :  3.062422484937361\n",
      "train_loss :  2.602003297568862\n",
      "epoch_num :  311\n",
      "test_loss :  3.0506105920976365\n",
      "train_loss :  2.588565691183198\n",
      "epoch_num :  316\n",
      "test_loss :  3.039310633897893\n",
      "train_loss :  2.575493537397447\n",
      "epoch_num :  321\n",
      "test_loss :  3.0284928484266644\n",
      "train_loss :  2.5627699332492972\n",
      "epoch_num :  326\n",
      "test_loss :  3.018129348003857\n",
      "train_loss :  2.5503797261647563\n",
      "epoch_num :  331\n",
      "test_loss :  3.008193950239115\n",
      "train_loss :  2.5383093326623944\n",
      "epoch_num :  336\n",
      "test_loss :  2.9986620258431795\n",
      "train_loss :  2.5265465535017393\n",
      "epoch_num :  341\n",
      "test_loss :  2.989510363862302\n",
      "train_loss :  2.515080388493721\n",
      "epoch_num :  346\n",
      "test_loss :  2.9807170545390216\n",
      "train_loss :  2.5039008550134945\n",
      "epoch_num :  351\n",
      "test_loss :  2.9722613894675987\n",
      "train_loss :  2.492998814633249\n",
      "epoch_num :  356\n",
      "test_loss :  2.9641237781420893\n",
      "train_loss :  2.4823658121810794\n",
      "epoch_num :  361\n",
      "test_loss :  2.956285679440679\n",
      "train_loss :  2.471993930961284\n",
      "epoch_num :  366\n",
      "test_loss :  2.9487295461115366\n",
      "train_loss :  2.4618756669362614\n",
      "epoch_num :  371\n",
      "test_loss :  2.941438779978485\n",
      "train_loss :  2.4520038235115584\n",
      "epoch_num :  376\n",
      "test_loss :  2.934397695405223\n",
      "train_loss :  2.442371427341965\n",
      "epoch_num :  381\n",
      "test_loss :  2.927591488558817\n",
      "train_loss :  2.432971664435487\n",
      "epoch_num :  386\n",
      "test_loss :  2.921006210186031\n",
      "train_loss :  2.4237978348884526\n",
      "epoch_num :  391\n",
      "test_loss :  2.914628739925435\n",
      "train_loss :  2.414843323907694\n",
      "epoch_num :  396\n",
      "test_loss :  2.908446760579954\n",
      "train_loss :  2.406101586386104\n",
      "epoch_num :  401\n",
      "test_loss :  2.902448731217501\n",
      "train_loss :  2.3975661421773\n",
      "epoch_num :  406\n",
      "test_loss :  2.896623858405063\n",
      "train_loss :  2.3892305793164526\n",
      "epoch_num :  411\n",
      "test_loss :  2.8909620652768586\n",
      "train_loss :  2.381088562696803\n",
      "epoch_num :  416\n",
      "test_loss :  2.885453958465354\n",
      "train_loss :  2.3731338460718465\n",
      "epoch_num :  421\n",
      "test_loss :  2.8800907931722004\n",
      "train_loss :  2.36536028565561\n",
      "epoch_num :  426\n",
      "test_loss :  2.874864436824103\n",
      "train_loss :  2.3577618539938907\n",
      "epoch_num :  431\n",
      "test_loss :  2.869767331851404\n",
      "train_loss :  2.350332653146904\n",
      "epoch_num :  436\n",
      "test_loss :  2.864792458158631\n",
      "train_loss :  2.343066926540976\n",
      "epoch_num :  441\n",
      "test_loss :  2.8599332958386694\n",
      "train_loss :  2.335959069106109\n",
      "epoch_num :  446\n",
      "test_loss :  2.8551837886311895\n",
      "train_loss :  2.329003635517521\n",
      "epoch_num :  451\n",
      "test_loss :  2.8505383085537903\n",
      "train_loss :  2.322195346507831\n",
      "epoch_num :  456\n",
      "test_loss :  2.8459916220516073\n",
      "train_loss :  2.315529093319874\n",
      "epoch_num :  461\n",
      "test_loss :  2.841538857927102\n",
      "train_loss :  2.3089999404369013\n",
      "epoch_num :  466\n",
      "test_loss :  2.837175477230934\n",
      "train_loss :  2.302603126765112\n",
      "epoch_num :  471\n",
      "test_loss :  2.832897245222646\n",
      "train_loss :  2.29633406546084\n",
      "epoch_num :  476\n",
      "test_loss :  2.828700205446794\n",
      "train_loss :  2.290188342597132\n",
      "epoch_num :  481\n",
      "test_loss :  2.8245806559180955\n",
      "train_loss :  2.2841617148571096\n",
      "epoch_num :  486\n",
      "test_loss :  2.8205351273678563\n",
      "train_loss :  2.2782501064279828\n",
      "epoch_num :  491\n",
      "test_loss :  2.816560363472018\n",
      "train_loss :  2.2724496052528345\n",
      "epoch_num :  496\n",
      "test_loss :  2.812653302958007\n",
      "train_loss :  2.266756458779185\n",
      "epoch_num :  501\n",
      "test_loss :  2.8088110634722887\n",
      "train_loss :  2.2611670693252304\n",
      "epoch_num :  506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  2.805030927080948\n",
      "train_loss :  2.2556779891674155\n",
      "epoch_num :  511\n",
      "test_loss :  2.8013103272713917\n",
      "train_loss :  2.2502859154370625\n",
      "epoch_num :  516\n",
      "test_loss :  2.7976468373230503\n",
      "train_loss :  2.2449876848994577\n",
      "epoch_num :  521\n",
      "test_loss :  2.7940381599172737\n",
      "train_loss :  2.2397802686761796\n",
      "epoch_num :  526\n",
      "test_loss :  2.7904821178618993\n",
      "train_loss :  2.2346607669603977\n",
      "epoch_num :  531\n",
      "test_loss :  2.7869766458119365\n",
      "train_loss :  2.229626403765468\n",
      "epoch_num :  536\n",
      "test_loss :  2.7835197828756324\n",
      "train_loss :  2.224674521739055\n",
      "epoch_num :  541\n",
      "test_loss :  2.780109666003066\n",
      "train_loss :  2.219802577068299\n",
      "epoch_num :  546\n",
      "test_loss :  2.776744524062569\n",
      "train_loss :  2.215008134495818\n",
      "epoch_num :  551\n",
      "test_loss :  2.7734226725189712\n",
      "train_loss :  2.2102888624617343\n",
      "epoch_num :  556\n",
      "test_loss :  2.7701425086355678\n",
      "train_loss :  2.205642528382981\n",
      "epoch_num :  561\n",
      "test_loss :  2.7669025071296836\n",
      "train_loss :  2.201066994078012\n",
      "epoch_num :  566\n",
      "test_loss :  2.7637012162194\n",
      "train_loss :  2.1965602113425193\n",
      "epoch_num :  571\n",
      "test_loss :  2.7605372540059254\n",
      "train_loss :  2.192120217679635\n",
      "epoch_num :  576\n",
      "test_loss :  2.7574093051429087\n",
      "train_loss :  2.187745132186474\n",
      "epoch_num :  581\n",
      "test_loss :  2.7543161177500086\n",
      "train_loss :  2.1834331515975545\n",
      "epoch_num :  586\n",
      "test_loss :  2.751256500533842\n",
      "train_loss :  2.179182546484552\n",
      "epoch_num :  591\n",
      "test_loss :  2.748229320084549\n",
      "train_loss :  2.1749916576110677\n",
      "epoch_num :  596\n",
      "test_loss :  2.745233498320877\n",
      "train_loss :  2.1708588924403434\n",
      "epoch_num :  601\n",
      "test_loss :  2.7422680100612395\n",
      "train_loss :  2.166782721793483\n",
      "epoch_num :  606\n",
      "test_loss :  2.73933188070142\n",
      "train_loss :  2.162761676655165\n",
      "epoch_num :  611\n",
      "test_loss :  2.736424183983815\n",
      "train_loss :  2.158794345123687\n",
      "epoch_num :  616\n",
      "test_loss :  2.7335440398452677\n",
      "train_loss :  2.1548793695017596\n",
      "epoch_num :  621\n",
      "test_loss :  2.7306906123337997\n",
      "train_loss :  2.1510154435244204\n",
      "epoch_num :  626\n",
      "test_loss :  2.7278631075865274\n",
      "train_loss :  2.147201309720164\n",
      "epoch_num :  631\n",
      "test_loss :  2.7250607718630344\n",
      "train_loss :  2.143435756901336\n",
      "epoch_num :  636\n",
      "test_loss :  2.7222828896302254\n",
      "train_loss :  2.139717617779681\n",
      "epoch_num :  641\n",
      "test_loss :  2.7195287816958675\n",
      "train_loss :  2.13604576670288\n",
      "epoch_num :  646\n",
      "test_loss :  2.7167978033894298\n",
      "train_loss :  2.1324191175078933\n",
      "epoch_num :  651\n",
      "test_loss :  2.7140893427894754\n",
      "train_loss :  2.128836621486768\n",
      "epoch_num :  656\n",
      "test_loss :  2.7114028189976733\n",
      "train_loss :  2.125297265460645\n",
      "epoch_num :  661\n",
      "test_loss :  2.7087376804600134\n",
      "train_loss :  2.1218000699576685\n",
      "epoch_num :  666\n",
      "test_loss :  2.706093403336155\n",
      "train_loss :  2.118344087490419\n",
      "epoch_num :  671\n",
      "test_loss :  2.7034694899180534\n",
      "train_loss :  2.1149284009286067\n",
      "epoch_num :  676\n",
      "test_loss :  2.700865467099148\n",
      "train_loss :  2.1115521219627174\n",
      "epoch_num :  681\n",
      "test_loss :  2.698280884895317\n",
      "train_loss :  2.1082143896543633\n",
      "epoch_num :  686\n",
      "test_loss :  2.695715315018907\n",
      "train_loss :  2.104914369069149\n",
      "epoch_num :  691\n",
      "test_loss :  2.693168349506856\n",
      "train_loss :  2.1016512499879028\n",
      "epoch_num :  696\n",
      "test_loss :  2.690639599403718\n",
      "train_loss :  2.098424245692215\n",
      "epoch_num :  701\n",
      "test_loss :  2.6881286935004804\n",
      "train_loss :  2.0952325918203365\n",
      "epoch_num :  706\n",
      "test_loss :  2.685635277129434\n",
      "train_loss :  2.092075545289509\n",
      "epoch_num :  711\n",
      "test_loss :  2.6831590110152574\n",
      "train_loss :  2.088952383280997\n",
      "epoch_num :  716\n",
      "test_loss :  2.6806995701824654\n",
      "train_loss :  2.0858624022841474\n",
      "epoch_num :  721\n",
      "test_loss :  2.6782566429186536\n",
      "train_loss :  2.0828049171959355\n",
      "epoch_num :  726\n",
      "test_loss :  2.675829929793077\n",
      "train_loss :  2.0797792604726135\n",
      "epoch_num :  731\n",
      "test_loss :  2.6734191427296934\n",
      "train_loss :  2.0767847813301477\n",
      "epoch_num :  736\n",
      "test_loss :  2.671024004133782\n",
      "train_loss :  2.073820844990353\n",
      "epoch_num :  741\n",
      "test_loss :  2.6686442460706226\n",
      "train_loss :  2.0708868319696805\n",
      "epoch_num :  746\n",
      "test_loss :  2.6662796094951666\n",
      "train_loss :  2.0679821374078435\n",
      "epoch_num :  751\n",
      "test_loss :  2.6639298435308127\n",
      "train_loss :  2.0651061704335265\n",
      "epoch_num :  756\n",
      "test_loss :  2.6615947047958026\n",
      "train_loss :  2.0622583535646366\n",
      "epoch_num :  761\n",
      "test_loss :  2.659273956775351\n",
      "train_loss :  2.059438122140629\n",
      "epoch_num :  766\n",
      "test_loss :  2.6569673692374765\n",
      "train_loss :  2.05664492378463\n",
      "epoch_num :  771\n",
      "test_loss :  2.654674717690811\n",
      "train_loss :  2.053878217893198\n",
      "epoch_num :  776\n",
      "test_loss :  2.6523957828821083\n",
      "train_loss :  2.0511374751516587\n",
      "epoch_num :  781\n",
      "test_loss :  2.650130350331525\n",
      "train_loss :  2.0484221770731326\n",
      "epoch_num :  786\n",
      "test_loss :  2.6478782099035385\n",
      "train_loss :  2.045731815559458\n",
      "epoch_num :  791\n",
      "test_loss :  2.64563915541151\n",
      "train_loss :  2.0430658924823546\n",
      "epoch_num :  796\n",
      "test_loss :  2.6434129842536604\n",
      "train_loss :  2.0404239192832505\n",
      "epoch_num :  801\n",
      "test_loss :  2.6411994970786083\n",
      "train_loss :  2.037805416590339\n",
      "epoch_num :  806\n",
      "test_loss :  2.6389984974783274\n",
      "train_loss :  2.0352099138515185\n",
      "epoch_num :  811\n",
      "test_loss :  2.6368097917065887\n",
      "train_loss :  2.0326369489819567\n",
      "epoch_num :  816\n",
      "test_loss :  2.634633188421022\n",
      "train_loss :  2.0300860680251254\n",
      "epoch_num :  821\n",
      "test_loss :  2.632468498446891\n",
      "train_loss :  2.0275568248262203\n",
      "epoch_num :  826\n",
      "test_loss :  2.6303155345607276\n",
      "train_loss :  2.0250487807170114\n",
      "epoch_num :  831\n",
      "test_loss :  2.628174111292252\n",
      "train_loss :  2.022561504211156\n",
      "epoch_num :  836\n",
      "test_loss :  2.626044044742722\n",
      "train_loss :  2.0200945707091598\n",
      "epoch_num :  841\n",
      "test_loss :  2.6239251524182183\n",
      "train_loss :  2.0176475622122214\n",
      "epoch_num :  846\n",
      "test_loss :  2.621817253076262\n",
      "train_loss :  2.0152200670442197\n",
      "epoch_num :  851\n",
      "test_loss :  2.6197201665843473\n",
      "train_loss :  2.0128116795812057\n",
      "epoch_num :  856\n",
      "test_loss :  2.6176337137889125\n",
      "train_loss :  2.010421999987799\n",
      "epoch_num :  861\n",
      "test_loss :  2.615557716393459\n",
      "train_loss :  2.0080506339599613\n",
      "epoch_num :  866\n",
      "test_loss :  2.6134919968444836\n",
      "train_loss :  2.0056971924736247\n",
      "epoch_num :  871\n",
      "test_loss :  2.6114363782240053\n",
      "train_loss :  2.0033612915387584\n",
      "epoch_num :  876\n",
      "test_loss :  2.609390684147559\n",
      "train_loss :  2.0010425519584722\n",
      "epoch_num :  881\n",
      "test_loss :  2.6073547386664555\n",
      "train_loss :  1.9987405990927913\n",
      "epoch_num :  886\n",
      "test_loss :  2.6053283661733033\n",
      "train_loss :  1.9964550626268203\n",
      "epoch_num :  891\n",
      "test_loss :  2.603311391309772\n",
      "train_loss :  1.9941855763430136\n",
      "epoch_num :  896\n",
      "test_loss :  2.6013036388755157\n",
      "train_loss :  1.9919317778973427\n",
      "epoch_num :  901\n",
      "test_loss :  2.5993049337374816\n",
      "train_loss :  1.9896933085991775\n",
      "epoch_num :  906\n",
      "test_loss :  2.597315100738646\n",
      "train_loss :  1.9874698131947839\n",
      "epoch_num :  911\n",
      "test_loss :  2.595333964605229\n",
      "train_loss :  1.9852609396542888\n",
      "epoch_num :  916\n",
      "test_loss :  2.5933613498516963\n",
      "train_loss :  1.9830663389621463\n",
      "epoch_num :  921\n",
      "test_loss :  2.5913970806827678\n",
      "train_loss :  1.9808856649110829\n",
      "epoch_num :  926\n",
      "test_loss :  2.5894409808915757\n",
      "train_loss :  1.978718573899582\n",
      "epoch_num :  931\n",
      "test_loss :  2.587492873753359\n",
      "train_loss :  1.9765647247330502\n",
      "epoch_num :  936\n",
      "test_loss :  2.585552581913993\n",
      "train_loss :  1.9744237784288354\n",
      "epoch_num :  941\n",
      "test_loss :  2.5836199272726987\n",
      "train_loss :  1.9722953980253446\n",
      "epoch_num :  946\n",
      "test_loss :  2.581694730858323\n",
      "train_loss :  1.9701792483955702\n",
      "epoch_num :  951\n",
      "test_loss :  2.5797768126987517\n",
      "train_loss :  1.968074996065416\n",
      "epoch_num :  956\n",
      "test_loss :  2.577865991682732\n",
      "train_loss :  1.9659823090372983\n",
      "epoch_num :  961\n",
      "test_loss :  2.5759620854138094\n",
      "train_loss :  1.9639008566195424\n",
      "epoch_num :  966\n",
      "test_loss :  2.574064910055998\n",
      "train_loss :  1.9618303092622627\n",
      "epoch_num :  971\n",
      "test_loss :  2.5721742801705925\n",
      "train_loss :  1.9597703384004115\n",
      "epoch_num :  976\n",
      "test_loss :  2.570290008544269\n",
      "train_loss :  1.9577206163049152\n",
      "epoch_num :  981\n",
      "test_loss :  2.568411906007895\n",
      "train_loss :  1.9556808159427665\n",
      "epoch_num :  986\n",
      "test_loss :  2.5665397812462314\n",
      "train_loss :  1.9536506108472464\n",
      "epoch_num :  991\n",
      "test_loss :  2.5646734405983818\n",
      "train_loss :  1.951629674999401\n",
      "epoch_num :  996\n"
     ]
    }
   ],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "biases1_squares = 0\n",
    "biases2_squares = 0\n",
    "weights1_squares = 0\n",
    "weights2_squares = 0\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "    if epoch_num%5==1:\n",
    "        print(\"test_loss : \", test_losses[-1])\n",
    "        print(\"train_loss : \", losses[-1])\n",
    "        print(\"epoch_num : \", epoch_num)\n",
    "    for sample_num in range(x_train.shape[1]):\n",
    "#         print(\"sample_num : \", sample_num)\n",
    "        #Forward propagation\n",
    "        out1_temp = np.matmul(weights1.T, x_train[:,sample_num])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        #Back-propogation\n",
    "        error_out = out2-y_train[:, sample_num]\n",
    "        err_2 = np.matrix(error_out)\n",
    "        grad_2 = np.matmul(out1, err_2.T)\n",
    "        err_1 = np.multiply(np.matmul(weights2, err_2), np.multiply(out1, np.ones(out1.shape)-out1))\n",
    "        grad_1 = np.multiply(x_train[:, sample_num], err_1.T)\n",
    "        #Updating the weights\n",
    "        \n",
    "        biases1_squares = alpha*biases1_squares+np.multiply(err_1, err_1)\n",
    "        biases2_squares = alpha*biases2_squares+np.multiply(err_2, err_2)\n",
    "        weights1_squares = alpha*weights1_squares+np.multiply(grad_1, grad_1)\n",
    "        weights2_squares = alpha*weights2_squares+np.multiply(grad_2, grad_2)\n",
    "        \n",
    "        biases1 = biases1 - np.multiply(learning_rate/(np.sqrt(biases1_squares)+epsilon), err_1)\n",
    "        biases2 = biases2 - np.multiply(learning_rate/(np.sqrt(biases2_squares)+epsilon), err_2)\n",
    "        weights1 = weights1 - np.multiply(learning_rate/(np.sqrt(weights1_squares)+epsilon), grad_1)\n",
    "        weights2 = weights2 - np.multiply(learning_rate/(np.sqrt(weights2_squares)+epsilon), grad_2)\n",
    "        #Updating the accuracy and loss\n",
    "        #End of for loop\n",
    "#     print(\"accuracy : \", np.mean(acc_arr))\n",
    "    test_losses.append(test_loss(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_test.append(acc_out(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_train.append(acc_out(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    losses.append(test_loss(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot(211)\n",
    "plt.plot(losses[1:], label = 'train_loss')\n",
    "plt.plot(test_losses[1:], label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.multiply(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test, pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.79281846 20.28498486 38.4759261  19.48539203 23.31090731]\n",
      " [14.42460173 24.60732846 40.85315148 21.62936253 26.81576646]]\n",
      "[[11.7  19.95 38.57 19.36 24.29]\n",
      " [13.88 21.97 43.86 22.73 25.95]]\n"
     ]
    }
   ],
   "source": [
    "ind = 101\n",
    "print(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)[:,5:10])\n",
    "print(y_test[:,5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9500193813153945\n"
     ]
    }
   ],
   "source": [
    "print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
