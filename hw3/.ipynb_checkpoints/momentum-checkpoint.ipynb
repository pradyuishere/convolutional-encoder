{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def test_loss(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out2 = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "        loss = []\n",
    "        for iter in range(out2.shape[1]):\n",
    "            loss.append(MSE(out2[:, iter], y_in[:, iter]))\n",
    "        return np.mean(loss)\n",
    "\n",
    "def pred_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out1_temp = np.matmul(weights1.T, x_in[:,:])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        return out2\n",
    "\n",
    "def acc_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "    pred = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "    sum_acc = 0\n",
    "    for iter in range(pred.shape[1]):\n",
    "#         print(iter)\n",
    "        sum_acc+=(np.argmax(pred[:, iter])==np.argmax(y_in[:, iter]))\n",
    "    return sum_acc/y_in.shape[1]\n",
    "\n",
    "def MSE(yHat, y):\n",
    "    return np.sum(np.multiply(yHat - y, yHat - y)) / y.size\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-5):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.mean(np.multiply(np.log(predictions),targets)) \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 10)\n"
     ]
    }
   ],
   "source": [
    "##Load and segregate the data\n",
    "letters = genfromtxt('energy_eff2.csv', delimiter=',')\n",
    "# np.random.shuffle(letters)\n",
    "print(letters.shape)\n",
    "x_in = letters[:, 0:8]\n",
    "x_mean = np.mean(x_in, axis = 0)\n",
    "x_std = np.std(x_in, axis = 0)\n",
    "y_in = letters[:, 8:10]\n",
    "y_mean = np.mean(y_in, axis=0)\n",
    "y_std = np.std(y_in, axis = 0)\n",
    "# y_in = (y_in-y_mean)/y_std\n",
    "x_in = (x_in-x_mean)/x_std\n",
    "# print(np.std(y_in, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_in[0:int(x_in.shape[0]*0.7), :]\n",
    "y_train = y_in[0:int(x_in.shape[0]*0.7), :]\n",
    "\n",
    "x_test = x_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]\n",
    "y_test = y_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "alpha = 0.9\n",
    "\n",
    "weights1 = np.random.normal(size=(x_in.shape[1], hidden_size))\n",
    "weights2 = np.random.normal(size=(hidden_size, y_in.shape[1]))\n",
    "print(weights1.shape)\n",
    "print(weights2.shape)\n",
    "biases1 = np.random.normal(size=(hidden_size, 1))\n",
    "biases2 = np.random.normal(size=(y_in.shape[1], 1))\n",
    "\n",
    "x_train = np.matrix(x_train)\n",
    "x_train = x_train.T\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = np.matrix(x_test)\n",
    "x_test = x_test.T\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  11.525572944785932\n",
      "train_loss :  9.682561978763081\n",
      "epoch_num :  1\n",
      "test_loss :  5.603395091513469\n",
      "train_loss :  4.552989788383027\n",
      "epoch_num :  6\n",
      "test_loss :  4.147131632611515\n",
      "train_loss :  3.2128004520437154\n",
      "epoch_num :  11\n",
      "test_loss :  3.5140369414047137\n",
      "train_loss :  2.6850411543002832\n",
      "epoch_num :  16\n",
      "test_loss :  3.0598970372729486\n",
      "train_loss :  2.345251757958317\n",
      "epoch_num :  21\n",
      "test_loss :  2.658561139536114\n",
      "train_loss :  2.0825461681492325\n",
      "epoch_num :  26\n",
      "test_loss :  2.4422804826123543\n",
      "train_loss :  1.9660107509354567\n",
      "epoch_num :  31\n",
      "test_loss :  2.3372061825954975\n",
      "train_loss :  1.9126698583534865\n",
      "epoch_num :  36\n",
      "test_loss :  2.2778509827566045\n",
      "train_loss :  1.8823145215462882\n",
      "epoch_num :  41\n",
      "test_loss :  2.2380651846631117\n",
      "train_loss :  1.8604221351747805\n",
      "epoch_num :  46\n",
      "test_loss :  2.2099088431349387\n",
      "train_loss :  1.8407879333304238\n",
      "epoch_num :  51\n",
      "test_loss :  2.189988566654446\n",
      "train_loss :  1.8221336659370195\n",
      "epoch_num :  56\n",
      "test_loss :  2.1758971484207623\n",
      "train_loss :  1.806234592321963\n",
      "epoch_num :  61\n",
      "test_loss :  2.1658748562793133\n",
      "train_loss :  1.7934051420447454\n",
      "epoch_num :  66\n",
      "test_loss :  2.1591199824996616\n",
      "train_loss :  1.7823628062263415\n",
      "epoch_num :  71\n",
      "test_loss :  2.15509542385295\n",
      "train_loss :  1.7720441167957728\n",
      "epoch_num :  76\n",
      "test_loss :  2.1531576938569725\n",
      "train_loss :  1.7618871305303283\n",
      "epoch_num :  81\n",
      "test_loss :  2.152823626861454\n",
      "train_loss :  1.7516762409050581\n",
      "epoch_num :  86\n",
      "test_loss :  2.1534202243228364\n",
      "train_loss :  1.7411457715371634\n",
      "epoch_num :  91\n",
      "test_loss :  2.1518228028496744\n",
      "train_loss :  1.729346313187471\n",
      "epoch_num :  96\n",
      "test_loss :  2.136517221294595\n",
      "train_loss :  1.7171340920756153\n",
      "epoch_num :  101\n",
      "test_loss :  2.110272618719401\n",
      "train_loss :  1.7063415523251573\n",
      "epoch_num :  106\n",
      "test_loss :  2.081891954820891\n",
      "train_loss :  1.6915266198122363\n",
      "epoch_num :  111\n",
      "test_loss :  2.0559195041165967\n",
      "train_loss :  1.6747748770181694\n",
      "epoch_num :  116\n",
      "test_loss :  2.039403860522885\n",
      "train_loss :  1.6618414264283725\n",
      "epoch_num :  121\n",
      "test_loss :  2.029340860027196\n",
      "train_loss :  1.652398853484051\n",
      "epoch_num :  126\n",
      "test_loss :  2.023321711072334\n",
      "train_loss :  1.6454200592990442\n",
      "epoch_num :  131\n",
      "test_loss :  2.0198705931545353\n",
      "train_loss :  1.6401475889588666\n",
      "epoch_num :  136\n",
      "test_loss :  2.0180231041364958\n",
      "train_loss :  1.6360607724649479\n",
      "epoch_num :  141\n",
      "test_loss :  2.017181981405953\n",
      "train_loss :  1.6328201829501146\n",
      "epoch_num :  146\n",
      "test_loss :  2.016982175325614\n",
      "train_loss :  1.6302060787476966\n",
      "epoch_num :  151\n",
      "test_loss :  2.017195254203933\n",
      "train_loss :  1.6280729237411946\n",
      "epoch_num :  156\n",
      "test_loss :  2.017674843577582\n",
      "train_loss :  1.6263203650727718\n",
      "epoch_num :  161\n",
      "test_loss :  2.018328935472704\n",
      "train_loss :  1.6248749665656375\n",
      "epoch_num :  166\n",
      "test_loss :  2.0191073479779704\n",
      "train_loss :  1.6236784698023916\n",
      "epoch_num :  171\n",
      "test_loss :  2.0199959046091402\n",
      "train_loss :  1.6226807149047875\n",
      "epoch_num :  176\n",
      "test_loss :  2.021010387323262\n",
      "train_loss :  1.621836711674409\n",
      "epoch_num :  181\n",
      "test_loss :  2.0221855521665084\n",
      "train_loss :  1.6211071152607368\n",
      "epoch_num :  186\n",
      "test_loss :  2.023559579259296\n",
      "train_loss :  1.6204602011210485\n",
      "epoch_num :  191\n",
      "test_loss :  2.0251596901814897\n",
      "train_loss :  1.619873066167191\n",
      "epoch_num :  196\n",
      "test_loss :  2.026994961997831\n",
      "train_loss :  1.6193308678499039\n",
      "epoch_num :  201\n",
      "test_loss :  2.0290573569852164\n",
      "train_loss :  1.6188243653710277\n",
      "epoch_num :  206\n",
      "test_loss :  2.0313273030524615\n",
      "train_loss :  1.6183468313548903\n",
      "epoch_num :  211\n",
      "test_loss :  2.0337794394175734\n",
      "train_loss :  1.6178914832773441\n",
      "epoch_num :  216\n",
      "test_loss :  2.036386090184136\n",
      "train_loss :  1.6174501966895856\n",
      "epoch_num :  221\n",
      "test_loss :  2.0391182466209745\n",
      "train_loss :  1.6170136610976273\n",
      "epoch_num :  226\n",
      "test_loss :  2.041945240911123\n",
      "train_loss :  1.616572585964376\n",
      "epoch_num :  231\n",
      "test_loss :  2.044834560521348\n",
      "train_loss :  1.6161192482367135\n",
      "epoch_num :  236\n",
      "test_loss :  2.047752512645098\n",
      "train_loss :  1.615648670305855\n",
      "epoch_num :  241\n",
      "test_loss :  2.0506655366426205\n",
      "train_loss :  1.6151590199051524\n",
      "epoch_num :  246\n",
      "test_loss :  2.0535416793135943\n",
      "train_loss :  1.6146512748520514\n",
      "epoch_num :  251\n",
      "test_loss :  2.0563519628662608\n",
      "train_loss :  1.6141285060269472\n",
      "epoch_num :  256\n",
      "test_loss :  2.0590715362425915\n",
      "train_loss :  1.6135951384476213\n",
      "epoch_num :  261\n",
      "test_loss :  2.0616804869475542\n",
      "train_loss :  1.6130563669845288\n",
      "epoch_num :  266\n",
      "test_loss :  2.0641642002611618\n",
      "train_loss :  1.6125177366168961\n",
      "epoch_num :  271\n",
      "test_loss :  2.066513255013125\n",
      "train_loss :  1.6119848342173528\n",
      "epoch_num :  276\n",
      "test_loss :  2.068722954284666\n",
      "train_loss :  1.6114630470519637\n",
      "epoch_num :  281\n",
      "test_loss :  2.070792636982396\n",
      "train_loss :  1.6109573679859055\n",
      "epoch_num :  286\n",
      "test_loss :  2.0727249049178917\n",
      "train_loss :  1.6104722429189433\n",
      "epoch_num :  291\n",
      "test_loss :  2.0745248618774306\n",
      "train_loss :  1.6100114604009579\n",
      "epoch_num :  296\n",
      "test_loss :  2.0761994212766175\n",
      "train_loss :  1.6095780822456733\n",
      "epoch_num :  301\n",
      "test_loss :  2.0777567081753254\n",
      "train_loss :  1.6091744116710842\n",
      "epoch_num :  306\n",
      "test_loss :  2.0792055614052822\n",
      "train_loss :  1.6088019938952205\n",
      "epoch_num :  311\n",
      "test_loss :  2.0805551303931322\n",
      "train_loss :  1.6084616434764498\n",
      "epoch_num :  316\n",
      "test_loss :  2.081814556228079\n",
      "train_loss :  1.608153492709172\n",
      "epoch_num :  321\n",
      "test_loss :  2.082992725239355\n",
      "train_loss :  1.6078770557289803\n",
      "epoch_num :  326\n",
      "test_loss :  2.0840980840492134\n",
      "train_loss :  1.6076313034419558\n",
      "epoch_num :  331\n",
      "test_loss :  2.0851385065755323\n",
      "train_loss :  1.6074147448960712\n",
      "epoch_num :  336\n",
      "test_loss :  2.086121205064667\n",
      "train_loss :  1.6072255112530391\n",
      "epoch_num :  341\n",
      "test_loss :  2.0870526785692327\n",
      "train_loss :  1.6070614391025173\n",
      "epoch_num :  346\n",
      "test_loss :  2.0879386932083626\n",
      "train_loss :  1.606920150475821\n",
      "epoch_num :  351\n",
      "test_loss :  2.088784289072533\n",
      "train_loss :  1.6067991275309246\n",
      "epoch_num :  356\n",
      "test_loss :  2.089593808863988\n",
      "train_loss :  1.6066957804546333\n",
      "epoch_num :  361\n",
      "test_loss :  2.090370943439783\n",
      "train_loss :  1.6066075076290227\n",
      "epoch_num :  366\n",
      "test_loss :  2.091118789491184\n",
      "train_loss :  1.6065317475228464\n",
      "epoch_num :  371\n",
      "test_loss :  2.091839914765644\n",
      "train_loss :  1.606466022098639\n",
      "epoch_num :  376\n",
      "test_loss :  2.0925364265838255\n",
      "train_loss :  1.6064079717889537\n",
      "epoch_num :  381\n",
      "test_loss :  2.093210039941808\n",
      "train_loss :  1.6063553823111734\n",
      "epoch_num :  386\n",
      "test_loss :  2.093862142187312\n",
      "train_loss :  1.606306203775566\n",
      "epoch_num :  391\n",
      "test_loss :  2.094493852057204\n",
      "train_loss :  1.6062585627035162\n",
      "epoch_num :  396\n",
      "test_loss :  2.0951060716849557\n",
      "train_loss :  1.6062107677115904\n",
      "epoch_num :  401\n",
      "test_loss :  2.0956995309560686\n",
      "train_loss :  1.6061613097254972\n",
      "epoch_num :  406\n",
      "test_loss :  2.096274824247157\n",
      "train_loss :  1.6061088576582974\n",
      "epoch_num :  411\n",
      "test_loss :  2.096832440093421\n",
      "train_loss :  1.6060522505134913\n",
      "epoch_num :  416\n",
      "test_loss :  2.0973727846755508\n",
      "train_loss :  1.6059904868546515\n",
      "epoch_num :  421\n",
      "test_loss :  2.097896200208988\n",
      "train_loss :  1.6059227125233306\n",
      "epoch_num :  426\n",
      "test_loss :  2.0984029793769974\n",
      "train_loss :  1.605848207393973\n",
      "epoch_num :  431\n",
      "test_loss :  2.0988933769044067\n",
      "train_loss :  1.6057663718397646\n",
      "epoch_num :  436\n",
      "test_loss :  2.099367619252056\n",
      "train_loss :  1.60567671345767\n",
      "epoch_num :  441\n",
      "test_loss :  2.099825913250453\n",
      "train_loss :  1.6055788344748667\n",
      "epoch_num :  446\n",
      "test_loss :  2.100268454306212\n",
      "train_loss :  1.6054724201396042\n",
      "epoch_num :  451\n",
      "test_loss :  2.1006954346217768\n",
      "train_loss :  1.6053572282928892\n",
      "epoch_num :  456\n",
      "test_loss :  2.1011070516747012\n",
      "train_loss :  1.6052330802247219\n",
      "epoch_num :  461\n",
      "test_loss :  2.1015035170124188\n",
      "train_loss :  1.6050998528405573\n",
      "epoch_num :  466\n",
      "test_loss :  2.101885065232162\n",
      "train_loss :  1.6049574720979083\n",
      "epoch_num :  471\n",
      "test_loss :  2.102251962836312\n",
      "train_loss :  1.6048059076174783\n",
      "epoch_num :  476\n",
      "test_loss :  2.1026045164853704\n",
      "train_loss :  1.6046451683252054\n",
      "epoch_num :  481\n",
      "test_loss :  2.1029430800244024\n",
      "train_loss :  1.6044752989394049\n",
      "epoch_num :  486\n",
      "test_loss :  2.1032680595495967\n",
      "train_loss :  1.604296377080266\n",
      "epoch_num :  491\n",
      "test_loss :  2.103579915732411\n",
      "train_loss :  1.6041085107489599\n",
      "epoch_num :  496\n",
      "test_loss :  2.103879162652382\n",
      "train_loss :  1.603911835903137\n",
      "epoch_num :  501\n",
      "test_loss :  2.104166362529826\n",
      "train_loss :  1.603706513849916\n",
      "epoch_num :  506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  2.1044421160076343\n",
      "train_loss :  1.603492728191179\n",
      "epoch_num :  511\n",
      "test_loss :  2.104707048002531\n",
      "train_loss :  1.6032706810940096\n",
      "epoch_num :  516\n",
      "test_loss :  2.1049617896013766\n",
      "train_loss :  1.6030405887228754\n",
      "epoch_num :  521\n",
      "test_loss :  2.105206956962333\n",
      "train_loss :  1.6028026757577751\n",
      "epoch_num :  526\n",
      "test_loss :  2.1054431286177264\n",
      "train_loss :  1.6025571690265468\n",
      "epoch_num :  531\n",
      "test_loss :  2.1056708228821446\n",
      "train_loss :  1.6023042903878566\n",
      "epoch_num :  536\n",
      "test_loss :  2.105890477172845\n",
      "train_loss :  1.6020442490989537\n",
      "epoch_num :  541\n",
      "test_loss :  2.1061024309091527\n",
      "train_loss :  1.6017772339739182\n",
      "epoch_num :  546\n",
      "test_loss :  2.106306913278131\n",
      "train_loss :  1.6015034056719895\n",
      "epoch_num :  551\n",
      "test_loss :  2.106504036589604\n",
      "train_loss :  1.601222889446798\n",
      "epoch_num :  556\n",
      "test_loss :  2.1066937952882796\n",
      "train_loss :  1.600935768638369\n",
      "epoch_num :  561\n",
      "test_loss :  2.1068760700542457\n",
      "train_loss :  1.6006420791110019\n",
      "epoch_num :  566\n",
      "test_loss :  2.107050635909059\n",
      "train_loss :  1.6003418047462605\n",
      "epoch_num :  571\n",
      "test_loss :  2.107217172922836\n",
      "train_loss :  1.6000348740071784\n",
      "epoch_num :  576\n",
      "test_loss :  2.1073752780177117\n",
      "train_loss :  1.5997211575108587\n",
      "epoch_num :  581\n",
      "test_loss :  2.107524476466704\n",
      "train_loss :  1.5994004664899144\n",
      "epoch_num :  586\n",
      "test_loss :  2.1076642319462278\n",
      "train_loss :  1.5990725519911952\n",
      "epoch_num :  591\n",
      "test_loss :  2.1077939543458206\n",
      "train_loss :  1.598737104649841\n",
      "epoch_num :  596\n",
      "test_loss :  2.1079130049017847\n",
      "train_loss :  1.5983937548818792\n",
      "epoch_num :  601\n",
      "test_loss :  2.108020698543522\n",
      "train_loss :  1.59804207335162\n",
      "epoch_num :  606\n",
      "test_loss :  2.1081163035830954\n",
      "train_loss :  1.5976815715847696\n",
      "epoch_num :  611\n",
      "test_loss :  2.1081990390207594\n",
      "train_loss :  1.5973117026101813\n",
      "epoch_num :  616\n",
      "test_loss :  2.1082680697801206\n",
      "train_loss :  1.5969318615211328\n",
      "epoch_num :  621\n",
      "test_loss :  2.108322500137472\n",
      "train_loss :  1.5965413858522672\n",
      "epoch_num :  626\n",
      "test_loss :  2.1083613654897175\n",
      "train_loss :  1.596139555674302\n",
      "epoch_num :  631\n",
      "test_loss :  2.1083836224333705\n",
      "train_loss :  1.5957255933188776\n",
      "epoch_num :  636\n",
      "test_loss :  2.108388136921422\n",
      "train_loss :  1.5952986626640597\n",
      "epoch_num :  641\n",
      "test_loss :  2.1083736700358977\n",
      "train_loss :  1.5948578679378\n",
      "epoch_num :  646\n",
      "test_loss :  2.1083388606632205\n",
      "train_loss :  1.594402252029268\n",
      "epoch_num :  651\n",
      "test_loss :  2.108282204078513\n",
      "train_loss :  1.5939307943281433\n",
      "epoch_num :  656\n",
      "test_loss :  2.1082020251120768\n",
      "train_loss :  1.5934424081234912\n",
      "epoch_num :  661\n",
      "test_loss :  2.108096444153642\n",
      "train_loss :  1.592935937561625\n",
      "epoch_num :  666\n",
      "test_loss :  2.1079633337037413\n",
      "train_loss :  1.5924101540499027\n",
      "epoch_num :  671\n",
      "test_loss :  2.107800262459167\n",
      "train_loss :  1.5918637517541707\n",
      "epoch_num :  676\n",
      "test_loss :  2.1076044229786812\n",
      "train_loss :  1.5912953414181619\n",
      "epoch_num :  681\n",
      "test_loss :  2.1073725378070205\n",
      "train_loss :  1.5907034410863252\n",
      "epoch_num :  686\n",
      "test_loss :  2.107100737595032\n",
      "train_loss :  1.5900864614176717\n",
      "epoch_num :  691\n",
      "test_loss :  2.1067844034218206\n",
      "train_loss :  1.5894426821828378\n",
      "epoch_num :  696\n",
      "test_loss :  2.106417964571072\n",
      "train_loss :  1.588770215398968\n",
      "epoch_num :  701\n",
      "test_loss :  2.105994643077299\n",
      "train_loss :  1.5880669497010254\n",
      "epoch_num :  706\n",
      "test_loss :  2.105506138419549\n",
      "train_loss :  1.5873304704988922\n",
      "epoch_num :  711\n",
      "test_loss :  2.1049422512010416\n",
      "train_loss :  1.5865579519545587\n",
      "epoch_num :  716\n",
      "test_loss :  2.104290455435426\n",
      "train_loss :  1.5857460207211929\n",
      "epoch_num :  721\n",
      "test_loss :  2.103535447797059\n",
      "train_loss :  1.5848905987040862\n",
      "epoch_num :  726\n",
      "test_loss :  2.102658732468288\n",
      "train_loss :  1.5839867438641304\n",
      "epoch_num :  731\n",
      "test_loss :  2.101638346584526\n",
      "train_loss :  1.5830285252852108\n",
      "epoch_num :  736\n",
      "test_loss :  2.100448898109631\n",
      "train_loss :  1.5820089919826401\n",
      "epoch_num :  741\n",
      "test_loss :  2.0990621752607814\n",
      "train_loss :  1.5809203232853104\n",
      "epoch_num :  746\n",
      "test_loss :  2.0974486800290135\n",
      "train_loss :  1.579754276723366\n",
      "epoch_num :  751\n",
      "test_loss :  2.0955804923078483\n",
      "train_loss :  1.578503062700692\n",
      "epoch_num :  756\n",
      "test_loss :  2.0934357910142225\n",
      "train_loss :  1.5771607457630428\n",
      "epoch_num :  761\n",
      "test_loss :  2.091005007244744\n",
      "train_loss :  1.5757251617677843\n",
      "epoch_num :  766\n",
      "test_loss :  2.088297860460683\n",
      "train_loss :  1.5742001241431842\n",
      "epoch_num :  771\n",
      "test_loss :  2.085349543174225\n",
      "train_loss :  1.5725974083229246\n",
      "epoch_num :  776\n",
      "test_loss :  2.0822235661559834\n",
      "train_loss :  1.570937791466757\n",
      "epoch_num :  781\n",
      "test_loss :  2.079008997125524\n",
      "train_loss :  1.569250482508809\n",
      "epoch_num :  786\n",
      "test_loss :  2.0758114026622145\n",
      "train_loss :  1.567570698506539\n",
      "epoch_num :  791\n",
      "test_loss :  2.072739156573465\n",
      "train_loss :  1.5659357723931535\n",
      "epoch_num :  796\n",
      "test_loss :  2.069888623598077\n",
      "train_loss :  1.5643806913887959\n",
      "epoch_num :  801\n",
      "test_loss :  2.0673320915350573\n",
      "train_loss :  1.562934125413694\n",
      "epoch_num :  806\n",
      "test_loss :  2.065111188663139\n",
      "train_loss :  1.5616157940198336\n",
      "epoch_num :  811\n",
      "test_loss :  2.063236580409774\n",
      "train_loss :  1.5604355725568577\n",
      "epoch_num :  816\n",
      "test_loss :  2.0616928581108813\n",
      "train_loss :  1.5593942320017933\n",
      "epoch_num :  821\n",
      "test_loss :  2.0604463867072265\n",
      "train_loss :  1.558485316343199\n",
      "epoch_num :  826\n",
      "test_loss :  2.0594537201534275\n",
      "train_loss :  1.5576975042609669\n",
      "epoch_num :  831\n",
      "test_loss :  2.058668808520677\n",
      "train_loss :  1.5570168823831796\n",
      "epoch_num :  836\n",
      "test_loss :  2.058048135270413\n",
      "train_loss :  1.5564287763852607\n",
      "epoch_num :  841\n",
      "test_loss :  2.0575537130870063\n",
      "train_loss :  1.5559190180970726\n",
      "epoch_num :  846\n",
      "test_loss :  2.0571543343028016\n",
      "train_loss :  1.5554746909289305\n",
      "epoch_num :  851\n",
      "test_loss :  2.056825628773223\n",
      "train_loss :  1.5550844738931995\n",
      "epoch_num :  856\n",
      "test_loss :  2.056549438881607\n",
      "train_loss :  1.5547387171615294\n",
      "epoch_num :  861\n",
      "test_loss :  2.0563128931877337\n",
      "train_loss :  1.5544293603939403\n",
      "epoch_num :  866\n",
      "test_loss :  2.0561074236584678\n",
      "train_loss :  1.5541497730809677\n",
      "epoch_num :  871\n",
      "test_loss :  2.0559278619674624\n",
      "train_loss :  1.5538945669844901\n",
      "epoch_num :  876\n",
      "test_loss :  2.055771676108555\n",
      "train_loss :  1.5536594090927145\n",
      "epoch_num :  881\n",
      "test_loss :  2.0556383644750333\n",
      "train_loss :  1.5534408493187903\n",
      "epoch_num :  886\n",
      "test_loss :  2.0555290018471286\n",
      "train_loss :  1.5532361687172818\n",
      "epoch_num :  891\n",
      "test_loss :  2.0554459224943833\n",
      "train_loss :  1.5530432493330182\n",
      "epoch_num :  896\n",
      "test_loss :  2.055392524208507\n",
      "train_loss :  1.5528604644150878\n",
      "epoch_num :  901\n",
      "test_loss :  2.0553731801143793\n",
      "train_loss :  1.5526865866122463\n",
      "epoch_num :  906\n",
      "test_loss :  2.055393250822896\n",
      "train_loss :  1.5525207112994397\n",
      "epoch_num :  911\n",
      "test_loss :  2.0554591973367486\n",
      "train_loss :  1.5523621920070239\n",
      "epoch_num :  916\n",
      "test_loss :  2.0555788054702298\n",
      "train_loss :  1.5522105848184378\n",
      "epoch_num :  921\n",
      "test_loss :  2.055761546675146\n",
      "train_loss :  1.5520655984257532\n",
      "epoch_num :  926\n",
      "test_loss :  2.05601912054511\n",
      "train_loss :  1.5519270461866363\n",
      "epoch_num :  931\n",
      "test_loss :  2.056366255198608\n",
      "train_loss :  1.551794795978244\n",
      "epoch_num :  936\n",
      "test_loss :  2.0568218904895157\n",
      "train_loss :  1.5516687130631408\n",
      "epoch_num :  941\n",
      "test_loss :  2.0574109475599713\n",
      "train_loss :  1.5515485913873182\n",
      "epoch_num :  946\n",
      "test_loss :  2.0581670153059424\n",
      "train_loss :  1.5514340724813267\n",
      "epoch_num :  951\n",
      "test_loss :  2.0591364844502387\n",
      "train_loss :  1.55132456665966\n",
      "epoch_num :  956\n",
      "test_loss :  2.0603849475316522\n",
      "train_loss :  1.551219240981601\n",
      "epoch_num :  961\n",
      "test_loss :  2.0620069850170433\n",
      "train_loss :  1.5511172770569928\n",
      "epoch_num :  966\n",
      "test_loss :  2.064140325195686\n",
      "train_loss :  1.551018932470183\n",
      "epoch_num :  971\n",
      "test_loss :  2.0669830732126306\n",
      "train_loss :  1.550928523030533\n",
      "epoch_num :  976\n",
      "test_loss :  2.0708030459222737\n",
      "train_loss :  1.550860683829966\n",
      "epoch_num :  981\n",
      "test_loss :  2.0759000547146256\n",
      "train_loss :  1.5508483678095442\n",
      "epoch_num :  986\n",
      "test_loss :  2.082431524074131\n",
      "train_loss :  1.5509412854138835\n",
      "epoch_num :  991\n",
      "test_loss :  2.090038548095772\n",
      "train_loss :  1.551176808016642\n",
      "epoch_num :  996\n"
     ]
    }
   ],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "biases1_momentum = 0\n",
    "biases2_momentum = 0\n",
    "weights1_momentum = 0\n",
    "weights2_momentum = 0\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "    if epoch_num%5==1:\n",
    "        print(\"test_loss : \", test_losses[-1])\n",
    "        print(\"train_loss : \", losses[-1])\n",
    "        print(\"epoch_num : \", epoch_num)\n",
    "    for sample_num in range(x_train.shape[1]):\n",
    "#         print(\"sample_num : \", sample_num)\n",
    "        #Forward propagation\n",
    "        out1_temp = np.matmul(weights1.T, x_train[:,sample_num])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        #Back-propogation\n",
    "        error_out = out2-y_train[:, sample_num]\n",
    "        err_2 = np.matrix(error_out)\n",
    "        grad_2 = np.matmul(out1, err_2.T)\n",
    "        err_1 = np.multiply(np.matmul(weights2, err_2), np.multiply(out1, np.ones(out1.shape)-out1))\n",
    "        grad_1 = np.multiply(x_train[:, sample_num], err_1.T)\n",
    "        #Updating the weights\n",
    "        \n",
    "        biases1_momentum = alpha*biases1_momentum-learning_rate*err_1\n",
    "        biases2_momentum = alpha*biases2_momentum-learning_rate*err_2\n",
    "        weights1_momentum = alpha*weights1_momentum-learning_rate*grad_1\n",
    "        weights2_momentum = alpha*weights2_momentum-learning_rate*grad_2\n",
    "        \n",
    "        biases1 = biases1 + biases1_momentum\n",
    "        biases2 = biases2 + biases2_momentum\n",
    "        weights1 = weights1 + weights1_momentum\n",
    "        weights2 = weights2 + weights2_momentum\n",
    "        #Updating the accuracy and loss\n",
    "        #End of for loop\n",
    "#     print(\"accuracy : \", np.mean(acc_arr))\n",
    "    test_losses.append(test_loss(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_test.append(acc_out(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_train.append(acc_out(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    losses.append(test_loss(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot(211)\n",
    "plt.plot(losses[1:], label = 'train_loss')\n",
    "plt.plot(test_losses[1:], label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.multiply(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test, pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.23178875 17.35596776 39.35655612 18.87411929 24.40337259]\n",
      " [14.25366703 24.8146826  40.92548352 21.94086636 27.17937146]]\n",
      "[[11.7  19.95 38.57 19.36 24.29]\n",
      " [13.88 21.97 43.86 22.73 25.95]]\n"
     ]
    }
   ],
   "source": [
    "ind = 101\n",
    "print(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)[:,5:10])\n",
    "print(y_test[:,5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.551452353596725\n"
     ]
    }
   ],
   "source": [
    "print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
