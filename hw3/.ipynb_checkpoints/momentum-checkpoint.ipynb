{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def test_loss(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out2 = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "        loss = []\n",
    "        for iter in range(out2.shape[1]):\n",
    "            loss.append(MSE(out2[:, iter], y_in[:, iter]))\n",
    "        return np.mean(loss)\n",
    "\n",
    "def pred_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out1_temp = np.matmul(weights1.T, x_in[:,:])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        return out2\n",
    "\n",
    "def acc_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "    pred = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "    sum_acc = 0\n",
    "    for iter in range(pred.shape[1]):\n",
    "#         print(iter)\n",
    "        sum_acc+=(np.argmax(pred[:, iter])==np.argmax(y_in[:, iter]))\n",
    "    return sum_acc/y_in.shape[1]\n",
    "\n",
    "def MSE(yHat, y):\n",
    "    return np.sum(np.multiply(yHat - y, yHat - y)) / y.size\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-5):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.mean(np.multiply(np.log(predictions),targets)) \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 10)\n"
     ]
    }
   ],
   "source": [
    "##Load and segregate the data\n",
    "letters = genfromtxt('energy_eff2.csv', delimiter=',')\n",
    "# np.random.shuffle(letters)\n",
    "print(letters.shape)\n",
    "x_in = letters[:, 0:8]\n",
    "x_mean = np.mean(x_in, axis = 0)\n",
    "x_std = np.std(x_in, axis = 0)\n",
    "y_in = letters[:, 8:10]\n",
    "y_mean = np.mean(y_in, axis=0)\n",
    "y_std = np.std(y_in, axis = 0)\n",
    "# y_in = (y_in-y_mean)/y_std\n",
    "x_in = (x_in-x_mean)/x_std\n",
    "# print(np.std(y_in, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_in[0:int(x_in.shape[0]*0.7), :]\n",
    "y_train = y_in[0:int(x_in.shape[0]*0.7), :]\n",
    "\n",
    "x_test = x_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]\n",
    "y_test = y_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "alpha = 0.9\n",
    "\n",
    "weights1 = np.random.normal(size=(x_in.shape[1], hidden_size))\n",
    "weights2 = np.random.normal(size=(hidden_size, y_in.shape[1]))\n",
    "print(weights1.shape)\n",
    "print(weights2.shape)\n",
    "biases1 = np.random.normal(size=(hidden_size, 1))\n",
    "biases2 = np.random.normal(size=(y_in.shape[1], 1))\n",
    "\n",
    "x_train = np.matrix(x_train)\n",
    "x_train = x_train.T\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = np.matrix(x_test)\n",
    "x_test = x_test.T\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  9.069093471287314\n",
      "train_loss :  9.27993605666209\n",
      "epoch_num :  1\n",
      "test_loss :  4.815855731291473\n",
      "train_loss :  4.9604313910594975\n",
      "epoch_num :  6\n",
      "test_loss :  3.6356578345513815\n",
      "train_loss :  3.544063661294256\n",
      "epoch_num :  11\n",
      "test_loss :  3.0756540687971112\n",
      "train_loss :  2.9774033718755275\n",
      "epoch_num :  16\n",
      "test_loss :  2.470870979294682\n",
      "train_loss :  2.5002937386751287\n",
      "epoch_num :  21\n",
      "test_loss :  2.203218536684876\n",
      "train_loss :  2.2579235797578994\n",
      "epoch_num :  26\n",
      "test_loss :  1.9826730880384933\n",
      "train_loss :  2.0616000855293146\n",
      "epoch_num :  31\n",
      "test_loss :  1.7213925930855176\n",
      "train_loss :  1.8597632171847425\n",
      "epoch_num :  36\n",
      "test_loss :  1.561453784458354\n",
      "train_loss :  1.7101177534036065\n",
      "epoch_num :  41\n",
      "test_loss :  1.4952151405703333\n",
      "train_loss :  1.614093038144952\n",
      "epoch_num :  46\n",
      "test_loss :  1.4823623355319173\n",
      "train_loss :  1.5818382265639213\n",
      "epoch_num :  51\n",
      "test_loss :  1.4775974097445848\n",
      "train_loss :  1.56687816664376\n",
      "epoch_num :  56\n",
      "test_loss :  1.4748273518516484\n",
      "train_loss :  1.558156833261316\n",
      "epoch_num :  61\n",
      "test_loss :  1.472631053478382\n",
      "train_loss :  1.5521647980658613\n",
      "epoch_num :  66\n",
      "test_loss :  1.4704716079996099\n",
      "train_loss :  1.5474567556165946\n",
      "epoch_num :  71\n",
      "test_loss :  1.4681840317741608\n",
      "train_loss :  1.543397480262463\n",
      "epoch_num :  76\n",
      "test_loss :  1.4657657481547932\n",
      "train_loss :  1.539710655765031\n",
      "epoch_num :  81\n",
      "test_loss :  1.463262873373258\n",
      "train_loss :  1.5362744996390072\n",
      "epoch_num :  86\n",
      "test_loss :  1.460721593885933\n",
      "train_loss :  1.5330288662941873\n",
      "epoch_num :  91\n",
      "test_loss :  1.4581713258563769\n",
      "train_loss :  1.529935107372125\n",
      "epoch_num :  96\n",
      "test_loss :  1.45562030574638\n",
      "train_loss :  1.5269595489011953\n",
      "epoch_num :  101\n",
      "test_loss :  1.4530563487354269\n",
      "train_loss :  1.52406767750456\n",
      "epoch_num :  106\n",
      "test_loss :  1.4504510185129633\n",
      "train_loss :  1.5212238147883321\n",
      "epoch_num :  111\n",
      "test_loss :  1.4477669713896875\n",
      "train_loss :  1.5183940108674299\n",
      "epoch_num :  116\n",
      "test_loss :  1.444967716348816\n",
      "train_loss :  1.5155506092401454\n",
      "epoch_num :  121\n",
      "test_loss :  1.442027781915306\n",
      "train_loss :  1.5126769003383935\n",
      "epoch_num :  126\n",
      "test_loss :  1.4389407492498216\n",
      "train_loss :  1.509770473397691\n",
      "epoch_num :  131\n",
      "test_loss :  1.4357237083541914\n",
      "train_loss :  1.5068446354206904\n",
      "epoch_num :  136\n",
      "test_loss :  1.432418697982111\n",
      "train_loss :  1.5039280769146548\n",
      "epoch_num :  141\n",
      "test_loss :  1.4290926024098596\n",
      "train_loss :  1.501062963911851\n",
      "epoch_num :  146\n",
      "test_loss :  1.425835200863434\n",
      "train_loss :  1.4983006455293821\n",
      "epoch_num :  151\n",
      "test_loss :  1.4227512387437236\n",
      "train_loss :  1.4956934886860769\n",
      "epoch_num :  156\n",
      "test_loss :  1.4199411069408643\n",
      "train_loss :  1.493283053761621\n",
      "epoch_num :  161\n",
      "test_loss :  1.417472286702499\n",
      "train_loss :  1.4910884950477181\n",
      "epoch_num :  166\n",
      "test_loss :  1.4153564567078647\n",
      "train_loss :  1.4891006189037517\n",
      "epoch_num :  171\n",
      "test_loss :  1.413546951604415\n",
      "train_loss :  1.4872840309622346\n",
      "epoch_num :  176\n",
      "test_loss :  1.411952596871674\n",
      "train_loss :  1.4855839187732092\n",
      "epoch_num :  181\n",
      "test_loss :  1.4104484221253966\n",
      "train_loss :  1.4839281983983577\n",
      "epoch_num :  186\n",
      "test_loss :  1.4088612542562204\n",
      "train_loss :  1.48221004952142\n",
      "epoch_num :  191\n",
      "test_loss :  1.4068905311569408\n",
      "train_loss :  1.4802124973267996\n",
      "epoch_num :  196\n",
      "test_loss :  1.4037977370731878\n",
      "train_loss :  1.4773023704267612\n",
      "epoch_num :  201\n",
      "test_loss :  1.3969985777151501\n",
      "train_loss :  1.4709593257091031\n",
      "epoch_num :  206\n",
      "test_loss :  1.3801563833317576\n",
      "train_loss :  1.4545603111356749\n",
      "epoch_num :  211\n",
      "test_loss :  1.3652901632283758\n",
      "train_loss :  1.4380674208342075\n",
      "epoch_num :  216\n",
      "test_loss :  1.3578517391911693\n",
      "train_loss :  1.4268103431054229\n",
      "epoch_num :  221\n",
      "test_loss :  1.352602254093387\n",
      "train_loss :  1.418160252062514\n",
      "epoch_num :  226\n",
      "test_loss :  1.3482425410654908\n",
      "train_loss :  1.4113756230740457\n",
      "epoch_num :  231\n",
      "test_loss :  1.3445129292219784\n",
      "train_loss :  1.4060318437330352\n",
      "epoch_num :  236\n",
      "test_loss :  1.3412513865349283\n",
      "train_loss :  1.4017439088627075\n",
      "epoch_num :  241\n",
      "test_loss :  1.3383021031652123\n",
      "train_loss :  1.3981920473578011\n",
      "epoch_num :  246\n",
      "test_loss :  1.3355460451800971\n",
      "train_loss :  1.3951379914064639\n",
      "epoch_num :  251\n",
      "test_loss :  1.332921442374123\n",
      "train_loss :  1.3924192456292683\n",
      "epoch_num :  256\n",
      "test_loss :  1.330417788264012\n",
      "train_loss :  1.3899332447463029\n",
      "epoch_num :  261\n",
      "test_loss :  1.3280505810437744\n",
      "train_loss :  1.3876182692452206\n",
      "epoch_num :  266\n",
      "test_loss :  1.3258357942638468\n",
      "train_loss :  1.385436800285636\n",
      "epoch_num :  271\n",
      "test_loss :  1.3237776573633204\n",
      "train_loss :  1.3833641612583694\n",
      "epoch_num :  276\n",
      "test_loss :  1.3218685756141049\n",
      "train_loss :  1.3813818573387269\n",
      "epoch_num :  281\n",
      "test_loss :  1.3200937672186341\n",
      "train_loss :  1.3794741268573585\n",
      "epoch_num :  286\n",
      "test_loss :  1.3184357657890178\n",
      "train_loss :  1.3776265795949774\n",
      "epoch_num :  291\n",
      "test_loss :  1.3168774141332855\n",
      "train_loss :  1.3758260221451597\n",
      "epoch_num :  296\n",
      "test_loss :  1.3154034937117076\n",
      "train_loss :  1.3740607551379558\n",
      "epoch_num :  301\n",
      "test_loss :  1.314001429739505\n",
      "train_loss :  1.3723209095818212\n",
      "epoch_num :  306\n",
      "test_loss :  1.3126614420310474\n",
      "train_loss :  1.3705986522166989\n",
      "epoch_num :  311\n",
      "test_loss :  1.311376394497884\n",
      "train_loss :  1.3688882443900101\n",
      "epoch_num :  316\n",
      "test_loss :  1.3101414995236083\n",
      "train_loss :  1.3671859940810611\n",
      "epoch_num :  321\n",
      "test_loss :  1.308953964170494\n",
      "train_loss :  1.3654901410417755\n",
      "epoch_num :  326\n",
      "test_loss :  1.307812620614941\n",
      "train_loss :  1.3638006975781203\n",
      "epoch_num :  331\n",
      "test_loss :  1.3067175587262219\n",
      "train_loss :  1.3621192517515894\n",
      "epoch_num :  336\n",
      "test_loss :  1.3056697687634686\n",
      "train_loss :  1.3604487323966983\n",
      "epoch_num :  341\n",
      "test_loss :  1.304670801118817\n",
      "train_loss :  1.3587931362539072\n",
      "epoch_num :  346\n",
      "test_loss :  1.303722452801694\n",
      "train_loss :  1.3571572237707392\n",
      "epoch_num :  351\n",
      "test_loss :  1.3028264929753133\n",
      "train_loss :  1.3555461979707866\n",
      "epoch_num :  356\n",
      "test_loss :  1.3019844398820761\n",
      "train_loss :  1.3539653868383177\n",
      "epoch_num :  361\n",
      "test_loss :  1.301197398203955\n",
      "train_loss :  1.3524199516996116\n",
      "epoch_num :  366\n",
      "test_loss :  1.3004659600780706\n",
      "train_loss :  1.3509146414963737\n",
      "epoch_num :  371\n",
      "test_loss :  1.2997901663082798\n",
      "train_loss :  1.3494536065677227\n",
      "epoch_num :  376\n",
      "test_loss :  1.2991695185638041\n",
      "train_loss :  1.348040277473363\n",
      "epoch_num :  381\n",
      "test_loss :  1.2986030297595084\n",
      "train_loss :  1.3466773065549986\n",
      "epoch_num :  386\n",
      "test_loss :  1.298089298747174\n",
      "train_loss :  1.3453665638935017\n",
      "epoch_num :  391\n",
      "test_loss :  1.2976265965439377\n",
      "train_loss :  1.3441091757960437\n",
      "epoch_num :  396\n",
      "test_loss :  1.2972129538020558\n",
      "train_loss :  1.3429055928824227\n",
      "epoch_num :  401\n",
      "test_loss :  1.296846242242231\n",
      "train_loss :  1.3417556756710098\n",
      "epoch_num :  406\n",
      "test_loss :  1.2965242456731387\n",
      "train_loss :  1.3406587875486897\n",
      "epoch_num :  411\n",
      "test_loss :  1.2962447186119628\n",
      "train_loss :  1.3396138874604204\n",
      "epoch_num :  416\n",
      "test_loss :  1.296005432262468\n",
      "train_loss :  1.3386196170722062\n",
      "epoch_num :  421\n",
      "test_loss :  1.2958042087239368\n",
      "train_loss :  1.3376743792512973\n",
      "epoch_num :  426\n",
      "test_loss :  1.2956389449096701\n",
      "train_loss :  1.3367764063460172\n",
      "epoch_num :  431\n",
      "test_loss :  1.2955076278888102\n",
      "train_loss :  1.3359238179279636\n",
      "epoch_num :  436\n",
      "test_loss :  1.2954083433589312\n",
      "train_loss :  1.3351146684377193\n",
      "epoch_num :  441\n",
      "test_loss :  1.2953392788136793\n",
      "train_loss :  1.334346985634738\n",
      "epoch_num :  446\n",
      "test_loss :  1.2952987227640467\n",
      "train_loss :  1.3336188009763583\n",
      "epoch_num :  451\n",
      "test_loss :  1.295285061152724\n",
      "train_loss :  1.3329281731136238\n",
      "epoch_num :  456\n",
      "test_loss :  1.2952967718974768\n",
      "train_loss :  1.3322732056504787\n",
      "epoch_num :  461\n",
      "test_loss :  1.2953324183267378\n",
      "train_loss :  1.3316520602111548\n",
      "epoch_num :  466\n",
      "test_loss :  1.295390642133419\n",
      "train_loss :  1.3310629657275752\n",
      "epoch_num :  471\n",
      "test_loss :  1.2954701563700974\n",
      "train_loss :  1.3305042247138936\n",
      "epoch_num :  476\n",
      "test_loss :  1.2955697389336347\n",
      "train_loss :  1.3299742171509652\n",
      "epoch_num :  481\n",
      "test_loss :  1.295688226930481\n",
      "train_loss :  1.329471402467214\n",
      "epoch_num :  486\n",
      "test_loss :  1.2958245122621672\n",
      "train_loss :  1.3289943199786876\n",
      "epoch_num :  491\n",
      "test_loss :  1.2959775387102461\n",
      "train_loss :  1.3285415880445561\n",
      "epoch_num :  496\n",
      "test_loss :  1.2961463007159664\n",
      "train_loss :  1.3281119021089338\n",
      "epoch_num :  501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  1.296329843931531\n",
      "train_loss :  1.3277040317409075\n",
      "epoch_num :  506\n",
      "test_loss :  1.2965272674616122\n",
      "train_loss :  1.3273168167567477\n",
      "epoch_num :  511\n",
      "test_loss :  1.29673772752239\n",
      "train_loss :  1.3269491625149217\n",
      "epoch_num :  516\n",
      "test_loss :  1.296960442040493\n",
      "train_loss :  1.3266000345152156\n",
      "epoch_num :  521\n",
      "test_loss :  1.2971946955296394\n",
      "train_loss :  1.3262684525009794\n",
      "epoch_num :  526\n",
      "test_loss :  1.297439843460852\n",
      "train_loss :  1.3259534843440748\n",
      "epoch_num :  531\n",
      "test_loss :  1.2976953153232287\n",
      "train_loss :  1.3256542400641123\n",
      "epoch_num :  536\n",
      "test_loss :  1.297960615681154\n",
      "train_loss :  1.3253698663726052\n",
      "epoch_num :  541\n",
      "test_loss :  1.2982353227694459\n",
      "train_loss :  1.3250995421185028\n",
      "epoch_num :  546\n",
      "test_loss :  1.298519084494841\n",
      "train_loss :  1.3248424749335739\n",
      "epoch_num :  551\n",
      "test_loss :  1.2988116120711082\n",
      "train_loss :  1.3245978992406842\n",
      "epoch_num :  556\n",
      "test_loss :  1.29911267183328\n",
      "train_loss :  1.3243650756162995\n",
      "epoch_num :  561\n",
      "test_loss :  1.2994220759930937\n",
      "train_loss :  1.324143291324066\n",
      "epoch_num :  566\n",
      "test_loss :  1.2997396731768485\n",
      "train_loss :  1.32393186169317\n",
      "epoch_num :  571\n",
      "test_loss :  1.3000653395283066\n",
      "train_loss :  1.323730131930058\n",
      "epoch_num :  576\n",
      "test_loss :  1.3003989709926875\n",
      "train_loss :  1.3235374789367875\n",
      "epoch_num :  581\n",
      "test_loss :  1.300740477171189\n",
      "train_loss :  1.3233533127588188\n",
      "epoch_num :  586\n",
      "test_loss :  1.3010897768995164\n",
      "train_loss :  1.3231770773820664\n",
      "epoch_num :  591\n",
      "test_loss :  1.3014467954989524\n",
      "train_loss :  1.3230082507195011\n",
      "epoch_num :  596\n",
      "test_loss :  1.3018114634985765\n",
      "train_loss :  1.3228463437476399\n",
      "epoch_num :  601\n",
      "test_loss :  1.3021837165391665\n",
      "train_loss :  1.322690898855537\n",
      "epoch_num :  606\n",
      "test_loss :  1.3025634961367683\n",
      "train_loss :  1.322541487542517\n",
      "epoch_num :  611\n",
      "test_loss :  1.302950750993296\n",
      "train_loss :  1.322397707643824\n",
      "epoch_num :  616\n",
      "test_loss :  1.3033454385771621\n",
      "train_loss :  1.3222591802784744\n",
      "epoch_num :  621\n",
      "test_loss :  1.303747526744127\n",
      "train_loss :  1.3221255467074147\n",
      "epoch_num :  626\n",
      "test_loss :  1.3041569952170737\n",
      "train_loss :  1.3219964652700924\n",
      "epoch_num :  631\n",
      "test_loss :  1.3045738367856483\n",
      "train_loss :  1.3218716085405717\n",
      "epoch_num :  636\n",
      "test_loss :  1.3049980581197833\n",
      "train_loss :  1.3217506608157545\n",
      "epoch_num :  641\n",
      "test_loss :  1.3054296801137006\n",
      "train_loss :  1.3216333160219118\n",
      "epoch_num :  646\n",
      "test_loss :  1.3058687376901745\n",
      "train_loss :  1.321519276103201\n",
      "epoch_num :  651\n",
      "test_loss :  1.3063152790002612\n",
      "train_loss :  1.3214082499379007\n",
      "epoch_num :  656\n",
      "test_loss :  1.3067693639539055\n",
      "train_loss :  1.3212999528142682\n",
      "epoch_num :  661\n",
      "test_loss :  1.3072310620140588\n",
      "train_loss :  1.3211941064866108\n",
      "epoch_num :  666\n",
      "test_loss :  1.3077004491842819\n",
      "train_loss :  1.3210904398221888\n",
      "epoch_num :  671\n",
      "test_loss :  1.3081776041195858\n",
      "train_loss :  1.320988690038541\n",
      "epoch_num :  676\n",
      "test_loss :  1.3086626032955846\n",
      "train_loss :  1.3208886045170871\n",
      "epoch_num :  681\n",
      "test_loss :  1.309155515184831\n",
      "train_loss :  1.3207899431604948\n",
      "epoch_num :  686\n",
      "test_loss :  1.309656393414139\n",
      "train_loss :  1.320692481236898\n",
      "epoch_num :  691\n",
      "test_loss :  1.3101652689158183\n",
      "train_loss :  1.3205960126230616\n",
      "epoch_num :  696\n",
      "test_loss :  1.3106821411403322\n",
      "train_loss :  1.320500353321123\n",
      "epoch_num :  701\n",
      "test_loss :  1.3112069684695538\n",
      "train_loss :  1.3204053450817372\n",
      "epoch_num :  706\n",
      "test_loss :  1.3117396580564382\n",
      "train_loss :  1.3203108589235226\n",
      "epoch_num :  711\n",
      "test_loss :  1.312280055415226\n",
      "train_loss :  1.3202167983001034\n",
      "epoch_num :  716\n",
      "test_loss :  1.3128279341890021\n",
      "train_loss :  1.3201231016389896\n",
      "epoch_num :  721\n",
      "test_loss :  1.313382986617548\n",
      "train_loss :  1.3200297439682895\n",
      "epoch_num :  726\n",
      "test_loss :  1.313944815304776\n",
      "train_loss :  1.319936737365828\n",
      "epoch_num :  731\n",
      "test_loss :  1.3145129269253757\n",
      "train_loss :  1.3198441300154262\n",
      "epoch_num :  736\n",
      "test_loss :  1.31508672850009\n",
      "train_loss :  1.3197520037383155\n",
      "epoch_num :  741\n",
      "test_loss :  1.3156655267962132\n",
      "train_loss :  1.3196604699792933\n",
      "epoch_num :  746\n",
      "test_loss :  1.3162485312695107\n",
      "train_loss :  1.3195696643567043\n",
      "epoch_num :  751\n",
      "test_loss :  1.3168348607604432\n",
      "train_loss :  1.319479740016648\n",
      "epoch_num :  756\n",
      "test_loss :  1.3174235539063424\n",
      "train_loss :  1.3193908601459794\n",
      "epoch_num :  761\n",
      "test_loss :  1.3180135829574087\n",
      "train_loss :  1.319303190078138\n",
      "epoch_num :  766\n",
      "test_loss :  1.3186038704195175\n",
      "train_loss :  1.3192168894573608\n",
      "epoch_num :  771\n",
      "test_loss :  1.3191933077244\n",
      "train_loss :  1.319132104906077\n",
      "epoch_num :  776\n",
      "test_loss :  1.31978077497592\n",
      "train_loss :  1.3190489635718732\n",
      "epoch_num :  781\n",
      "test_loss :  1.320365160758534\n",
      "train_loss :  1.318967567827271\n",
      "epoch_num :  786\n",
      "test_loss :  1.3209453810258804\n",
      "train_loss :  1.3188879912758142\n",
      "epoch_num :  791\n",
      "test_loss :  1.3215203962051891\n",
      "train_loss :  1.3188102761002853\n",
      "epoch_num :  796\n",
      "test_loss :  1.3220892258368604\n",
      "train_loss :  1.3187344316884941\n",
      "epoch_num :  801\n",
      "test_loss :  1.3226509602920173\n",
      "train_loss :  1.3186604343991883\n",
      "epoch_num :  806\n",
      "test_loss :  1.3232047693448543\n",
      "train_loss :  1.318588228287575\n",
      "epoch_num :  811\n",
      "test_loss :  1.3237499075965937\n",
      "train_loss :  1.318517726595043\n",
      "epoch_num :  816\n",
      "test_loss :  1.3242857169340254\n",
      "train_loss :  1.318448813814\n",
      "epoch_num :  821\n",
      "test_loss :  1.3248116263467256\n",
      "train_loss :  1.3183813481592341\n",
      "epoch_num :  826\n",
      "test_loss :  1.3253271495192234\n",
      "train_loss :  1.3183151643044244\n",
      "epoch_num :  831\n",
      "test_loss :  1.3258318806597371\n",
      "train_loss :  1.3182500762707583\n",
      "epoch_num :  836\n",
      "test_loss :  1.326325489032516\n",
      "train_loss :  1.3181858803804822\n",
      "epoch_num :  841\n",
      "test_loss :  1.3268077126350206\n",
      "train_loss :  1.3181223582097428\n",
      "epoch_num :  846\n",
      "test_loss :  1.327278351413773\n",
      "train_loss :  1.3180592794918016\n",
      "epoch_num :  851\n",
      "test_loss :  1.3277372603528095\n",
      "train_loss :  1.3179964049343955\n",
      "epoch_num :  856\n",
      "test_loss :  1.3281843427031694\n",
      "train_loss :  1.317933488923956\n",
      "epoch_num :  861\n",
      "test_loss :  1.3286195435573656\n",
      "train_loss :  1.3178702820962973\n",
      "epoch_num :  866\n",
      "test_loss :  1.3290428439124184\n",
      "train_loss :  1.3178065337582952\n",
      "epoch_num :  871\n",
      "test_loss :  1.3294542553121234\n",
      "train_loss :  1.317741994149142\n",
      "epoch_num :  876\n",
      "test_loss :  1.3298538151142654\n",
      "train_loss :  1.3176764165330752\n",
      "epoch_num :  881\n",
      "test_loss :  1.330241582392501\n",
      "train_loss :  1.3176095591183925\n",
      "epoch_num :  886\n",
      "test_loss :  1.3306176344545788\n",
      "train_loss :  1.3175411868000828\n",
      "epoch_num :  891\n",
      "test_loss :  1.3309820639383703\n",
      "train_loss :  1.3174710727256995\n",
      "epoch_num :  896\n",
      "test_loss :  1.3313349764333329\n",
      "train_loss :  1.3173989996860023\n",
      "epoch_num :  901\n",
      "test_loss :  1.331676488566846\n",
      "train_loss :  1.3173247613335661\n",
      "epoch_num :  906\n",
      "test_loss :  1.3320067264911355\n",
      "train_loss :  1.317248163233839\n",
      "epoch_num :  911\n",
      "test_loss :  1.3323258247063168\n",
      "train_loss :  1.3171690237542486\n",
      "epoch_num :  916\n",
      "test_loss :  1.3326339251576345\n",
      "train_loss :  1.317087174797723\n",
      "epoch_num :  921\n",
      "test_loss :  1.332931176549252\n",
      "train_loss :  1.317002462387603\n",
      "epoch_num :  926\n",
      "test_loss :  1.3332177338227529\n",
      "train_loss :  1.3169147471112692\n",
      "epoch_num :  931\n",
      "test_loss :  1.3334937577548398\n",
      "train_loss :  1.3168239044302514\n",
      "epoch_num :  936\n",
      "test_loss :  1.3337594146354754\n",
      "train_loss :  1.3167298248645762\n",
      "epoch_num :  941\n",
      "test_loss :  1.3340148759943051\n",
      "train_loss :  1.3166324140594001\n",
      "epoch_num :  946\n",
      "test_loss :  1.3342603183497754\n",
      "train_loss :  1.3165315927419459\n",
      "epoch_num :  951\n",
      "test_loss :  1.3344959229610733\n",
      "train_loss :  1.3164272965768713\n",
      "epoch_num :  956\n",
      "test_loss :  1.3347218755685961\n",
      "train_loss :  1.3163194759282335\n",
      "epoch_num :  961\n",
      "test_loss :  1.3349383661130885\n",
      "train_loss :  1.3162080955361892\n",
      "epoch_num :  966\n",
      "test_loss :  1.3351455884278216\n",
      "train_loss :  1.316093134116608\n",
      "epoch_num :  971\n",
      "test_loss :  1.3353437399013028\n",
      "train_loss :  1.3159745838917725\n",
      "epoch_num :  976\n",
      "test_loss :  1.3355330211106917\n",
      "train_loss :  1.3158524500602393\n",
      "epoch_num :  981\n",
      "test_loss :  1.3357136354282528\n",
      "train_loss :  1.3157267502139243\n",
      "epoch_num :  986\n",
      "test_loss :  1.3358857886043565\n",
      "train_loss :  1.3155975137102656\n",
      "epoch_num :  991\n",
      "test_loss :  1.336049688331959\n",
      "train_loss :  1.3154647810072881\n",
      "epoch_num :  996\n"
     ]
    }
   ],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "biases1_momentum = 0\n",
    "biases2_momentum = 0\n",
    "weights1_momentum = 0\n",
    "weights2_momentum = 0\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "    if epoch_num%5==1:\n",
    "        print(\"test_loss : \", test_losses[-1])\n",
    "        print(\"train_loss : \", losses[-1])\n",
    "        print(\"epoch_num : \", epoch_num)\n",
    "    for sample_num in range(x_train.shape[1]):\n",
    "#         print(\"sample_num : \", sample_num)\n",
    "        #Forward propagation\n",
    "        out1_temp = np.matmul(weights1.T, x_train[:,sample_num])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        #Back-propogation\n",
    "        error_out = out2-y_train[:, sample_num]\n",
    "        err_2 = np.matrix(error_out)\n",
    "        grad_2 = np.matmul(out1, err_2.T)\n",
    "        err_1 = np.multiply(np.matmul(weights2, err_2), np.multiply(out1, np.ones(out1.shape)-out1))\n",
    "        grad_1 = np.multiply(x_train[:, sample_num], err_1.T)\n",
    "        #Updating the weights\n",
    "        \n",
    "        biases1_momentum = alpha*biases1_momentum-learning_rate*err_1\n",
    "        biases2_momentum = alpha*biases2_momentum-learning_rate*err_2\n",
    "        weights1_momentum = alpha*weights1_momentum-learning_rate*grad_1\n",
    "        weights2_momentum = alpha*weights2_momentum-learning_rate*grad_2\n",
    "        \n",
    "        biases1 = biases1 + biases1_momentum\n",
    "        biases2 = biases2 + biases2_momentum\n",
    "        weights1 = weights1 + weights1_momentum\n",
    "        weights2 = weights2 + weights2_momentum\n",
    "        #Updating the accuracy and loss\n",
    "        #End of for loop\n",
    "#     print(\"accuracy : \", np.mean(acc_arr))\n",
    "    test_losses.append(test_loss(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_test.append(acc_out(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_train.append(acc_out(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    losses.append(test_loss(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot(211)\n",
    "plt.plot(losses[1:], label = 'train_loss')\n",
    "plt.plot(test_losses[1:], label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.multiply(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test, pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26.89104596 12.50566327 10.61484629 29.03013825 28.95189688]\n",
      " [29.4824432  15.18912731 13.60200788 27.45600164 30.43696642]]\n",
      "[[26.48 12.16 10.35 29.71 29.47]\n",
      " [30.91 15.18 13.65 28.02 29.77]]\n"
     ]
    }
   ],
   "source": [
    "ind = 101\n",
    "print(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)[:,5:10])\n",
    "print(y_test[:,5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
