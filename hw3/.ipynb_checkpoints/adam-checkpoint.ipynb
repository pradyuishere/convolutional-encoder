{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def test_loss(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out2 = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "        loss = []\n",
    "        for iter in range(out2.shape[1]):\n",
    "            loss.append(MSE(out2[:, iter], y_in[:, iter]))\n",
    "        return np.mean(loss)\n",
    "\n",
    "def pred_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out1_temp = np.matmul(weights1.T, x_in[:,:])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        return out2\n",
    "\n",
    "def acc_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "    pred = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "    sum_acc = 0\n",
    "    for iter in range(pred.shape[1]):\n",
    "#         print(iter)\n",
    "        sum_acc+=(np.argmax(pred[:, iter])==np.argmax(y_in[:, iter]))\n",
    "    return sum_acc/y_in.shape[1]\n",
    "\n",
    "def MSE(yHat, y):\n",
    "    return np.sum(np.multiply(yHat - y, yHat - y)) / y.size\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-5):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.mean(np.multiply(np.log(predictions),targets)) \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 10)\n"
     ]
    }
   ],
   "source": [
    "##Load and segregate the data\n",
    "letters = genfromtxt('energy_eff2.csv', delimiter=',')\n",
    "# np.random.shuffle(letters)\n",
    "print(letters.shape)\n",
    "x_in = letters[:, 0:8]\n",
    "x_mean = np.mean(x_in, axis = 0)\n",
    "x_std = np.std(x_in, axis = 0)\n",
    "y_in = letters[:, 8:10]\n",
    "y_mean = np.mean(y_in, axis=0)\n",
    "y_std = np.std(y_in, axis = 0)\n",
    "# y_in = (y_in-y_mean)/y_std\n",
    "x_in = (x_in-x_mean)/x_std\n",
    "# print(np.std(y_in, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_in[0:int(x_in.shape[0]*0.7), :]\n",
    "y_train = y_in[0:int(x_in.shape[0]*0.7), :]\n",
    "\n",
    "x_test = x_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]\n",
    "y_test = y_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "alpha = 0.999\n",
    "alpha1 = 0.9\n",
    "epsilon = 1e-8\n",
    "\n",
    "weights1 = np.random.normal(size=(x_in.shape[1], hidden_size))\n",
    "weights2 = np.random.normal(size=(hidden_size, y_in.shape[1]))\n",
    "print(weights1.shape)\n",
    "print(weights2.shape)\n",
    "biases1 = np.random.normal(size=(hidden_size, 1))\n",
    "biases2 = np.random.normal(size=(y_in.shape[1], 1))\n",
    "\n",
    "x_train = np.matrix(x_train)\n",
    "x_train = x_train.T\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = np.matrix(x_test)\n",
    "x_test = x_test.T\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  318.4257090952691\n",
      "train_loss :  311.2534752172193\n",
      "epoch_num :  1\n",
      "test_loss :  25.145800548450943\n",
      "train_loss :  23.519098912258\n",
      "epoch_num :  6\n",
      "test_loss :  15.284308414729216\n",
      "train_loss :  12.322216432605128\n",
      "epoch_num :  11\n",
      "test_loss :  10.935657521730336\n",
      "train_loss :  9.661060842492578\n",
      "epoch_num :  16\n",
      "test_loss :  9.701450268317604\n",
      "train_loss :  8.778062470636902\n",
      "epoch_num :  21\n",
      "test_loss :  9.11454124833586\n",
      "train_loss :  8.334651970709219\n",
      "epoch_num :  26\n",
      "test_loss :  8.557712984602887\n",
      "train_loss :  7.913290863403184\n",
      "epoch_num :  31\n",
      "test_loss :  8.008250387919828\n",
      "train_loss :  7.463408142915387\n",
      "epoch_num :  36\n",
      "test_loss :  7.517728588378196\n",
      "train_loss :  7.027973753509007\n",
      "epoch_num :  41\n",
      "test_loss :  7.103318769394258\n",
      "train_loss :  6.63886660459129\n",
      "epoch_num :  46\n",
      "test_loss :  6.756706883682766\n",
      "train_loss :  6.301985804984806\n",
      "epoch_num :  51\n",
      "test_loss :  6.366886942620526\n",
      "train_loss :  5.929367743739293\n",
      "epoch_num :  56\n",
      "test_loss :  5.927841814534466\n",
      "train_loss :  5.505152032239648\n",
      "epoch_num :  61\n",
      "test_loss :  5.573504344764659\n",
      "train_loss :  5.148377001225765\n",
      "epoch_num :  66\n",
      "test_loss :  5.266063329299576\n",
      "train_loss :  4.838602845577382\n",
      "epoch_num :  71\n",
      "test_loss :  4.989357796597121\n",
      "train_loss :  4.56051463157918\n",
      "epoch_num :  76\n",
      "test_loss :  4.739345292216015\n",
      "train_loss :  4.308446969293903\n",
      "epoch_num :  81\n",
      "test_loss :  4.512445581236735\n",
      "train_loss :  4.079054541102541\n",
      "epoch_num :  86\n",
      "test_loss :  4.304775346171614\n",
      "train_loss :  3.8689430765799693\n",
      "epoch_num :  91\n",
      "test_loss :  4.111942601041848\n",
      "train_loss :  3.674009616663957\n",
      "epoch_num :  96\n",
      "test_loss :  3.928727872873054\n",
      "train_loss :  3.4897596346513486\n",
      "epoch_num :  101\n",
      "test_loss :  3.7512961360469643\n",
      "train_loss :  3.312803786883723\n",
      "epoch_num :  106\n",
      "test_loss :  3.584020779903958\n",
      "train_loss :  3.1441458204872195\n",
      "epoch_num :  111\n",
      "test_loss :  3.4379168882691302\n",
      "train_loss :  2.990980412187955\n",
      "epoch_num :  116\n",
      "test_loss :  3.3160467520642385\n",
      "train_loss :  2.859242554883896\n",
      "epoch_num :  121\n",
      "test_loss :  3.2132064581707387\n",
      "train_loss :  2.7474999982083763\n",
      "epoch_num :  126\n",
      "test_loss :  3.124274515558483\n",
      "train_loss :  2.6515344918270665\n",
      "epoch_num :  131\n",
      "test_loss :  3.0458000159910132\n",
      "train_loss :  2.5679060145318116\n",
      "epoch_num :  136\n",
      "test_loss :  2.9755491713744338\n",
      "train_loss :  2.494260621465764\n",
      "epoch_num :  141\n",
      "test_loss :  2.912127492618292\n",
      "train_loss :  2.428888497101256\n",
      "epoch_num :  146\n",
      "test_loss :  2.854562503193852\n",
      "train_loss :  2.3704266250860355\n",
      "epoch_num :  151\n",
      "test_loss :  2.8020388920408674\n",
      "train_loss :  2.317749537011584\n",
      "epoch_num :  156\n",
      "test_loss :  2.7538289281241957\n",
      "train_loss :  2.26992795758192\n",
      "epoch_num :  161\n",
      "test_loss :  2.7093083509114213\n",
      "train_loss :  2.226206530943803\n",
      "epoch_num :  166\n",
      "test_loss :  2.6679821610010963\n",
      "train_loss :  2.1859949443264104\n",
      "epoch_num :  171\n",
      "test_loss :  2.6294873383599726\n",
      "train_loss :  2.1488612433062433\n",
      "epoch_num :  176\n",
      "test_loss :  2.5935570721571213\n",
      "train_loss :  2.1145013002015083\n",
      "epoch_num :  181\n",
      "test_loss :  2.559961092716823\n",
      "train_loss :  2.082677361665097\n",
      "epoch_num :  186\n",
      "test_loss :  2.5284608480137316\n",
      "train_loss :  2.0531586420998877\n",
      "epoch_num :  191\n",
      "test_loss :  2.4987990325047247\n",
      "train_loss :  2.0256953735013097\n",
      "epoch_num :  196\n",
      "test_loss :  2.4707092173352967\n",
      "train_loss :  2.000021783787837\n",
      "epoch_num :  201\n",
      "test_loss :  2.4439221929346866\n",
      "train_loss :  1.9758660399170105\n",
      "epoch_num :  206\n",
      "test_loss :  2.4181501206743485\n",
      "train_loss :  1.9529492356926403\n",
      "epoch_num :  211\n",
      "test_loss :  2.393022600299391\n",
      "train_loss :  1.9309577654364416\n",
      "epoch_num :  216\n",
      "test_loss :  2.3679253434892753\n",
      "train_loss :  1.9094708224317183\n",
      "epoch_num :  221\n",
      "test_loss :  2.3417627022961467\n",
      "train_loss :  1.8878797175364643\n",
      "epoch_num :  226\n",
      "test_loss :  2.3133495904370407\n",
      "train_loss :  1.8656542116298944\n",
      "epoch_num :  231\n",
      "test_loss :  2.283626037138614\n",
      "train_loss :  1.8433825527166192\n",
      "epoch_num :  236\n",
      "test_loss :  2.2557769841657898\n",
      "train_loss :  1.8226991995985924\n",
      "epoch_num :  241\n",
      "test_loss :  2.2310724312912913\n",
      "train_loss :  1.8042471270769316\n",
      "epoch_num :  246\n",
      "test_loss :  2.2088888278363075\n",
      "train_loss :  1.7875845971726254\n",
      "epoch_num :  251\n",
      "test_loss :  2.1885519095043633\n",
      "train_loss :  1.772187378669651\n",
      "epoch_num :  256\n",
      "test_loss :  2.169610976007551\n",
      "train_loss :  1.7577233116772275\n",
      "epoch_num :  261\n",
      "test_loss :  2.1517459727023005\n",
      "train_loss :  1.7439987180055387\n",
      "epoch_num :  266\n",
      "test_loss :  2.134732868858074\n",
      "train_loss :  1.730895364456972\n",
      "epoch_num :  271\n",
      "test_loss :  2.1184228493392134\n",
      "train_loss :  1.7183351865702758\n",
      "epoch_num :  276\n",
      "test_loss :  2.1027199595492436\n",
      "train_loss :  1.7062621164185086\n",
      "epoch_num :  281\n",
      "test_loss :  2.0875616554336083\n",
      "train_loss :  1.6946325086444975\n",
      "epoch_num :  286\n",
      "test_loss :  2.0729046905764537\n",
      "train_loss :  1.6834098898658065\n",
      "epoch_num :  291\n",
      "test_loss :  2.0587159418524466\n",
      "train_loss :  1.6725620019864194\n",
      "epoch_num :  296\n",
      "test_loss :  2.044966911459562\n",
      "train_loss :  1.662059140050265\n",
      "epoch_num :  301\n",
      "test_loss :  2.0316307010282846\n",
      "train_loss :  1.6518732501471547\n",
      "epoch_num :  306\n",
      "test_loss :  2.018680536709805\n",
      "train_loss :  1.6419774743023507\n",
      "epoch_num :  311\n",
      "test_loss :  2.006089199485917\n",
      "train_loss :  1.6323459473478692\n",
      "epoch_num :  316\n",
      "test_loss :  1.9938289332344874\n",
      "train_loss :  1.622953721646388\n",
      "epoch_num :  321\n",
      "test_loss :  1.9818715651282446\n",
      "train_loss :  1.6137767405389816\n",
      "epoch_num :  326\n",
      "test_loss :  1.970188688929106\n",
      "train_loss :  1.6047918091533406\n",
      "epoch_num :  331\n",
      "test_loss :  1.958751842412113\n",
      "train_loss :  1.5959765264038257\n",
      "epoch_num :  336\n",
      "test_loss :  1.9475326653998588\n",
      "train_loss :  1.5873091479861166\n",
      "epoch_num :  341\n",
      "test_loss :  1.936503063485122\n",
      "train_loss :  1.5787683492915847\n",
      "epoch_num :  346\n",
      "test_loss :  1.9256354320709834\n",
      "train_loss :  1.5703328504610001\n",
      "epoch_num :  351\n",
      "test_loss :  1.9149030220493501\n",
      "train_loss :  1.5619808523194065\n",
      "epoch_num :  356\n",
      "test_loss :  1.9042805557643774\n",
      "train_loss :  1.5536892081704807\n",
      "epoch_num :  361\n",
      "test_loss :  1.893745227093694\n",
      "train_loss :  1.5454322167042691\n",
      "epoch_num :  366\n",
      "test_loss :  1.8832782265511263\n",
      "train_loss :  1.5371798629591173\n",
      "epoch_num :  371\n",
      "test_loss :  1.8728668820712862\n",
      "train_loss :  1.528895278574434\n",
      "epoch_num :  376\n",
      "test_loss :  1.8625073264077439\n",
      "train_loss :  1.520531243348852\n",
      "epoch_num :  381\n",
      "test_loss :  1.852207184523787\n",
      "train_loss :  1.5120260119338857\n",
      "epoch_num :  386\n",
      "test_loss :  1.8419869892721716\n",
      "train_loss :  1.503300218082587\n",
      "epoch_num :  391\n",
      "test_loss :  1.8318778342068096\n",
      "train_loss :  1.4942595038852668\n",
      "epoch_num :  396\n",
      "test_loss :  1.821911750566216\n",
      "train_loss :  1.4848098980181492\n",
      "epoch_num :  401\n",
      "test_loss :  1.8121033777502797\n",
      "train_loss :  1.4748876785007528\n",
      "epoch_num :  406\n",
      "test_loss :  1.802432741964387\n",
      "train_loss :  1.464487785939624\n",
      "epoch_num :  411\n",
      "test_loss :  1.7928577973303668\n",
      "train_loss :  1.4536670689870277\n",
      "epoch_num :  416\n",
      "test_loss :  1.783382707142908\n",
      "train_loss :  1.4425342884759988\n",
      "epoch_num :  421\n",
      "test_loss :  1.7741517936458027\n",
      "train_loss :  1.4312722242211529\n",
      "epoch_num :  426\n",
      "test_loss :  1.7654666928096623\n",
      "train_loss :  1.420172880639307\n",
      "epoch_num :  431\n",
      "test_loss :  1.7576549041581135\n",
      "train_loss :  1.409581340578782\n",
      "epoch_num :  436\n",
      "test_loss :  1.7508930970906067\n",
      "train_loss :  1.3997493556948066\n",
      "epoch_num :  441\n",
      "test_loss :  1.7451531194961605\n",
      "train_loss :  1.3907516735004521\n",
      "epoch_num :  446\n",
      "test_loss :  1.7402737235162593\n",
      "train_loss :  1.3825242381606822\n",
      "epoch_num :  451\n",
      "test_loss :  1.7360559521306098\n",
      "train_loss :  1.3749468593562675\n",
      "epoch_num :  456\n",
      "test_loss :  1.7323229811200938\n",
      "train_loss :  1.3679028305753864\n",
      "epoch_num :  461\n",
      "test_loss :  1.728941449307308\n",
      "train_loss :  1.3613028825161102\n",
      "epoch_num :  466\n",
      "test_loss :  1.7258201500324477\n",
      "train_loss :  1.3550863064838599\n",
      "epoch_num :  471\n",
      "test_loss :  1.722900186540212\n",
      "train_loss :  1.3492134169114096\n",
      "epoch_num :  476\n",
      "test_loss :  1.7201444016569531\n",
      "train_loss :  1.3436574013281513\n",
      "epoch_num :  481\n",
      "test_loss :  1.717529038654445\n",
      "train_loss :  1.338398343887286\n",
      "epoch_num :  486\n",
      "test_loss :  1.7150380648767989\n",
      "train_loss :  1.333419588159876\n",
      "epoch_num :  491\n",
      "test_loss :  1.7126596572607728\n",
      "train_loss :  1.3287058037315977\n",
      "epoch_num :  496\n",
      "test_loss :  1.7103841864894145\n",
      "train_loss :  1.3242421016358188\n",
      "epoch_num :  501\n",
      "test_loss :  1.7082031474727073\n",
      "train_loss :  1.320013729452461\n",
      "epoch_num :  506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  1.7061086462569552\n",
      "train_loss :  1.316006055497826\n",
      "epoch_num :  511\n",
      "test_loss :  1.7040931917416908\n",
      "train_loss :  1.312204676416633\n",
      "epoch_num :  516\n",
      "test_loss :  1.702149639846557\n",
      "train_loss :  1.3085955598015209\n",
      "epoch_num :  521\n",
      "test_loss :  1.7002712031191891\n",
      "train_loss :  1.3051651784330394\n",
      "epoch_num :  526\n",
      "test_loss :  1.6984514793998842\n",
      "train_loss :  1.3019006177700312\n",
      "epoch_num :  531\n",
      "test_loss :  1.696684477227687\n",
      "train_loss :  1.2987896516262019\n",
      "epoch_num :  536\n",
      "test_loss :  1.6949646292293985\n",
      "train_loss :  1.295820787560926\n",
      "epoch_num :  541\n",
      "test_loss :  1.6932867918852534\n",
      "train_loss :  1.2929832863653636\n",
      "epoch_num :  546\n",
      "test_loss :  1.6916462334607703\n",
      "train_loss :  1.2902671608727856\n",
      "epoch_num :  551\n",
      "test_loss :  1.6900386131791683\n",
      "train_loss :  1.28766315916005\n",
      "epoch_num :  556\n",
      "test_loss :  1.6884599548708041\n",
      "train_loss :  1.2851627365945464\n",
      "epoch_num :  561\n",
      "test_loss :  1.686906617954961\n",
      "train_loss :  1.2827580204207225\n",
      "epoch_num :  566\n",
      "test_loss :  1.6853752680124872\n",
      "train_loss :  1.2804417698276915\n",
      "epoch_num :  571\n",
      "test_loss :  1.6838628485744085\n",
      "train_loss :  1.2782073337652857\n",
      "epoch_num :  576\n",
      "test_loss :  1.6823665551762772\n",
      "train_loss :  1.27604860820558\n",
      "epoch_num :  581\n",
      "test_loss :  1.680883812251408\n",
      "train_loss :  1.273959994081395\n",
      "epoch_num :  586\n",
      "test_loss :  1.6794122530753235\n",
      "train_loss :  1.2719363567630018\n",
      "epoch_num :  591\n",
      "test_loss :  1.6779497027231187\n",
      "train_loss :  1.2699729876450636\n",
      "epoch_num :  596\n",
      "test_loss :  1.6764941638513735\n",
      "train_loss :  1.2680655681936237\n",
      "epoch_num :  601\n",
      "test_loss :  1.6750438050464658\n",
      "train_loss :  1.2662101366344551\n",
      "epoch_num :  606\n",
      "test_loss :  1.6735969514729498\n",
      "train_loss :  1.264403057338536\n",
      "epoch_num :  611\n",
      "test_loss :  1.672152077587448\n",
      "train_loss :  1.262640992868579\n",
      "epoch_num :  616\n",
      "test_loss :  1.6707078017362447\n",
      "train_loss :  1.2609208785852484\n",
      "epoch_num :  621\n",
      "test_loss :  1.6692628825120734\n",
      "train_loss :  1.2592398996672542\n",
      "epoch_num :  626\n",
      "test_loss :  1.6678162167939072\n",
      "train_loss :  1.2575954703710246\n",
      "epoch_num :  631\n",
      "test_loss :  1.6663668394234037\n",
      "train_loss :  1.2559852153395907\n",
      "epoch_num :  636\n",
      "test_loss :  1.6649139244780449\n",
      "train_loss :  1.2544069527633672\n",
      "epoch_num :  641\n",
      "test_loss :  1.6634567880799613\n",
      "train_loss :  1.2528586791952705\n",
      "epoch_num :  646\n",
      "test_loss :  1.661994892634197\n",
      "train_loss :  1.2513385558271861\n",
      "epoch_num :  651\n",
      "test_loss :  1.6605278523209144\n",
      "train_loss :  1.2498448960423456\n",
      "epoch_num :  656\n",
      "test_loss :  1.659055439581205\n",
      "train_loss :  1.248376154068144\n",
      "epoch_num :  661\n",
      "test_loss :  1.6575775922402973\n",
      "train_loss :  1.246930914565163\n",
      "epoch_num :  666\n",
      "test_loss :  1.6560944208149098\n",
      "train_loss :  1.2455078830006825\n",
      "epoch_num :  671\n",
      "test_loss :  1.6546062154623253\n",
      "train_loss :  1.2441058766685469\n",
      "epoch_num :  676\n",
      "test_loss :  1.653113451957084\n",
      "train_loss :  1.2427238162319973\n",
      "epoch_num :  681\n",
      "test_loss :  1.6516167960373764\n",
      "train_loss :  1.2413607176821129\n",
      "epoch_num :  686\n",
      "test_loss :  1.6501171054553745\n",
      "train_loss :  1.2400156846218056\n",
      "epoch_num :  691\n",
      "test_loss :  1.648615429102004\n",
      "train_loss :  1.2386879008038323\n",
      "epoch_num :  696\n",
      "test_loss :  1.647113002659566\n",
      "train_loss :  1.2373766228704548\n",
      "epoch_num :  701\n",
      "test_loss :  1.6456112403669856\n",
      "train_loss :  1.236081173261717\n",
      "epoch_num :  706\n",
      "test_loss :  1.6441117226559043\n",
      "train_loss :  1.2348009332777\n",
      "epoch_num :  711\n",
      "test_loss :  1.6426161796239596\n",
      "train_loss :  1.2335353362965749\n",
      "epoch_num :  716\n",
      "test_loss :  1.6411264705384285\n",
      "train_loss :  1.232283861163553\n",
      "epoch_num :  721\n",
      "test_loss :  1.6396445597935005\n",
      "train_loss :  1.2310460257748514\n",
      "epoch_num :  726\n",
      "test_loss :  1.6381724899574377\n",
      "train_loss :  1.2298213808847631\n",
      "epoch_num :  731\n",
      "test_loss :  1.6367123527242398\n",
      "train_loss :  1.228609504162296\n",
      "epoch_num :  736\n",
      "test_loss :  1.6352662587113966\n",
      "train_loss :  1.2274099945167283\n",
      "epoch_num :  741\n",
      "test_loss :  1.6338363071104365\n",
      "train_loss :  1.2262224666991146\n",
      "epoch_num :  746\n",
      "test_loss :  1.6324245561952044\n",
      "train_loss :  1.225046546170373\n",
      "epoch_num :  751\n",
      "test_loss :  1.6310329956243756\n",
      "train_loss :  1.2238818642068714\n",
      "epoch_num :  756\n",
      "test_loss :  1.6296635213498283\n",
      "train_loss :  1.2227280531929818\n",
      "epoch_num :  761\n",
      "test_loss :  1.628317913771231\n",
      "train_loss :  1.221584742027728\n",
      "epoch_num :  766\n",
      "test_loss :  1.6269978195774677\n",
      "train_loss :  1.2204515515503556\n",
      "epoch_num :  771\n",
      "test_loss :  1.6257047375031537\n",
      "train_loss :  1.2193280898679388\n",
      "epoch_num :  776\n",
      "test_loss :  1.6244400080199375\n",
      "train_loss :  1.2182139474468383\n",
      "epoch_num :  781\n",
      "test_loss :  1.623204806789876\n",
      "train_loss :  1.2171086918085727\n",
      "epoch_num :  786\n",
      "test_loss :  1.6220001415421477\n",
      "train_loss :  1.2160118616486058\n",
      "epoch_num :  791\n",
      "test_loss :  1.6208268518980415\n",
      "train_loss :  1.214922960172313\n",
      "epoch_num :  796\n",
      "test_loss :  1.619685611563759\n",
      "train_loss :  1.2138414474148518\n",
      "epoch_num :  801\n",
      "test_loss :  1.6185769322311685\n",
      "train_loss :  1.2127667312792867\n",
      "epoch_num :  806\n",
      "test_loss :  1.6175011684660567\n",
      "train_loss :  1.2116981569894054\n",
      "epoch_num :  811\n",
      "test_loss :  1.616458522810788\n",
      "train_loss :  1.2106349946100665\n",
      "epoch_num :  816\n",
      "test_loss :  1.6154490502710184\n",
      "train_loss :  1.2095764242404847\n",
      "epoch_num :  821\n",
      "test_loss :  1.6144726612792106\n",
      "train_loss :  1.2085215184396152\n",
      "epoch_num :  826\n",
      "test_loss :  1.6135291221146117\n",
      "train_loss :  1.2074692214085456\n",
      "epoch_num :  831\n",
      "test_loss :  1.61261805159346\n",
      "train_loss :  1.2064183244535722\n",
      "epoch_num :  836\n",
      "test_loss :  1.6117389126068769\n",
      "train_loss :  1.2053674373236414\n",
      "epoch_num :  841\n",
      "test_loss :  1.6108909967655254\n",
      "train_loss :  1.2043149552247245\n",
      "epoch_num :  846\n",
      "test_loss :  1.6100734000113976\n",
      "train_loss :  1.2032590217767387\n",
      "epoch_num :  851\n",
      "test_loss :  1.60928498661385\n",
      "train_loss :  1.2021974890876317\n",
      "epoch_num :  856\n",
      "test_loss :  1.6085243385912171\n",
      "train_loss :  1.2011278777803212\n",
      "epoch_num :  861\n",
      "test_loss :  1.607789687558959\n",
      "train_loss :  1.2000473426865952\n",
      "epoch_num :  866\n",
      "test_loss :  1.6070788268640182\n",
      "train_loss :  1.1989526546655485\n",
      "epoch_num :  871\n",
      "test_loss :  1.6063890047094513\n",
      "train_loss :  1.1978402163772746\n",
      "epoch_num :  876\n",
      "test_loss :  1.6057168056978812\n",
      "train_loss :  1.1967061404024009\n",
      "epoch_num :  881\n",
      "test_loss :  1.605058041692103\n",
      "train_loss :  1.1955464312883994\n",
      "epoch_num :  886\n",
      "test_loss :  1.6044076963660387\n",
      "train_loss :  1.1943573253033009\n",
      "epoch_num :  891\n",
      "test_loss :  1.6037600022184466\n",
      "train_loss :  1.1931358429899328\n",
      "epoch_num :  896\n",
      "test_loss :  1.6031087656398668\n",
      "train_loss :  1.1918805808091346\n",
      "epoch_num :  901\n",
      "test_loss :  1.6024480650980921\n",
      "train_loss :  1.1905926834813552\n",
      "epoch_num :  906\n",
      "test_loss :  1.6017733713554154\n",
      "train_loss :  1.1892767857024567\n",
      "epoch_num :  911\n",
      "test_loss :  1.6010829165283293\n",
      "train_loss :  1.1879415350031615\n",
      "epoch_num :  916\n",
      "test_loss :  1.6003788009824338\n",
      "train_loss :  1.1865992434978865\n",
      "epoch_num :  921\n",
      "test_loss :  1.5996671040480874\n",
      "train_loss :  1.1852644362688514\n",
      "epoch_num :  926\n",
      "test_loss :  1.5989565098647778\n",
      "train_loss :  1.1839515756723589\n",
      "epoch_num :  931\n",
      "test_loss :  1.5982557504343353\n",
      "train_loss :  1.1826727369097685\n",
      "epoch_num :  936\n",
      "test_loss :  1.5975709795283162\n",
      "train_loss :  1.1814360748606634\n",
      "epoch_num :  941\n",
      "test_loss :  1.596904291864535\n",
      "train_loss :  1.1802454782324638\n",
      "epoch_num :  946\n",
      "test_loss :  1.5962538810602616\n",
      "train_loss :  1.1791012209733545\n",
      "epoch_num :  951\n",
      "test_loss :  1.5956154284324477\n",
      "train_loss :  1.1780010998259525\n",
      "epoch_num :  956\n",
      "test_loss :  1.5949838853459537\n",
      "train_loss :  1.1769415787918778\n",
      "epoch_num :  961\n",
      "test_loss :  1.594354947599333\n",
      "train_loss :  1.1759186730981686\n",
      "epoch_num :  966\n",
      "test_loss :  1.5937259073842445\n",
      "train_loss :  1.1749285086075738\n",
      "epoch_num :  971\n",
      "test_loss :  1.5930959001218004\n",
      "train_loss :  1.1739676082070836\n",
      "epoch_num :  976\n",
      "test_loss :  1.5924657267089213\n",
      "train_loss :  1.1730329924959477\n",
      "epoch_num :  981\n",
      "test_loss :  1.5918374558461026\n",
      "train_loss :  1.1721221738989105\n",
      "epoch_num :  986\n",
      "test_loss :  1.5912139657878335\n",
      "train_loss :  1.1712331004975196\n",
      "epoch_num :  991\n",
      "test_loss :  1.5905985233314064\n",
      "train_loss :  1.170364083738017\n",
      "epoch_num :  996\n"
     ]
    }
   ],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "biases1_squares = 0\n",
    "biases2_squares = 0\n",
    "weights1_squares = 0\n",
    "weights2_squares = 0\n",
    "\n",
    "biases1_momentum = 0\n",
    "biases2_momentum = 0\n",
    "weights1_momentum = 0\n",
    "weights2_momentum = 0\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "    if epoch_num%5==1:\n",
    "        print(\"test_loss : \", test_losses[-1])\n",
    "        print(\"train_loss : \", losses[-1])\n",
    "        print(\"epoch_num : \", epoch_num)\n",
    "    for sample_num in range(x_train.shape[1]):\n",
    "#         print(\"sample_num : \", sample_num)\n",
    "        #Forward propagation\n",
    "        out1_temp = np.matmul(weights1.T, x_train[:,sample_num])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        #Back-propogation\n",
    "        error_out = out2-y_train[:, sample_num]\n",
    "        err_2 = np.matrix(error_out)\n",
    "        grad_2 = np.matmul(out1, err_2.T)\n",
    "        err_1 = np.multiply(np.matmul(weights2, err_2), np.multiply(out1, np.ones(out1.shape)-out1))\n",
    "        grad_1 = np.multiply(x_train[:, sample_num], err_1.T)\n",
    "        #Updating the weights\n",
    "        biases1_momentum = alpha1*biases1_momentum+(1-alpha1)*err_1\n",
    "        biases2_momentum = alpha1*biases2_momentum+(1-alpha1)*err_2\n",
    "        weights1_momentum = alpha1*weights1_momentum+(1-alpha1)*grad_1\n",
    "        weights2_momentum = alpha1*weights2_momentum+(1-alpha1)*grad_2\n",
    "        \n",
    "        biases1_squares = alpha*biases1_squares+(1-alpha)*np.multiply(err_1, err_1)\n",
    "        biases2_squares = alpha*biases2_squares+(1-alpha)*np.multiply(err_2, err_2)\n",
    "        weights1_squares = alpha*weights1_squares+(1-alpha)*np.multiply(grad_1, grad_1)\n",
    "        weights2_squares = alpha*weights2_squares+(1-alpha)*np.multiply(grad_2, grad_2)\n",
    "        \n",
    "        if epoch_num==0 and sample_num<50:\n",
    "            temp_num = sample_num+1\n",
    "            biases1_momentum_cap = biases1_momentum/(1-alpha1**temp_num)\n",
    "            biases2_momentum_cap = biases2_momentum/(1-alpha1**temp_num)\n",
    "            weights1_momentum_cap = weights1_momentum/(1-alpha1**temp_num)\n",
    "            weights2_momentum_cap = weights2_momentum/(1-alpha1**temp_num)\n",
    "            \n",
    "            biases1_squares_cap = biases1_squares/(1-alpha**temp_num)\n",
    "            biases2_squares_cap = biases2_squares/(1-alpha**temp_num)\n",
    "            weights1_squares_cap = weights1_squares/(1-alpha**temp_num)\n",
    "            weights2_squares_cap = weights2_squares/(1-alpha**temp_num)\n",
    "        else:\n",
    "            biases1_momentum_cap = biases1_momentum\n",
    "            biases2_momentum_cap = biases2_momentum\n",
    "            weights1_momentum_cap = weights1_momentum\n",
    "            weights2_momentum_cap = weights2_momentum\n",
    "\n",
    "            biases1_squares_cap = biases1_squares\n",
    "            biases2_squares_cap = biases2_squares\n",
    "            weights1_squares_cap = weights1_squares\n",
    "            weights2_squares_cap = weights2_squares\n",
    "\n",
    "        biases1 = biases1 - np.multiply(learning_rate/(np.sqrt(biases1_squares_cap)+epsilon), biases1_momentum_cap)\n",
    "        biases2 = biases2 - np.multiply(learning_rate/(np.sqrt(biases2_squares_cap)+epsilon), biases2_momentum_cap)\n",
    "        weights1 = weights1 - np.multiply(learning_rate/(np.sqrt(weights1_squares_cap)+epsilon), weights1_momentum_cap)\n",
    "        weights2 = weights2 - np.multiply(learning_rate/(np.sqrt(weights2_squares_cap)+epsilon), weights2_momentum_cap)\n",
    "        #Updating the accuracy and loss\n",
    "        #End of for loop\n",
    "#     print(\"accuracy : \", np.mean(acc_arr))\n",
    "    test_losses.append(test_loss(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_test.append(acc_out(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_train.append(acc_out(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    losses.append(test_loss(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot(211)\n",
    "plt.plot(losses[1:], label = 'train_loss')\n",
    "plt.plot(test_losses[1:], label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.multiply(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test, pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41.74418942 31.83807371 16.80085286 29.20293452 14.17925139]\n",
      " [43.01112017 31.61893602 19.96978337 31.99266066 16.64158999]]\n",
      "[[43.1  31.81 16.86 29.87 14.28]\n",
      " [39.41 31.2  20.01 29.87 15.87]]\n"
     ]
    }
   ],
   "source": [
    "ind = 101\n",
    "print(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)[:,10:15])\n",
    "print(y_test[:,10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1696823649768222\n"
     ]
    }
   ],
   "source": [
    "print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
