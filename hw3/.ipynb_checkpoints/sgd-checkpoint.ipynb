{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def test_loss(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out2 = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "        loss = []\n",
    "        for iter in range(out2.shape[1]):\n",
    "            loss.append(MSE(out2[:, iter], y_in[:, iter]))\n",
    "        return np.mean(loss)\n",
    "\n",
    "def pred_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out1_temp = np.matmul(weights1.T, x_in[:,:])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        return out2\n",
    "\n",
    "def acc_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "    pred = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "    sum_acc = 0\n",
    "    for iter in range(pred.shape[1]):\n",
    "#         print(iter)\n",
    "        sum_acc+=(np.argmax(pred[:, iter])==np.argmax(y_in[:, iter]))\n",
    "    return sum_acc/y_in.shape[1]\n",
    "\n",
    "def MSE(yHat, y):\n",
    "    return np.sum(np.multiply(yHat - y, yHat - y)) / y.size\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-5):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.mean(np.multiply(np.log(predictions),targets)) \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 10)\n"
     ]
    }
   ],
   "source": [
    "##Load and segregate the data\n",
    "letters = genfromtxt('energy_eff2.csv', delimiter=',')\n",
    "# np.random.shuffle(letters)\n",
    "print(letters.shape)\n",
    "x_in = letters[:, 0:8]\n",
    "x_mean = np.mean(x_in, axis = 0)\n",
    "x_std = np.std(x_in, axis = 0)\n",
    "y_in = letters[:, 8:10]\n",
    "y_mean = np.mean(y_in, axis=0)\n",
    "y_std = np.std(y_in, axis = 0)\n",
    "# y_in = (y_in-y_mean)/y_std\n",
    "x_in = (x_in-x_mean)/x_std\n",
    "# print(np.std(y_in, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_in[0:int(x_in.shape[0]*0.7), :]\n",
    "y_train = y_in[0:int(x_in.shape[0]*0.7), :]\n",
    "\n",
    "x_test = x_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]\n",
    "y_test = y_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "weights1 = np.random.normal(size=(x_in.shape[1], hidden_size))\n",
    "weights2 = np.random.normal(size=(hidden_size, y_in.shape[1]))\n",
    "print(weights1.shape)\n",
    "print(weights2.shape)\n",
    "biases1 = np.random.normal(size=(hidden_size, 1))\n",
    "biases2 = np.random.normal(size=(y_in.shape[1], 1))\n",
    "\n",
    "x_train = np.matrix(x_train)\n",
    "x_train = x_train.T\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = np.matrix(x_test)\n",
    "x_test = x_test.T\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  20.467044785341308\n",
      "train_loss :  19.479397357785967\n",
      "epoch_num :  1\n",
      "test_loss :  10.049657962340195\n",
      "train_loss :  9.429367302413924\n",
      "epoch_num :  6\n",
      "test_loss :  9.03704895337479\n",
      "train_loss :  8.389686235096784\n",
      "epoch_num :  11\n",
      "test_loss :  8.381498961712953\n",
      "train_loss :  7.8210718288584715\n",
      "epoch_num :  16\n",
      "test_loss :  7.785677167154521\n",
      "train_loss :  7.31638920198937\n",
      "epoch_num :  21\n",
      "test_loss :  7.2487331457357795\n",
      "train_loss :  6.844026205955151\n",
      "epoch_num :  26\n",
      "test_loss :  6.766357847832174\n",
      "train_loss :  6.403450471085573\n",
      "epoch_num :  31\n",
      "test_loss :  6.30090044305938\n",
      "train_loss :  5.981681647746738\n",
      "epoch_num :  36\n",
      "test_loss :  5.746549727260753\n",
      "train_loss :  5.54213262991442\n",
      "epoch_num :  41\n",
      "test_loss :  5.099157512369844\n",
      "train_loss :  5.0436390436765866\n",
      "epoch_num :  46\n",
      "test_loss :  4.630732386725709\n",
      "train_loss :  4.603169988253548\n",
      "epoch_num :  51\n",
      "test_loss :  4.341529067575766\n",
      "train_loss :  4.2964385572853585\n",
      "epoch_num :  56\n",
      "test_loss :  4.141349698150445\n",
      "train_loss :  4.063109885094971\n",
      "epoch_num :  61\n",
      "test_loss :  3.983093606984559\n",
      "train_loss :  3.8712193680840175\n",
      "epoch_num :  66\n",
      "test_loss :  3.8501391187682663\n",
      "train_loss :  3.707749093426637\n",
      "epoch_num :  71\n",
      "test_loss :  3.7344842808710808\n",
      "train_loss :  3.565047538407353\n",
      "epoch_num :  76\n",
      "test_loss :  3.6309762932068486\n",
      "train_loss :  3.4382342646891835\n",
      "epoch_num :  81\n",
      "test_loss :  3.5360386007619273\n",
      "train_loss :  3.3241610583467875\n",
      "epoch_num :  86\n",
      "test_loss :  3.447396999418016\n",
      "train_loss :  3.2207776821948024\n",
      "epoch_num :  91\n",
      "test_loss :  3.363923017869671\n",
      "train_loss :  3.1267140497070134\n",
      "epoch_num :  96\n",
      "test_loss :  3.285384197520403\n",
      "train_loss :  3.0409913530007135\n",
      "epoch_num :  101\n",
      "test_loss :  3.212044316019066\n",
      "train_loss :  2.962790996315687\n",
      "epoch_num :  106\n",
      "test_loss :  3.1441950421802587\n",
      "train_loss :  2.891276298359175\n",
      "epoch_num :  111\n",
      "test_loss :  3.081832743966468\n",
      "train_loss :  2.825526727860654\n",
      "epoch_num :  116\n",
      "test_loss :  3.0246031060883927\n",
      "train_loss :  2.7645966065114607\n",
      "epoch_num :  121\n",
      "test_loss :  2.971926020646635\n",
      "train_loss :  2.7076306810747375\n",
      "epoch_num :  126\n",
      "test_loss :  2.9231511617147805\n",
      "train_loss :  2.653966380924129\n",
      "epoch_num :  131\n",
      "test_loss :  2.877671182412691\n",
      "train_loss :  2.6031829067369854\n",
      "epoch_num :  136\n",
      "test_loss :  2.8349896068600766\n",
      "train_loss :  2.5550757249764833\n",
      "epoch_num :  141\n",
      "test_loss :  2.7947516084544004\n",
      "train_loss :  2.509566947827355\n",
      "epoch_num :  146\n",
      "test_loss :  2.75673498716049\n",
      "train_loss :  2.4666052849411053\n",
      "epoch_num :  151\n",
      "test_loss :  2.720810684832601\n",
      "train_loss :  2.4261110678588813\n",
      "epoch_num :  156\n",
      "test_loss :  2.6868995738814734\n",
      "train_loss :  2.387973918703619\n",
      "epoch_num :  161\n",
      "test_loss :  2.654940923292906\n",
      "train_loss :  2.352071324964116\n",
      "epoch_num :  166\n",
      "test_loss :  2.6248690122716134\n",
      "train_loss :  2.3182802886152527\n",
      "epoch_num :  171\n",
      "test_loss :  2.5965949577974294\n",
      "train_loss :  2.286478163671525\n",
      "epoch_num :  176\n",
      "test_loss :  2.5699984988675735\n",
      "train_loss :  2.256541189970238\n",
      "epoch_num :  181\n",
      "test_loss :  2.5449325965043923\n",
      "train_loss :  2.228345796873712\n",
      "epoch_num :  186\n",
      "test_loss :  2.521236915980727\n",
      "train_loss :  2.2017715959302997\n",
      "epoch_num :  191\n",
      "test_loss :  2.4987533598086604\n",
      "train_loss :  2.1767036798674133\n",
      "epoch_num :  196\n",
      "test_loss :  2.4773382922781995\n",
      "train_loss :  2.1530334471396686\n",
      "epoch_num :  201\n",
      "test_loss :  2.4568690651142204\n",
      "train_loss :  2.1306584431906095\n",
      "epoch_num :  206\n",
      "test_loss :  2.4372450980934137\n",
      "train_loss :  2.1094819096534168\n",
      "epoch_num :  211\n",
      "test_loss :  2.418385306795665\n",
      "train_loss :  2.0894124246457215\n",
      "epoch_num :  216\n",
      "test_loss :  2.4002239720748673\n",
      "train_loss :  2.0703637137109143\n",
      "epoch_num :  221\n",
      "test_loss :  2.3827066183813392\n",
      "train_loss :  2.0522545695671903\n",
      "epoch_num :  226\n",
      "test_loss :  2.3657866984522067\n",
      "train_loss :  2.0350087995125175\n",
      "epoch_num :  231\n",
      "test_loss :  2.349423272807482\n",
      "train_loss :  2.0185551479410675\n",
      "epoch_num :  236\n",
      "test_loss :  2.333579544082298\n",
      "train_loss :  2.0028271735368723\n",
      "epoch_num :  241\n",
      "test_loss :  2.3182220019653217\n",
      "train_loss :  1.9877630819137495\n",
      "epoch_num :  246\n",
      "test_loss :  2.303319951866462\n",
      "train_loss :  1.9733055248098073\n",
      "epoch_num :  251\n",
      "test_loss :  2.2888452602332006\n",
      "train_loss :  1.959401380169564\n",
      "epoch_num :  256\n",
      "test_loss :  2.2747722096973204\n",
      "train_loss :  1.946001526875751\n",
      "epoch_num :  261\n",
      "test_loss :  2.261077402868981\n",
      "train_loss :  1.9330606256542895\n",
      "epoch_num :  266\n",
      "test_loss :  2.2477396831647884\n",
      "train_loss :  1.9205369149662632\n",
      "epoch_num :  271\n",
      "test_loss :  2.2347400579787973\n",
      "train_loss :  1.9083920281460414\n",
      "epoch_num :  276\n",
      "test_loss :  2.2220616178201404\n",
      "train_loss :  1.8965908359589323\n",
      "epoch_num :  281\n",
      "test_loss :  2.2096894479588394\n",
      "train_loss :  1.885101317294515\n",
      "epoch_num :  286\n",
      "test_loss :  2.1976105288428367\n",
      "train_loss :  1.873894459967769\n",
      "epoch_num :  291\n",
      "test_loss :  2.1858136195233433\n",
      "train_loss :  1.8629441935879227\n",
      "epoch_num :  296\n",
      "test_loss :  2.1742891155675843\n",
      "train_loss :  1.8522273570589287\n",
      "epoch_num :  301\n",
      "test_loss :  2.163028870324392\n",
      "train_loss :  1.8417237040961114\n",
      "epoch_num :  306\n",
      "test_loss :  2.1520259669258315\n",
      "train_loss :  1.831415950283649\n",
      "epoch_num :  311\n",
      "test_loss :  2.1412744293757453\n",
      "train_loss :  1.8212898630595877\n",
      "epoch_num :  316\n",
      "test_loss :  2.130768866277214\n",
      "train_loss :  1.8113343892917424\n",
      "epoch_num :  321\n",
      "test_loss :  2.120504052287404\n",
      "train_loss :  1.8015418012937419\n",
      "epoch_num :  326\n",
      "test_loss :  2.1104744718913837\n",
      "train_loss :  1.791907819983817\n",
      "epoch_num :  331\n",
      "test_loss :  2.1006738768978983\n",
      "train_loss :  1.782431646039526\n",
      "epoch_num :  336\n",
      "test_loss :  2.0910949375228367\n",
      "train_loss :  1.7731158058290553\n",
      "epoch_num :  341\n",
      "test_loss :  2.081729084343428\n",
      "train_loss :  1.763965715653896\n",
      "epoch_num :  346\n",
      "test_loss :  2.0725666271142185\n",
      "train_loss :  1.754988904446168\n",
      "epoch_num :  351\n",
      "test_loss :  2.0635971832630418\n",
      "train_loss :  1.7461939188301097\n",
      "epoch_num :  356\n",
      "test_loss :  2.0548103597271674\n",
      "train_loss :  1.7375890456664997\n",
      "epoch_num :  361\n",
      "test_loss :  2.046196539611523\n",
      "train_loss :  1.7291810778174321\n",
      "epoch_num :  366\n",
      "test_loss :  2.037747577378963\n",
      "train_loss :  1.7209743665890727\n",
      "epoch_num :  371\n",
      "test_loss :  2.0294572334851697\n",
      "train_loss :  1.7129703300481596\n",
      "epoch_num :  376\n",
      "test_loss :  2.021321270184784\n",
      "train_loss :  1.7051674527549041\n",
      "epoch_num :  381\n",
      "test_loss :  2.013337237857332\n",
      "train_loss :  1.6975616829288762\n",
      "epoch_num :  386\n",
      "test_loss :  2.0055040566889604\n",
      "train_loss :  1.6901470597283634\n",
      "epoch_num :  391\n",
      "test_loss :  1.9978215203556333\n",
      "train_loss :  1.682916399417963\n",
      "epoch_num :  396\n",
      "test_loss :  1.990289826111573\n",
      "train_loss :  1.6758619136035917\n",
      "epoch_num :  401\n",
      "test_loss :  1.9829091937750236\n",
      "train_loss :  1.6689756928229404\n",
      "epoch_num :  406\n",
      "test_loss :  1.9756795959234488\n",
      "train_loss :  1.662250040000446\n",
      "epoch_num :  411\n",
      "test_loss :  1.9686005938420785\n",
      "train_loss :  1.6556776704378129\n",
      "epoch_num :  416\n",
      "test_loss :  1.9616712597022767\n",
      "train_loss :  1.6492518089698207\n",
      "epoch_num :  421\n",
      "test_loss :  1.9548901616178076\n",
      "train_loss :  1.6429662163996923\n",
      "epoch_num :  426\n",
      "test_loss :  1.9482553902506197\n",
      "train_loss :  1.6368151723949484\n",
      "epoch_num :  431\n",
      "test_loss :  1.9417646100704131\n",
      "train_loss :  1.6307934350473288\n",
      "epoch_num :  436\n",
      "test_loss :  1.9354151231313215\n",
      "train_loss :  1.6248961907134643\n",
      "epoch_num :  441\n",
      "test_loss :  1.9292039373246106\n",
      "train_loss :  1.6191190025446875\n",
      "epoch_num :  446\n",
      "test_loss :  1.9231278341944698\n",
      "train_loss :  1.6134577624327702\n",
      "epoch_num :  451\n",
      "test_loss :  1.9171834336053897\n",
      "train_loss :  1.6079086487111072\n",
      "epoch_num :  456\n",
      "test_loss :  1.9113672539971498\n",
      "train_loss :  1.6024680905180861\n",
      "epoch_num :  461\n",
      "test_loss :  1.9056757678511609\n",
      "train_loss :  1.5971327389345826\n",
      "epoch_num :  466\n",
      "test_loss :  1.9001054524830956\n",
      "train_loss :  1.5918994446035815\n",
      "epoch_num :  471\n",
      "test_loss :  1.8946528364964483\n",
      "train_loss :  1.5867652413516284\n",
      "epoch_num :  476\n",
      "test_loss :  1.8893145422643838\n",
      "train_loss :  1.5817273352444194\n",
      "epoch_num :  481\n",
      "test_loss :  1.884087324711239\n",
      "train_loss :  1.5767830984508961\n",
      "epoch_num :  486\n",
      "test_loss :  1.8789681064774317\n",
      "train_loss :  1.5719300672220187\n",
      "epoch_num :  491\n",
      "test_loss :  1.8739540093027258\n",
      "train_loss :  1.5671659431936413\n",
      "epoch_num :  496\n",
      "test_loss :  1.8690423811793695\n",
      "train_loss :  1.5624885970962834\n",
      "epoch_num :  501\n",
      "test_loss :  1.8642308185402774\n",
      "train_loss :  1.557896073809796\n",
      "epoch_num :  506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  1.859517182497448\n",
      "train_loss :  1.5533865975617311\n",
      "epoch_num :  511\n",
      "test_loss :  1.8548996079794617\n",
      "train_loss :  1.5489585759669964\n",
      "epoch_num :  516\n",
      "test_loss :  1.850376504585367\n",
      "train_loss :  1.5446106015813494\n",
      "epoch_num :  521\n",
      "test_loss :  1.8459465481253285\n",
      "train_loss :  1.5403414497288737\n",
      "epoch_num :  526\n",
      "test_loss :  1.841608662188186\n",
      "train_loss :  1.5361500715910172\n",
      "epoch_num :  531\n",
      "test_loss :  1.8373619896686897\n",
      "train_loss :  1.5320355819193858\n",
      "epoch_num :  536\n",
      "test_loss :  1.833205854964634\n",
      "train_loss :  1.5279972412366465\n",
      "epoch_num :  541\n",
      "test_loss :  1.8291397184343445\n",
      "train_loss :  1.5240344329688145\n",
      "epoch_num :  546\n",
      "test_loss :  1.825163125562586\n",
      "train_loss :  1.520146636531322\n",
      "epoch_num :  551\n",
      "test_loss :  1.8212756539720416\n",
      "train_loss :  1.516333397880454\n",
      "epoch_num :  556\n",
      "test_loss :  1.8174768618000943\n",
      "train_loss :  1.5125942993573636\n",
      "epoch_num :  561\n",
      "test_loss :  1.8137662409414044\n",
      "train_loss :  1.5089289307373934\n",
      "epoch_num :  566\n",
      "test_loss :  1.810143178211424\n",
      "train_loss :  1.5053368632387043\n",
      "epoch_num :  571\n",
      "test_loss :  1.8066069266715006\n",
      "train_loss :  1.5018176278744069\n",
      "epoch_num :  576\n",
      "test_loss :  1.80315658830074\n",
      "train_loss :  1.4983706990229186\n",
      "epoch_num :  581\n",
      "test_loss :  1.7997911080724711\n",
      "train_loss :  1.494995483533563\n",
      "epoch_num :  586\n",
      "test_loss :  1.796509278465083\n",
      "train_loss :  1.491691315167582\n",
      "epoch_num :  591\n",
      "test_loss :  1.793309752642928\n",
      "train_loss :  1.4884574537662298\n",
      "epoch_num :  596\n",
      "test_loss :  1.7901910640600536\n",
      "train_loss :  1.4852930882733744\n",
      "epoch_num :  601\n",
      "test_loss :  1.7871516500792175\n",
      "train_loss :  1.4821973426237862\n",
      "epoch_num :  606\n",
      "test_loss :  1.7841898773183078\n",
      "train_loss :  1.4791692835185104\n",
      "epoch_num :  611\n",
      "test_loss :  1.781304066760237\n",
      "train_loss :  1.476207929210435\n",
      "epoch_num :  616\n",
      "test_loss :  1.7784925171023453\n",
      "train_loss :  1.4733122585776393\n",
      "epoch_num :  621\n",
      "test_loss :  1.7757535252971088\n",
      "train_loss :  1.4704812199358492\n",
      "epoch_num :  626\n",
      "test_loss :  1.773085403684557\n",
      "train_loss :  1.467713739209154\n",
      "epoch_num :  631\n",
      "test_loss :  1.7704864934990268\n",
      "train_loss :  1.4650087272248165\n",
      "epoch_num :  636\n",
      "test_loss :  1.7679551748289917\n",
      "train_loss :  1.4623650860165702\n",
      "epoch_num :  641\n",
      "test_loss :  1.765489873317024\n",
      "train_loss :  1.4597817141100222\n",
      "epoch_num :  646\n",
      "test_loss :  1.7630890640151147\n",
      "train_loss :  1.4572575108266377\n",
      "epoch_num :  651\n",
      "test_loss :  1.7607512728731907\n",
      "train_loss :  1.4547913796834113\n",
      "epoch_num :  656\n",
      "test_loss :  1.7584750763508483\n",
      "train_loss :  1.4523822309887064\n",
      "epoch_num :  661\n",
      "test_loss :  1.7562590996198437\n",
      "train_loss :  1.4500289837453997\n",
      "epoch_num :  666\n",
      "test_loss :  1.754102013779567\n",
      "train_loss :  1.4477305669739746\n",
      "epoch_num :  671\n",
      "test_loss :  1.7520025324508313\n",
      "train_loss :  1.4454859205637727\n",
      "epoch_num :  676\n",
      "test_loss :  1.749959408051115\n",
      "train_loss :  1.443293995752333\n",
      "epoch_num :  681\n",
      "test_loss :  1.7479714279933771\n",
      "train_loss :  1.4411537553222837\n",
      "epoch_num :  686\n",
      "test_loss :  1.7460374109930334\n",
      "train_loss :  1.4390641735938203\n",
      "epoch_num :  691\n",
      "test_loss :  1.7441562036164902\n",
      "train_loss :  1.4370242362790886\n",
      "epoch_num :  696\n",
      "test_loss :  1.7423266771603398\n",
      "train_loss :  1.4350329402535322\n",
      "epoch_num :  701\n",
      "test_loss :  1.7405477249132357\n",
      "train_loss :  1.4330892932885408\n",
      "epoch_num :  706\n",
      "test_loss :  1.7388182598228787\n",
      "train_loss :  1.431192313780109\n",
      "epoch_num :  711\n",
      "test_loss :  1.737137212566865\n",
      "train_loss :  1.4293410304994791\n",
      "epoch_num :  716\n",
      "test_loss :  1.7355035300094672\n",
      "train_loss :  1.4275344823842502\n",
      "epoch_num :  721\n",
      "test_loss :  1.7339161740138869\n",
      "train_loss :  1.4257717183819454\n",
      "epoch_num :  726\n",
      "test_loss :  1.7323741205721916\n",
      "train_loss :  1.4240517973527058\n",
      "epoch_num :  731\n",
      "test_loss :  1.730876359210857\n",
      "train_loss :  1.4223737880334177\n",
      "epoch_num :  736\n",
      "test_loss :  1.729421892628854\n",
      "train_loss :  1.4207367690621524\n",
      "epoch_num :  741\n",
      "test_loss :  1.728009736525973\n",
      "train_loss :  1.4191398290592465\n",
      "epoch_num :  746\n",
      "test_loss :  1.7266389195817078\n",
      "train_loss :  1.417582066759471\n",
      "epoch_num :  751\n",
      "test_loss :  1.725308483548624\n",
      "train_loss :  1.416062591188536\n",
      "epoch_num :  756\n",
      "test_loss :  1.7240174834282098\n",
      "train_loss :  1.414580521876443\n",
      "epoch_num :  761\n",
      "test_loss :  1.722764987701647\n",
      "train_loss :  1.4131349890999445\n",
      "epoch_num :  766\n",
      "test_loss :  1.7215500785925262\n",
      "train_loss :  1.411725134146404\n",
      "epoch_num :  771\n",
      "test_loss :  1.7203718523426648\n",
      "train_loss :  1.410350109591656\n",
      "epoch_num :  776\n",
      "test_loss :  1.7192294194860458\n",
      "train_loss :  1.4090090795849632\n",
      "epoch_num :  781\n",
      "test_loss :  1.7181219051095253\n",
      "train_loss :  1.4077012201347596\n",
      "epoch_num :  786\n",
      "test_loss :  1.7170484490920466\n",
      "train_loss :  1.4064257193895744\n",
      "epoch_num :  791\n",
      "test_loss :  1.7160082063165367\n",
      "train_loss :  1.4051817779092348\n",
      "epoch_num :  796\n",
      "test_loss :  1.7150003468509785\n",
      "train_loss :  1.4039686089221821\n",
      "epoch_num :  801\n",
      "test_loss :  1.7140240560968427\n",
      "train_loss :  1.4027854385654153\n",
      "epoch_num :  806\n",
      "test_loss :  1.7130785349044255\n",
      "train_loss :  1.4016315061042788\n",
      "epoch_num :  811\n",
      "test_loss :  1.7121629996557906\n",
      "train_loss :  1.4005060641298908\n",
      "epoch_num :  816\n",
      "test_loss :  1.7112766823165584\n",
      "train_loss :  1.3994083787325864\n",
      "epoch_num :  821\n",
      "test_loss :  1.7104188304586652\n",
      "train_loss :  1.3983377296502717\n",
      "epoch_num :  826\n",
      "test_loss :  1.7095887072561824\n",
      "train_loss :  1.397293410390987\n",
      "epoch_num :  831\n",
      "test_loss :  1.7087855914568673\n",
      "train_loss :  1.3962747283294243\n",
      "epoch_num :  836\n",
      "test_loss :  1.7080087773317996\n",
      "train_loss :  1.3952810047774025\n",
      "epoch_num :  841\n",
      "test_loss :  1.7072575746057552\n",
      "train_loss :  1.3943115750286683\n",
      "epoch_num :  846\n",
      "test_loss :  1.706531308370773\n",
      "train_loss :  1.3933657883785406\n",
      "epoch_num :  851\n",
      "test_loss :  1.7058293189851361\n",
      "train_loss :  1.3924430081191983\n",
      "epoch_num :  856\n",
      "test_loss :  1.7051509619600231\n",
      "train_loss :  1.391542611511465\n",
      "epoch_num :  861\n",
      "test_loss :  1.7044956078357443\n",
      "train_loss :  1.3906639897341728\n",
      "epoch_num :  866\n",
      "test_loss :  1.7038626420493925\n",
      "train_loss :  1.3898065478121813\n",
      "epoch_num :  871\n",
      "test_loss :  1.703251464795428\n",
      "train_loss :  1.3889697045242615\n",
      "epoch_num :  876\n",
      "test_loss :  1.7026614908807074\n",
      "train_loss :  1.3881528922920723\n",
      "epoch_num :  881\n",
      "test_loss :  1.7020921495750916\n",
      "train_loss :  1.3873555570514817\n",
      "epoch_num :  886\n",
      "test_loss :  1.701542884458749\n",
      "train_loss :  1.386577158107504\n",
      "epoch_num :  891\n",
      "test_loss :  1.7010131532669874\n",
      "train_loss :  1.3858171679741083\n",
      "epoch_num :  896\n",
      "test_loss :  1.7005024277334124\n",
      "train_loss :  1.3850750722001732\n",
      "epoch_num :  901\n",
      "test_loss :  1.7000101934320302\n",
      "train_loss :  1.384350369182767\n",
      "epoch_num :  906\n",
      "test_loss :  1.6995359496186782\n",
      "train_loss :  1.3836425699690054\n",
      "epoch_num :  911\n",
      "test_loss :  1.6990792090723181\n",
      "train_loss :  1.3829511980475968\n",
      "epoch_num :  916\n",
      "test_loss :  1.69863949793638\n",
      "train_loss :  1.3822757891312214\n",
      "epoch_num :  921\n",
      "test_loss :  1.6982163555602985\n",
      "train_loss :  1.3816158909307996\n",
      "epoch_num :  926\n",
      "test_loss :  1.697809334341479\n",
      "train_loss :  1.3809710629226772\n",
      "epoch_num :  931\n",
      "test_loss :  1.6974179995676464\n",
      "train_loss :  1.3803408761097011\n",
      "epoch_num :  936\n",
      "test_loss :  1.6970419292596193\n",
      "train_loss :  1.3797249127771043\n",
      "epoch_num :  941\n",
      "test_loss :  1.696680714014388\n",
      "train_loss :  1.3791227662440604\n",
      "epoch_num :  946\n",
      "test_loss :  1.6963339568484088\n",
      "train_loss :  1.37853404061174\n",
      "epoch_num :  951\n",
      "test_loss :  1.6960012730410376\n",
      "train_loss :  1.377958350508616\n",
      "epoch_num :  956\n",
      "test_loss :  1.6956822899778208\n",
      "train_loss :  1.3773953208337404\n",
      "epoch_num :  961\n",
      "test_loss :  1.6953766469934899\n",
      "train_loss :  1.376844586498643\n",
      "epoch_num :  966\n",
      "test_loss :  1.6950839952144283\n",
      "train_loss :  1.3763057921684743\n",
      "epoch_num :  971\n",
      "test_loss :  1.6948039974003886\n",
      "train_loss :  1.3757785920029557\n",
      "epoch_num :  976\n",
      "test_loss :  1.6945363277850989\n",
      "train_loss :  1.3752626493976456\n",
      "epoch_num :  981\n",
      "test_loss :  1.6942806719155894\n",
      "train_loss :  1.3747576367260064\n",
      "epoch_num :  986\n",
      "test_loss :  1.6940367264898786\n",
      "train_loss :  1.3742632350827038\n",
      "epoch_num :  991\n",
      "test_loss :  1.693804199192673\n",
      "train_loss :  1.3737791340285268\n",
      "epoch_num :  996\n"
     ]
    }
   ],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = [0]\n",
    "losses = [0]\n",
    "test_losses = [0]\n",
    "for epoch_num in range(num_epochs):\n",
    "    if epoch_num%5==1:\n",
    "#         print(\"test_accuracies : \", accuracies_test[-1])\n",
    "        print(\"test_loss : \", test_losses[-1])\n",
    "        print(\"train_loss : \", losses[-1])\n",
    "        print(\"epoch_num : \", epoch_num)\n",
    "    for sample_num in range(x_train.shape[1]):\n",
    "#         if(sample_num%100==0):\n",
    "#         print(\"sample_num : \", sample_num)\n",
    "        #Forward propagation\n",
    "        out1_temp = np.matmul(weights1.T, x_train[:,sample_num])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        #Back-propogation\n",
    "        error_out = out2-y_train[:, sample_num]\n",
    "        err_2 = np.matrix(error_out)\n",
    "        grad_2 = np.matmul(out1, err_2.T)\n",
    "        err_1 = np.multiply(np.matmul(weights2, err_2), np.multiply(out1, np.ones(out1.shape)-out1))\n",
    "        grad_1 = np.multiply(x_train[:, sample_num], err_1.T)\n",
    "        #Updating the weights\n",
    "#         print(weights1)\n",
    "        biases1 = biases1 - learning_rate*err_1\n",
    "        biases2 = biases2 - learning_rate*err_2\n",
    "        weights1 = weights1-learning_rate*grad_1\n",
    "        weights2 = weights2-learning_rate*grad_2\n",
    "#         print(cross_entropy(out2, y_train[:, sample_num]))\n",
    "        #Updating the accuracy and loss\n",
    "        #End of for loop\n",
    "#     print(\"accuracy : \", np.mean(acc_arr))\n",
    "    test_losses.append(test_loss(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_test.append(acc_out(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_train.append(acc_out(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    losses.append(test_loss(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot(211)\n",
    "plt.plot(losses[1:], label = 'train_loss')\n",
    "plt.plot(test_losses[1:], label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.multiply(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test, pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.23602841 19.75280424 38.52543694 19.5108349  23.91223126]\n",
      " [14.80093695 25.25565381 40.12284675 22.1489168  26.41458798]]\n",
      "[[11.7  19.95 38.57 19.36 24.29]\n",
      " [13.88 21.97 43.86 22.73 25.95]]\n"
     ]
    }
   ],
   "source": [
    "ind = 101\n",
    "print(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)[:,5:10])\n",
    "print(y_test[:,5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_losses)\n",
    "# print(losses)\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(weights1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3733990662438185\n"
     ]
    }
   ],
   "source": [
    "print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
