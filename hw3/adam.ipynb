{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def test_loss(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out2 = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "        loss = []\n",
    "        for iter in range(out2.shape[1]):\n",
    "            loss.append(MSE(out2[:, iter], y_in[:, iter]))\n",
    "        return np.mean(loss)\n",
    "\n",
    "def pred_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out1_temp = np.matmul(weights1.T, x_in[:,:])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        return out2\n",
    "\n",
    "def acc_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "    pred = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "    sum_acc = 0\n",
    "    for iter in range(pred.shape[1]):\n",
    "#         print(iter)\n",
    "        sum_acc+=(np.argmax(pred[:, iter])==np.argmax(y_in[:, iter]))\n",
    "    return sum_acc/y_in.shape[1]\n",
    "\n",
    "def MSE(yHat, y):\n",
    "    return np.sum(np.multiply(yHat - y, yHat - y)) / y.size\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-5):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.mean(np.multiply(np.log(predictions),targets)) \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 10)\n"
     ]
    }
   ],
   "source": [
    "##Load and segregate the data\n",
    "letters = genfromtxt('energy_eff2.csv', delimiter=',')\n",
    "# np.random.shuffle(letters)\n",
    "print(letters.shape)\n",
    "x_in = letters[:, 0:8]\n",
    "x_mean = np.mean(x_in, axis = 0)\n",
    "x_std = np.std(x_in, axis = 0)\n",
    "y_in = letters[:, 8:10]\n",
    "y_mean = np.mean(y_in, axis=0)\n",
    "y_std = np.std(y_in, axis = 0)\n",
    "# y_in = (y_in-y_mean)/y_std\n",
    "x_in = (x_in-x_mean)/x_std\n",
    "# print(np.std(y_in, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_in[0:int(x_in.shape[0]*0.7), :]\n",
    "y_train = y_in[0:int(x_in.shape[0]*0.7), :]\n",
    "\n",
    "x_test = x_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]\n",
    "y_test = y_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "learning_rate = 0.005\n",
    "num_epochs = 1000\n",
    "alpha = 0.9\n",
    "alpha1 = 0.9\n",
    "epsilon = 1e-12\n",
    "\n",
    "weights1 = np.random.normal(size=(x_in.shape[1], hidden_size))\n",
    "weights2 = np.random.normal(size=(hidden_size, y_in.shape[1]))\n",
    "print(weights1.shape)\n",
    "print(weights2.shape)\n",
    "biases1 = np.random.normal(size=(hidden_size, 1))\n",
    "biases2 = np.random.normal(size=(y_in.shape[1], 1))\n",
    "\n",
    "x_train = np.matrix(x_train)\n",
    "x_train = x_train.T\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = np.matrix(x_test)\n",
    "x_test = x_test.T\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  103.86397205154607\n",
      "train_loss :  123.63718768829543\n",
      "epoch_num :  1\n",
      "test_loss :  8.893642387982437\n",
      "train_loss :  8.903471053027552\n",
      "epoch_num :  6\n",
      "test_loss :  6.647415435061257\n",
      "train_loss :  6.714355303612526\n",
      "epoch_num :  11\n",
      "test_loss :  4.961467933273049\n",
      "train_loss :  4.96379149469009\n",
      "epoch_num :  16\n",
      "test_loss :  3.969031166429032\n",
      "train_loss :  3.9116892175851823\n",
      "epoch_num :  21\n",
      "test_loss :  3.365842013339278\n",
      "train_loss :  3.220882975422686\n",
      "epoch_num :  26\n",
      "test_loss :  2.909061432595817\n",
      "train_loss :  2.8099515967080233\n",
      "epoch_num :  31\n",
      "test_loss :  2.622296236924618\n",
      "train_loss :  2.5518428410885883\n",
      "epoch_num :  36\n",
      "test_loss :  2.4752225247676924\n",
      "train_loss :  2.381513250890849\n",
      "epoch_num :  41\n",
      "test_loss :  2.4013981437771466\n",
      "train_loss :  2.267336458847333\n",
      "epoch_num :  46\n",
      "test_loss :  2.337411513188852\n",
      "train_loss :  2.1904624174926486\n",
      "epoch_num :  51\n",
      "test_loss :  2.244132307435059\n",
      "train_loss :  2.119117968423279\n",
      "epoch_num :  56\n",
      "test_loss :  2.1670354959824087\n",
      "train_loss :  2.0643033438951472\n",
      "epoch_num :  61\n",
      "test_loss :  2.103265654831156\n",
      "train_loss :  2.017195454491915\n",
      "epoch_num :  66\n",
      "test_loss :  2.047198225524614\n",
      "train_loss :  1.9749986328696714\n",
      "epoch_num :  71\n",
      "test_loss :  1.9966112822796367\n",
      "train_loss :  1.9352372939275826\n",
      "epoch_num :  76\n",
      "test_loss :  1.9490850811469\n",
      "train_loss :  1.8965188709120753\n",
      "epoch_num :  81\n",
      "test_loss :  1.8916863062575557\n",
      "train_loss :  1.842604809087019\n",
      "epoch_num :  86\n",
      "test_loss :  1.8304194294513838\n",
      "train_loss :  1.7800065932775846\n",
      "epoch_num :  91\n",
      "test_loss :  1.7821577558189483\n",
      "train_loss :  1.730079612453262\n",
      "epoch_num :  96\n",
      "test_loss :  1.7482191848419175\n",
      "train_loss :  1.690709413398021\n",
      "epoch_num :  101\n",
      "test_loss :  1.7230769546582383\n",
      "train_loss :  1.6603504045834188\n",
      "epoch_num :  106\n",
      "test_loss :  1.7059066689484559\n",
      "train_loss :  1.6359128497331414\n",
      "epoch_num :  111\n",
      "test_loss :  1.6946420667547297\n",
      "train_loss :  1.6164783980847688\n",
      "epoch_num :  116\n",
      "test_loss :  1.6882241353057832\n",
      "train_loss :  1.6013773975893326\n",
      "epoch_num :  121\n",
      "test_loss :  1.6848530640679207\n",
      "train_loss :  1.5899402607870485\n",
      "epoch_num :  126\n",
      "test_loss :  1.6811742681357265\n",
      "train_loss :  1.579779827838029\n",
      "epoch_num :  131\n",
      "test_loss :  1.6821488026806983\n",
      "train_loss :  1.5731309000272473\n",
      "epoch_num :  136\n",
      "test_loss :  1.6852988313982158\n",
      "train_loss :  1.5693172511818034\n",
      "epoch_num :  141\n",
      "test_loss :  1.6885170870991468\n",
      "train_loss :  1.5664790959462411\n",
      "epoch_num :  146\n",
      "test_loss :  1.6926741218217163\n",
      "train_loss :  1.5670363643774028\n",
      "epoch_num :  151\n",
      "test_loss :  1.6940420826414364\n",
      "train_loss :  1.5681259290832092\n",
      "epoch_num :  156\n",
      "test_loss :  1.6967052700375662\n",
      "train_loss :  1.5711066657774087\n",
      "epoch_num :  161\n",
      "test_loss :  1.6975130730166499\n",
      "train_loss :  1.5707489034804287\n",
      "epoch_num :  166\n",
      "test_loss :  1.6971049046842055\n",
      "train_loss :  1.5703971567318584\n",
      "epoch_num :  171\n",
      "test_loss :  1.6959535498354499\n",
      "train_loss :  1.5688238321285397\n",
      "epoch_num :  176\n",
      "test_loss :  1.6946953522834136\n",
      "train_loss :  1.5707108634561187\n",
      "epoch_num :  181\n",
      "test_loss :  1.688903101426694\n",
      "train_loss :  1.5704991670850765\n",
      "epoch_num :  186\n",
      "test_loss :  1.671877870246228\n",
      "train_loss :  1.5659014189555336\n",
      "epoch_num :  191\n",
      "test_loss :  1.6241647855951222\n",
      "train_loss :  1.5411278119629535\n",
      "epoch_num :  196\n",
      "test_loss :  1.5975594803510862\n",
      "train_loss :  1.5101163892786393\n",
      "epoch_num :  201\n",
      "test_loss :  1.6035018314713825\n",
      "train_loss :  1.4756263581438855\n",
      "epoch_num :  206\n",
      "test_loss :  1.604454869579113\n",
      "train_loss :  1.4375907066745466\n",
      "epoch_num :  211\n",
      "test_loss :  1.5986866218223554\n",
      "train_loss :  1.409145022653404\n",
      "epoch_num :  216\n",
      "test_loss :  1.5935727355571583\n",
      "train_loss :  1.3917001543959164\n",
      "epoch_num :  221\n",
      "test_loss :  1.5931247340944088\n",
      "train_loss :  1.380397871189148\n",
      "epoch_num :  226\n",
      "test_loss :  1.5965252294820766\n",
      "train_loss :  1.3716474333631723\n",
      "epoch_num :  231\n",
      "test_loss :  1.6001513165682189\n",
      "train_loss :  1.3640415421073695\n",
      "epoch_num :  236\n",
      "test_loss :  1.6034707493220344\n",
      "train_loss :  1.357446359662889\n",
      "epoch_num :  241\n",
      "test_loss :  1.6071452682821366\n",
      "train_loss :  1.3517903466856767\n",
      "epoch_num :  246\n",
      "test_loss :  1.611597275738354\n",
      "train_loss :  1.347133567727172\n",
      "epoch_num :  251\n",
      "test_loss :  1.616904021807696\n",
      "train_loss :  1.343645236472973\n",
      "epoch_num :  256\n",
      "test_loss :  1.6231633947126873\n",
      "train_loss :  1.3415265434816743\n",
      "epoch_num :  261\n",
      "test_loss :  1.6307061828726346\n",
      "train_loss :  1.340962692855166\n",
      "epoch_num :  266\n",
      "test_loss :  1.6397446879620539\n",
      "train_loss :  1.3421429579161435\n",
      "epoch_num :  271\n",
      "test_loss :  1.6502143829605935\n",
      "train_loss :  1.3454026472637752\n",
      "epoch_num :  276\n",
      "test_loss :  1.661378898426731\n",
      "train_loss :  1.3507943463529821\n",
      "epoch_num :  281\n",
      "test_loss :  1.6728410027940732\n",
      "train_loss :  1.3581564017906378\n",
      "epoch_num :  286\n",
      "test_loss :  1.6836365019746247\n",
      "train_loss :  1.3670371362330096\n",
      "epoch_num :  291\n",
      "test_loss :  1.6919062684095871\n",
      "train_loss :  1.376121215067899\n",
      "epoch_num :  296\n",
      "test_loss :  1.6966945826435818\n",
      "train_loss :  1.3841517674534844\n",
      "epoch_num :  301\n",
      "test_loss :  1.6966772433647423\n",
      "train_loss :  1.3896658880567494\n",
      "epoch_num :  306\n",
      "test_loss :  1.688387393373092\n",
      "train_loss :  1.3885221822536773\n",
      "epoch_num :  311\n",
      "test_loss :  1.6707747377923887\n",
      "train_loss :  1.3778878412441\n",
      "epoch_num :  316\n",
      "test_loss :  1.6469663459153074\n",
      "train_loss :  1.365596432679065\n",
      "epoch_num :  321\n",
      "test_loss :  1.6240217920055537\n",
      "train_loss :  1.3561132678232766\n",
      "epoch_num :  326\n",
      "test_loss :  1.6037218063907746\n",
      "train_loss :  1.348589383188661\n",
      "epoch_num :  331\n",
      "test_loss :  1.5850232713042416\n",
      "train_loss :  1.3417649702333634\n",
      "epoch_num :  336\n",
      "test_loss :  1.5676298585263617\n",
      "train_loss :  1.3351682575325088\n",
      "epoch_num :  341\n",
      "test_loss :  1.5513838334015788\n",
      "train_loss :  1.328646670081885\n",
      "epoch_num :  346\n",
      "test_loss :  1.5363814624866081\n",
      "train_loss :  1.3223079462256047\n",
      "epoch_num :  351\n",
      "test_loss :  1.5227729909975825\n",
      "train_loss :  1.3162456680509507\n",
      "epoch_num :  356\n",
      "test_loss :  1.5105949106287961\n",
      "train_loss :  1.3103727409948893\n",
      "epoch_num :  361\n",
      "test_loss :  1.4998939180741857\n",
      "train_loss :  1.304640284171971\n",
      "epoch_num :  366\n",
      "test_loss :  1.4907840427317893\n",
      "train_loss :  1.2991816414343906\n",
      "epoch_num :  371\n",
      "test_loss :  1.4833062739962741\n",
      "train_loss :  1.2942232685180444\n",
      "epoch_num :  376\n",
      "test_loss :  1.4774614234285284\n",
      "train_loss :  1.289990719812978\n",
      "epoch_num :  381\n",
      "test_loss :  1.473046355645502\n",
      "train_loss :  1.2865713787107171\n",
      "epoch_num :  386\n",
      "test_loss :  1.469513214614796\n",
      "train_loss :  1.2836449946905917\n",
      "epoch_num :  391\n",
      "test_loss :  1.4666661133149588\n",
      "train_loss :  1.2811079506077137\n",
      "epoch_num :  396\n",
      "test_loss :  1.4644074068175899\n",
      "train_loss :  1.2788948573347496\n",
      "epoch_num :  401\n",
      "test_loss :  1.4626276943768224\n",
      "train_loss :  1.276961317979941\n",
      "epoch_num :  406\n",
      "test_loss :  1.4612404236425531\n",
      "train_loss :  1.2752911091372188\n",
      "epoch_num :  411\n",
      "test_loss :  1.4601996440140925\n",
      "train_loss :  1.2739272047708707\n",
      "epoch_num :  416\n",
      "test_loss :  1.459389257907782\n",
      "train_loss :  1.2728114415963485\n",
      "epoch_num :  421\n",
      "test_loss :  1.4587537425402315\n",
      "train_loss :  1.271861971253794\n",
      "epoch_num :  426\n",
      "test_loss :  1.4582464917016236\n",
      "train_loss :  1.27102031156307\n",
      "epoch_num :  431\n",
      "test_loss :  1.4578319663067785\n",
      "train_loss :  1.2702435319777192\n",
      "epoch_num :  436\n",
      "test_loss :  1.457495920494825\n",
      "train_loss :  1.269508645788759\n",
      "epoch_num :  441\n",
      "test_loss :  1.45722162535642\n",
      "train_loss :  1.268801527443284\n",
      "epoch_num :  446\n",
      "test_loss :  1.4569749202053355\n",
      "train_loss :  1.2681059160542902\n",
      "epoch_num :  451\n",
      "test_loss :  1.4567102719327787\n",
      "train_loss :  1.2673994772535548\n",
      "epoch_num :  456\n",
      "test_loss :  1.4563834351591256\n",
      "train_loss :  1.2666542839734103\n",
      "epoch_num :  461\n",
      "test_loss :  1.455957932029801\n",
      "train_loss :  1.2658384601606376\n",
      "epoch_num :  466\n",
      "test_loss :  1.4554059530464163\n",
      "train_loss :  1.2649179059802786\n",
      "epoch_num :  471\n",
      "test_loss :  1.4547079208895957\n",
      "train_loss :  1.2638592852302322\n",
      "epoch_num :  476\n",
      "test_loss :  1.4538523936904022\n",
      "train_loss :  1.262636413372003\n",
      "epoch_num :  481\n",
      "test_loss :  1.4528333791121129\n",
      "train_loss :  1.261233793339848\n",
      "epoch_num :  486\n",
      "test_loss :  1.4516429808065463\n",
      "train_loss :  1.2596251697406429\n",
      "epoch_num :  491\n",
      "test_loss :  1.450287512543924\n",
      "train_loss :  1.257751633900264\n",
      "epoch_num :  496\n",
      "test_loss :  1.448810666363912\n",
      "train_loss :  1.2555562852109914\n",
      "epoch_num :  501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  1.4472794893830174\n",
      "train_loss :  1.2530246119652313\n",
      "epoch_num :  506\n",
      "test_loss :  1.4457876967542331\n",
      "train_loss :  1.2502423786969084\n",
      "epoch_num :  511\n",
      "test_loss :  1.4444815533512174\n",
      "train_loss :  1.2475033245275422\n",
      "epoch_num :  516\n",
      "test_loss :  1.443528765407456\n",
      "train_loss :  1.2452726258402511\n",
      "epoch_num :  521\n",
      "test_loss :  1.442976932371595\n",
      "train_loss :  1.2437921329863082\n",
      "epoch_num :  526\n",
      "test_loss :  1.4427083529627793\n",
      "train_loss :  1.2428728973226348\n",
      "epoch_num :  531\n",
      "test_loss :  1.4425767919588364\n",
      "train_loss :  1.2422090094505176\n",
      "epoch_num :  536\n",
      "test_loss :  1.4425042605763982\n",
      "train_loss :  1.24161484115472\n",
      "epoch_num :  541\n",
      "test_loss :  1.4424664264484628\n",
      "train_loss :  1.2410212527007949\n",
      "epoch_num :  546\n",
      "test_loss :  1.4424545388712586\n",
      "train_loss :  1.2404208556496397\n",
      "epoch_num :  551\n",
      "test_loss :  1.4424567528047136\n",
      "train_loss :  1.2398314634512457\n",
      "epoch_num :  556\n",
      "test_loss :  1.442456388807146\n",
      "train_loss :  1.2392747974506413\n",
      "epoch_num :  561\n",
      "test_loss :  1.442435714166617\n",
      "train_loss :  1.2387651249449878\n",
      "epoch_num :  566\n",
      "test_loss :  1.4423780724422521\n",
      "train_loss :  1.2383059808648604\n",
      "epoch_num :  571\n",
      "test_loss :  1.4422689279405023\n",
      "train_loss :  1.2378927187190798\n",
      "epoch_num :  576\n",
      "test_loss :  1.4420978107668516\n",
      "train_loss :  1.2375177018299195\n",
      "epoch_num :  581\n",
      "test_loss :  1.4418605637318822\n",
      "train_loss :  1.2371749595375978\n",
      "epoch_num :  586\n",
      "test_loss :  1.4415601913028262\n",
      "train_loss :  1.2368624540866948\n",
      "epoch_num :  591\n",
      "test_loss :  1.44120557780196\n",
      "train_loss :  1.2365818842692555\n",
      "epoch_num :  596\n",
      "test_loss :  1.4408086265293973\n",
      "train_loss :  1.236336928444645\n",
      "epoch_num :  601\n",
      "test_loss :  1.4403808758655592\n",
      "train_loss :  1.2361307554206296\n",
      "epoch_num :  606\n",
      "test_loss :  1.4399303782809127\n",
      "train_loss :  1.2359632447047588\n",
      "epoch_num :  611\n",
      "test_loss :  1.4394588610077748\n",
      "train_loss :  1.2358281901219037\n",
      "epoch_num :  616\n",
      "test_loss :  1.4389585557117612\n",
      "train_loss :  1.2357106385524343\n",
      "epoch_num :  621\n",
      "test_loss :  1.4384092433078925\n",
      "train_loss :  1.235584371179703\n",
      "epoch_num :  626\n",
      "test_loss :  1.43777920486788\n",
      "train_loss :  1.2354106868071546\n",
      "epoch_num :  631\n",
      "test_loss :  1.4370340184475057\n",
      "train_loss :  1.2351425392816595\n",
      "epoch_num :  636\n",
      "test_loss :  1.4361511037143468\n",
      "train_loss :  1.2347369490648208\n",
      "epoch_num :  641\n",
      "test_loss :  1.4351319149710575\n",
      "train_loss :  1.2341695854098582\n",
      "epoch_num :  646\n",
      "test_loss :  1.434005403693462\n",
      "train_loss :  1.2334405066621266\n",
      "epoch_num :  651\n",
      "test_loss :  1.4328240260408727\n",
      "train_loss :  1.2325690464997872\n",
      "epoch_num :  656\n",
      "test_loss :  1.4316577372472499\n",
      "train_loss :  1.2315854702068427\n",
      "epoch_num :  661\n",
      "test_loss :  1.4305882602646467\n",
      "train_loss :  1.2305255873726582\n",
      "epoch_num :  666\n",
      "test_loss :  1.4297009531301006\n",
      "train_loss :  1.2294283234221306\n",
      "epoch_num :  671\n",
      "test_loss :  1.4290705943622939\n",
      "train_loss :  1.2283327323181734\n",
      "epoch_num :  676\n",
      "test_loss :  1.4287426831839711\n",
      "train_loss :  1.227271452340619\n",
      "epoch_num :  681\n",
      "test_loss :  1.4287198839288435\n",
      "train_loss :  1.226261784817788\n",
      "epoch_num :  686\n",
      "test_loss :  1.4289639019570832\n",
      "train_loss :  1.2252998057832205\n",
      "epoch_num :  691\n",
      "test_loss :  1.4294127775422483\n",
      "train_loss :  1.2243611605176241\n",
      "epoch_num :  696\n",
      "test_loss :  1.4300023302802674\n",
      "train_loss :  1.2234073544160895\n",
      "epoch_num :  701\n",
      "test_loss :  1.4306779306956687\n",
      "train_loss :  1.222398733428913\n",
      "epoch_num :  706\n",
      "test_loss :  1.4313924026090845\n",
      "train_loss :  1.2213205603726858\n",
      "epoch_num :  711\n",
      "test_loss :  1.432104979026268\n",
      "train_loss :  1.220216071435522\n",
      "epoch_num :  716\n",
      "test_loss :  1.4328022040887112\n",
      "train_loss :  1.2191908465127752\n",
      "epoch_num :  721\n",
      "test_loss :  1.4335255587191236\n",
      "train_loss :  1.2183667554598092\n",
      "epoch_num :  726\n",
      "test_loss :  1.4343639388486333\n",
      "train_loss :  1.2178263433334546\n",
      "epoch_num :  731\n",
      "test_loss :  1.435415590378317\n",
      "train_loss :  1.2175898652580255\n",
      "epoch_num :  736\n",
      "test_loss :  1.4367565233567432\n",
      "train_loss :  1.2176283795100034\n",
      "epoch_num :  741\n",
      "test_loss :  1.4384260586681248\n",
      "train_loss :  1.217897585967991\n",
      "epoch_num :  746\n",
      "test_loss :  1.4404236785092168\n",
      "train_loss :  1.2183656829072402\n",
      "epoch_num :  751\n",
      "test_loss :  1.442714070776997\n",
      "train_loss :  1.2190158003291367\n",
      "epoch_num :  756\n",
      "test_loss :  1.4452302000473296\n",
      "train_loss :  1.2198268687784595\n",
      "epoch_num :  761\n",
      "test_loss :  1.4478646476680146\n",
      "train_loss :  1.2207513819169693\n",
      "epoch_num :  766\n",
      "test_loss :  1.450458859095273\n",
      "train_loss :  1.2217080789840706\n",
      "epoch_num :  771\n",
      "test_loss :  1.4528185837841041\n",
      "train_loss :  1.222603455705732\n",
      "epoch_num :  776\n",
      "test_loss :  1.4547613046077246\n",
      "train_loss :  1.2233716871109725\n",
      "epoch_num :  781\n",
      "test_loss :  1.4561572917663554\n",
      "train_loss :  1.2239923995118378\n",
      "epoch_num :  786\n",
      "test_loss :  1.4569416304083702\n",
      "train_loss :  1.2244761609846975\n",
      "epoch_num :  791\n",
      "test_loss :  1.4571183315174285\n",
      "train_loss :  1.2248416150743806\n",
      "epoch_num :  796\n",
      "test_loss :  1.4567500830135593\n",
      "train_loss :  1.2250957453915203\n",
      "epoch_num :  801\n",
      "test_loss :  1.4559081567102667\n",
      "train_loss :  1.2252187959435583\n",
      "epoch_num :  806\n",
      "test_loss :  1.4546307663491562\n",
      "train_loss :  1.2251680562299752\n",
      "epoch_num :  811\n",
      "test_loss :  1.4529248498267064\n",
      "train_loss :  1.224900942461626\n",
      "epoch_num :  816\n",
      "test_loss :  1.4507890514868431\n",
      "train_loss :  1.2243978009162242\n",
      "epoch_num :  821\n",
      "test_loss :  1.4482327616265647\n",
      "train_loss :  1.2236704109036018\n",
      "epoch_num :  826\n",
      "test_loss :  1.4452836305128582\n",
      "train_loss :  1.2227563774957368\n",
      "epoch_num :  831\n",
      "test_loss :  1.441988171053701\n",
      "train_loss :  1.2217081804591852\n",
      "epoch_num :  836\n",
      "test_loss :  1.4384105337817443\n",
      "train_loss :  1.2205835379175112\n",
      "epoch_num :  841\n",
      "test_loss :  1.4346298888446631\n",
      "train_loss :  1.2194381029255121\n",
      "epoch_num :  846\n",
      "test_loss :  1.4307349413271706\n",
      "train_loss :  1.2183195454556948\n",
      "epoch_num :  851\n",
      "test_loss :  1.4268154315913786\n",
      "train_loss :  1.2172627259146747\n",
      "epoch_num :  856\n",
      "test_loss :  1.422952582286694\n",
      "train_loss :  1.2162862622510935\n",
      "epoch_num :  861\n",
      "test_loss :  1.41921169972824\n",
      "train_loss :  1.215390937501648\n",
      "epoch_num :  866\n",
      "test_loss :  1.4156397373828762\n",
      "train_loss :  1.2145606338559651\n",
      "epoch_num :  871\n",
      "test_loss :  1.4122680679608082\n",
      "train_loss :  1.213766219508265\n",
      "epoch_num :  876\n",
      "test_loss :  1.409117131260622\n",
      "train_loss :  1.21297126405085\n",
      "epoch_num :  881\n",
      "test_loss :  1.4061989200281022\n",
      "train_loss :  1.2121373999429876\n",
      "epoch_num :  886\n",
      "test_loss :  1.4035168952774713\n",
      "train_loss :  1.2112283921412916\n",
      "epoch_num :  891\n",
      "test_loss :  1.4010659844775308\n",
      "train_loss :  1.2102133665419623\n",
      "epoch_num :  896\n",
      "test_loss :  1.398834352063727\n",
      "train_loss :  1.2090692301294046\n",
      "epoch_num :  901\n",
      "test_loss :  1.396806385116002\n",
      "train_loss :  1.2077816562808121\n",
      "epoch_num :  906\n",
      "test_loss :  1.3949656131273718\n",
      "train_loss :  1.2063444448907727\n",
      "epoch_num :  911\n",
      "test_loss :  1.3932967075967093\n",
      "train_loss :  1.2047578299271888\n",
      "epoch_num :  916\n",
      "test_loss :  1.3917862414081248\n",
      "train_loss :  1.2030265159906557\n",
      "epoch_num :  921\n",
      "test_loss :  1.3904222887307576\n",
      "train_loss :  1.2011579772757033\n",
      "epoch_num :  926\n",
      "test_loss :  1.3891932393994413\n",
      "train_loss :  1.1991612511142993\n",
      "epoch_num :  931\n",
      "test_loss :  1.3880863645235266\n",
      "train_loss :  1.1970462704349614\n",
      "epoch_num :  936\n",
      "test_loss :  1.387086717782735\n",
      "train_loss :  1.194823690584978\n",
      "epoch_num :  941\n",
      "test_loss :  1.386176915469439\n",
      "train_loss :  1.1925051227537409\n",
      "epoch_num :  946\n",
      "test_loss :  1.3853381914485274\n",
      "train_loss :  1.190103641304678\n",
      "epoch_num :  951\n",
      "test_loss :  1.3845529163348804\n",
      "train_loss :  1.1876343728759822\n",
      "epoch_num :  956\n",
      "test_loss :  1.3837885240109342\n",
      "train_loss :  1.1851132376269384\n",
      "epoch_num :  961\n",
      "test_loss :  1.3821996748825582\n",
      "train_loss :  1.18223016030431\n",
      "epoch_num :  966\n",
      "test_loss :  1.3810111592397694\n",
      "train_loss :  1.179800609230497\n",
      "epoch_num :  971\n",
      "test_loss :  1.3807379941259228\n",
      "train_loss :  1.1770641666645152\n",
      "epoch_num :  976\n",
      "test_loss :  1.3806239943992635\n",
      "train_loss :  1.1748496062341245\n",
      "epoch_num :  981\n",
      "test_loss :  1.3801855921938626\n",
      "train_loss :  1.1721204565306593\n",
      "epoch_num :  986\n",
      "test_loss :  1.3802048763327364\n",
      "train_loss :  1.17007919229146\n",
      "epoch_num :  991\n",
      "test_loss :  1.379847986499977\n",
      "train_loss :  1.1676476997360692\n",
      "epoch_num :  996\n"
     ]
    }
   ],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "biases1_squares = 0\n",
    "biases2_squares = 0\n",
    "weights1_squares = 0\n",
    "weights2_squares = 0\n",
    "\n",
    "biases1_momentum = 0\n",
    "biases2_momentum = 0\n",
    "weights1_momentum = 0\n",
    "weights2_momentum = 0\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "    if epoch_num%5==1:\n",
    "        print(\"test_loss : \", test_losses[-1])\n",
    "        print(\"train_loss : \", losses[-1])\n",
    "        print(\"epoch_num : \", epoch_num)\n",
    "    for sample_num in range(x_train.shape[1]):\n",
    "#         print(\"sample_num : \", sample_num)\n",
    "        #Forward propagation\n",
    "        out1_temp = np.matmul(weights1.T, x_train[:,sample_num])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        #Back-propogation\n",
    "        error_out = out2-y_train[:, sample_num]\n",
    "        err_2 = np.matrix(error_out)\n",
    "        grad_2 = np.matmul(out1, err_2.T)\n",
    "        err_1 = np.multiply(np.matmul(weights2, err_2), np.multiply(out1, np.ones(out1.shape)-out1))\n",
    "        grad_1 = np.multiply(x_train[:, sample_num], err_1.T)\n",
    "        #Updating the weights\n",
    "        biases1_momentum = alpha1*biases1_momentum+(1-alpha1)*err_1\n",
    "        biases2_momentum = alpha1*biases2_momentum+(1-alpha1)*err_2\n",
    "        weights1_momentum = alpha1*weights1_momentum+(1-alpha1)*grad_1\n",
    "        weights2_momentum = alpha1*weights2_momentum+(1-alpha1)*grad_2\n",
    "        \n",
    "        biases1_squares = alpha*biases1_squares+(1-alpha)*np.multiply(err_1, err_1)\n",
    "        biases2_squares = alpha*biases2_squares+(1-alpha)*np.multiply(err_2, err_2)\n",
    "        weights1_squares = alpha*weights1_squares+(1-alpha)*np.multiply(grad_1, grad_1)\n",
    "        weights2_squares = alpha*weights2_squares+(1-alpha)*np.multiply(grad_2, grad_2)\n",
    "        \n",
    "        if epoch_num==0 and sample_num<50:\n",
    "            temp_num = sample_num+1\n",
    "            biases1_momentum_cap = biases1_momentum/(1-alpha1**temp_num)\n",
    "            biases2_momentum_cap = biases2_momentum/(1-alpha1**temp_num)\n",
    "            weights1_momentum_cap = weights1_momentum/(1-alpha1**temp_num)\n",
    "            weights2_momentum_cap = weights2_momentum/(1-alpha1**temp_num)\n",
    "            \n",
    "            biases1_squares_cap = biases1_squares/(1-alpha**temp_num)\n",
    "            biases2_squares_cap = biases2_squares/(1-alpha**temp_num)\n",
    "            weights1_squares_cap = weights1_squares/(1-alpha**temp_num)\n",
    "            weights2_squares_cap = weights2_squares/(1-alpha**temp_num)\n",
    "        else:\n",
    "            biases1_momentum_cap = biases1_momentum\n",
    "            biases2_momentum_cap = biases2_momentum\n",
    "            weights1_momentum_cap = weights1_momentum\n",
    "            weights2_momentum_cap = weights2_momentum\n",
    "\n",
    "            biases1_squares_cap = biases1_squares\n",
    "            biases2_squares_cap = biases2_squares\n",
    "            weights1_squares_cap = weights1_squares\n",
    "            weights2_squares_cap = weights2_squares\n",
    "\n",
    "        biases1 = biases1 - np.multiply(learning_rate/(np.sqrt(biases1_squares_cap)+epsilon), biases1_momentum_cap)\n",
    "        biases2 = biases2 - np.multiply(learning_rate/(np.sqrt(biases2_squares_cap)+epsilon), biases2_momentum_cap)\n",
    "        weights1 = weights1 - np.multiply(learning_rate/(np.sqrt(weights1_squares_cap)+epsilon), weights1_momentum_cap)\n",
    "        weights2 = weights2 - np.multiply(learning_rate/(np.sqrt(weights2_squares_cap)+epsilon), weights2_momentum_cap)\n",
    "        #Updating the accuracy and loss\n",
    "        #End of for loop\n",
    "#     print(\"accuracy : \", np.mean(acc_arr))\n",
    "    test_losses.append(test_loss(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_test.append(acc_out(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_train.append(acc_out(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    losses.append(test_loss(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot(211)\n",
    "plt.plot(losses[1:], label = 'train_loss')\n",
    "plt.plot(test_losses[1:], label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.multiply(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test, pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.29670849  9.31566072 17.01299125 13.02677299 17.1354265 ]\n",
      " [16.79471888 11.70481402 17.40165717 15.96902678 20.36823485]]\n",
      "[[16.35  8.45 17.17 13.17 17.02]\n",
      " [17.14 12.08 17.21 16.39 20.48]]\n"
     ]
    }
   ],
   "source": [
    "ind = 101\n",
    "print(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)[:,10:15])\n",
    "print(y_test[:,10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
