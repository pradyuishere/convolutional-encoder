{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def test_loss(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out2 = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "        loss = []\n",
    "        for iter in range(out2.shape[1]):\n",
    "            loss.append(MSE(out2[:, iter], y_in[:, iter]))\n",
    "        return np.mean(loss)\n",
    "\n",
    "def pred_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out1_temp = np.matmul(weights1.T, x_in[:,:])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        return out2\n",
    "\n",
    "def acc_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "    pred = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "    sum_acc = 0\n",
    "    for iter in range(pred.shape[1]):\n",
    "#         print(iter)\n",
    "        sum_acc+=(np.argmax(pred[:, iter])==np.argmax(y_in[:, iter]))\n",
    "    return sum_acc/y_in.shape[1]\n",
    "\n",
    "def MSE(yHat, y):\n",
    "    return np.sum(np.multiply(yHat - y, yHat - y)) / y.size\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-5):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.mean(np.multiply(np.log(predictions),targets)) \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 10)\n"
     ]
    }
   ],
   "source": [
    "##Load and segregate the data\n",
    "letters = genfromtxt('energy_eff.csv', delimiter=',')\n",
    "np.random.shuffle(letters)\n",
    "print(letters.shape)\n",
    "x_in = letters[:, 0:8]\n",
    "x_mean = np.mean(x_in, axis = 0)\n",
    "x_std = np.std(x_in, axis = 0)\n",
    "y_in = letters[:, 8:10]\n",
    "y_mean = np.mean(y_in, axis=0)\n",
    "y_std = np.std(y_in, axis = 0)\n",
    "# y_in = (y_in-y_mean)/y_std\n",
    "x_in = (x_in-x_mean)/x_std\n",
    "# print(np.std(y_in, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_in[0:int(x_in.shape[0]*0.7), :]\n",
    "y_train = y_in[0:int(x_in.shape[0]*0.7), :]\n",
    "\n",
    "x_test = x_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]\n",
    "y_test = y_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "weights1 = np.random.normal(size=(x_in.shape[1], hidden_size))\n",
    "weights2 = np.random.normal(size=(hidden_size, y_in.shape[1]))\n",
    "print(weights1.shape)\n",
    "print(weights2.shape)\n",
    "biases1 = np.random.normal(size=(hidden_size, 1))\n",
    "biases2 = np.random.normal(size=(y_in.shape[1], 1))\n",
    "\n",
    "x_train = np.matrix(x_train)\n",
    "x_train = x_train.T\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = np.matrix(x_test)\n",
    "x_test = x_test.T\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies :  0\n",
      "test_loss :  29.75354961481233\n",
      "train_loss :  23.618484705551253\n",
      "epoch_num :  1\n",
      "test_accuracies :  0\n",
      "test_loss :  11.327626790087301\n",
      "train_loss :  9.379845807969001\n",
      "epoch_num :  6\n",
      "test_accuracies :  0\n",
      "test_loss :  9.973262413431245\n",
      "train_loss :  8.02083645586244\n",
      "epoch_num :  11\n",
      "test_accuracies :  0\n",
      "test_loss :  9.175482164159188\n",
      "train_loss :  7.281461279344217\n",
      "epoch_num :  16\n",
      "test_accuracies :  0\n",
      "test_loss :  8.509597938124042\n",
      "train_loss :  6.789375887979585\n",
      "epoch_num :  21\n",
      "test_accuracies :  0\n",
      "test_loss :  7.874657238141396\n",
      "train_loss :  6.385620517519417\n",
      "epoch_num :  26\n",
      "test_accuracies :  0\n",
      "test_loss :  7.282474060600906\n",
      "train_loss :  6.011412799122345\n",
      "epoch_num :  31\n",
      "test_accuracies :  0\n",
      "test_loss :  6.767158811207754\n",
      "train_loss :  5.647805721993913\n",
      "epoch_num :  36\n",
      "test_accuracies :  0\n",
      "test_loss :  6.309003431698291\n",
      "train_loss :  5.30104389087586\n",
      "epoch_num :  41\n",
      "test_accuracies :  0\n",
      "test_loss :  5.8905508034607035\n",
      "train_loss :  4.978183674393061\n",
      "epoch_num :  46\n",
      "test_accuracies :  0\n",
      "test_loss :  5.499734695397248\n",
      "train_loss :  4.668597356642268\n",
      "epoch_num :  51\n",
      "test_accuracies :  0\n",
      "test_loss :  5.113029326456651\n",
      "train_loss :  4.358187123315957\n",
      "epoch_num :  56\n",
      "test_accuracies :  0\n",
      "test_loss :  4.744119970182576\n",
      "train_loss :  4.062517052956058\n",
      "epoch_num :  61\n",
      "test_accuracies :  0\n",
      "test_loss :  4.424986081376331\n",
      "train_loss :  3.807478529726942\n",
      "epoch_num :  66\n",
      "test_accuracies :  0\n",
      "test_loss :  4.154131448323676\n",
      "train_loss :  3.588814894395845\n",
      "epoch_num :  71\n",
      "test_accuracies :  0\n",
      "test_loss :  3.9220348353851073\n",
      "train_loss :  3.398708408392849\n",
      "epoch_num :  76\n",
      "test_accuracies :  0\n",
      "test_loss :  3.7217570035239347\n",
      "train_loss :  3.232253052812204\n",
      "epoch_num :  81\n",
      "test_accuracies :  0\n",
      "test_loss :  3.548213355818065\n",
      "train_loss :  3.0858675449609176\n",
      "epoch_num :  86\n",
      "test_accuracies :  0\n",
      "test_loss :  3.3973198169803074\n",
      "train_loss :  2.9565747470206474\n",
      "epoch_num :  91\n",
      "test_accuracies :  0\n",
      "test_loss :  3.2656037603191215\n",
      "train_loss :  2.841760770352722\n",
      "epoch_num :  96\n",
      "test_accuracies :  0\n",
      "test_loss :  3.1500429126767395\n",
      "train_loss :  2.7390998813470677\n",
      "epoch_num :  101\n",
      "test_accuracies :  0\n",
      "test_loss :  3.0480175063428225\n",
      "train_loss :  2.646548430851783\n",
      "epoch_num :  106\n",
      "test_accuracies :  0\n",
      "test_loss :  2.9573363540433197\n",
      "train_loss :  2.5623705554535845\n",
      "epoch_num :  111\n",
      "test_accuracies :  0\n",
      "test_loss :  2.87633214676206\n",
      "train_loss :  2.4851499884899515\n",
      "epoch_num :  116\n",
      "test_accuracies :  0\n",
      "test_loss :  2.803998128097981\n",
      "train_loss :  2.4136918004193877\n",
      "epoch_num :  121\n",
      "test_accuracies :  0\n",
      "test_loss :  2.7401110416912777\n",
      "train_loss :  2.3467177744335412\n",
      "epoch_num :  126\n",
      "test_accuracies :  0\n",
      "test_loss :  2.6852714166512253\n",
      "train_loss :  2.282571499155706\n",
      "epoch_num :  131\n",
      "test_accuracies :  0\n",
      "test_loss :  2.6398588550216173\n",
      "train_loss :  2.2204096088577443\n",
      "epoch_num :  136\n",
      "test_accuracies :  0\n",
      "test_loss :  2.600020086679955\n",
      "train_loss :  2.1636933590286636\n",
      "epoch_num :  141\n",
      "test_accuracies :  0\n",
      "test_loss :  2.5604044449577983\n",
      "train_loss :  2.1155674352004508\n",
      "epoch_num :  146\n",
      "test_accuracies :  0\n",
      "test_loss :  2.521377942466794\n",
      "train_loss :  2.073925135797649\n",
      "epoch_num :  151\n",
      "test_accuracies :  0\n",
      "test_loss :  2.4843795584920954\n",
      "train_loss :  2.036517604053738\n",
      "epoch_num :  156\n",
      "test_accuracies :  0\n",
      "test_loss :  2.4496984602111596\n",
      "train_loss :  2.0022257131320687\n",
      "epoch_num :  161\n",
      "test_accuracies :  0\n",
      "test_loss :  2.4171405250732994\n",
      "train_loss :  1.9704290543604128\n",
      "epoch_num :  166\n",
      "test_accuracies :  0\n",
      "test_loss :  2.3864382713981382\n",
      "train_loss :  1.9406989937662884\n",
      "epoch_num :  171\n",
      "test_accuracies :  0\n",
      "test_loss :  2.357359533642904\n",
      "train_loss :  1.9127019822612397\n",
      "epoch_num :  176\n",
      "test_accuracies :  0\n",
      "test_loss :  2.329721588143863\n",
      "train_loss :  1.8861665136569872\n",
      "epoch_num :  181\n",
      "test_accuracies :  0\n",
      "test_loss :  2.3033836779890713\n",
      "train_loss :  1.8608682270431913\n",
      "epoch_num :  186\n",
      "test_accuracies :  0\n",
      "test_loss :  2.2782363660142604\n",
      "train_loss :  1.8366205609009119\n",
      "epoch_num :  191\n",
      "test_accuracies :  0\n",
      "test_loss :  2.254191682982061\n",
      "train_loss :  1.813267326584495\n",
      "epoch_num :  196\n",
      "test_accuracies :  0\n",
      "test_loss :  2.2311751218379183\n",
      "train_loss :  1.7906762620466625\n",
      "epoch_num :  201\n",
      "test_accuracies :  0\n",
      "test_loss :  2.209120290016855\n",
      "train_loss :  1.7687337787186752\n",
      "epoch_num :  206\n",
      "test_accuracies :  0\n",
      "test_loss :  2.1879671083510845\n",
      "train_loss :  1.7473419146522364\n",
      "epoch_num :  211\n",
      "test_accuracies :  0\n",
      "test_loss :  2.1676642141344034\n",
      "train_loss :  1.7264191144778331\n",
      "epoch_num :  216\n",
      "test_accuracies :  0\n",
      "test_loss :  2.1481753818995153\n",
      "train_loss :  1.7059064399223127\n",
      "epoch_num :  221\n",
      "test_accuracies :  0\n",
      "test_loss :  2.129487915026687\n",
      "train_loss :  1.685779095582414\n",
      "epoch_num :  226\n",
      "test_accuracies :  0\n",
      "test_loss :  2.111618110815988\n",
      "train_loss :  1.666058687486798\n",
      "epoch_num :  231\n",
      "test_accuracies :  0\n",
      "test_loss :  2.0946072054690688\n",
      "train_loss :  1.6468161953230405\n",
      "epoch_num :  236\n",
      "test_accuracies :  0\n",
      "test_loss :  2.0785050911507814\n",
      "train_loss :  1.6281563545646145\n",
      "epoch_num :  241\n",
      "test_accuracies :  0\n",
      "test_loss :  2.0633490272649615\n",
      "train_loss :  1.6101865985111852\n",
      "epoch_num :  246\n",
      "test_accuracies :  0\n",
      "test_loss :  2.04915070333225\n",
      "train_loss :  1.5929879245874725\n",
      "epoch_num :  251\n",
      "test_accuracies :  0\n",
      "test_loss :  2.035898239350349\n",
      "train_loss :  1.5766031705776382\n",
      "epoch_num :  256\n",
      "test_accuracies :  0\n",
      "test_loss :  2.0235676200699744\n",
      "train_loss :  1.5610425818808813\n",
      "epoch_num :  261\n",
      "test_accuracies :  0\n",
      "test_loss :  2.0121339103013387\n",
      "train_loss :  1.5462962557809088\n",
      "epoch_num :  266\n",
      "test_accuracies :  0\n",
      "test_loss :  2.001576724058368\n",
      "train_loss :  1.532344753801393\n",
      "epoch_num :  271\n",
      "test_accuracies :  0\n",
      "test_loss :  1.991879524784055\n",
      "train_loss :  1.5191650748914227\n",
      "epoch_num :  276\n",
      "test_accuracies :  0\n",
      "test_loss :  1.9830249926622023\n",
      "train_loss :  1.5067328604901522\n",
      "epoch_num :  281\n",
      "test_accuracies :  0\n",
      "test_loss :  1.9749893730293693\n",
      "train_loss :  1.495022582034651\n",
      "epoch_num :  286\n",
      "test_accuracies :  0\n",
      "test_loss :  1.9677382411717013\n",
      "train_loss :  1.4840069955855326\n",
      "epoch_num :  291\n",
      "test_accuracies :  0\n",
      "test_loss :  1.9612249769533086\n",
      "train_loss :  1.4736564622579722\n",
      "epoch_num :  296\n",
      "test_accuracies :  0\n",
      "test_loss :  1.9553919801272204\n",
      "train_loss :  1.4639383199788267\n",
      "epoch_num :  301\n",
      "test_accuracies :  0\n",
      "test_loss :  1.950173752935736\n",
      "train_loss :  1.4548163734515107\n",
      "epoch_num :  306\n",
      "test_accuracies :  0\n",
      "test_loss :  1.945500627135673\n",
      "train_loss :  1.446250555964985\n",
      "epoch_num :  311\n",
      "test_accuracies :  0\n",
      "test_loss :  1.941302052851074\n",
      "train_loss :  1.4381967696259965\n",
      "epoch_num :  316\n",
      "test_accuracies :  0\n",
      "test_loss :  1.937508790636581\n",
      "train_loss :  1.4306068268922043\n",
      "epoch_num :  321\n",
      "test_accuracies :  0\n",
      "test_loss :  1.9340538202912467\n",
      "train_loss :  1.423428359701168\n",
      "epoch_num :  326\n",
      "test_accuracies :  0\n",
      "test_loss :  1.9308721400031907\n",
      "train_loss :  1.4166045882226943\n",
      "epoch_num :  331\n",
      "test_accuracies :  0\n",
      "test_loss :  1.9278998584751568\n",
      "train_loss :  1.410073994355908\n",
      "epoch_num :  336\n",
      "test_accuracies :  0\n",
      "test_loss :  1.9250732015572083\n",
      "train_loss :  1.4037703099302679\n",
      "epoch_num :  341\n",
      "test_accuracies :  0\n",
      "test_loss :  1.9223284725838286\n",
      "train_loss :  1.3976239637259362\n",
      "epoch_num :  346\n",
      "test_accuracies :  0\n",
      "test_loss :  1.9196047840634791\n",
      "train_loss :  1.391567330091176\n",
      "epoch_num :  351\n",
      "test_accuracies :  0\n",
      "test_loss :  1.916852160834487\n",
      "train_loss :  1.3855471002464241\n",
      "epoch_num :  356\n",
      "test_accuracies :  0\n",
      "test_loss :  1.9140464302746676\n",
      "train_loss :  1.3795447543360668\n",
      "epoch_num :  361\n",
      "test_accuracies :  0\n",
      "test_loss :  1.9112055291616417\n",
      "train_loss :  1.373595360137348\n",
      "epoch_num :  366\n",
      "test_accuracies :  0\n",
      "test_loss :  1.9083914210316222\n",
      "train_loss :  1.3677810598972822\n",
      "epoch_num :  371\n",
      "test_accuracies :  0\n",
      "test_loss :  1.905685538971462\n",
      "train_loss :  1.3621881978744137\n",
      "epoch_num :  376\n",
      "test_accuracies :  0\n",
      "test_loss :  1.9031525353714147\n",
      "train_loss :  1.3568617816833997\n",
      "epoch_num :  381\n",
      "test_accuracies :  0\n",
      "test_loss :  1.900820595644869\n",
      "train_loss :  1.3517981754898505\n",
      "epoch_num :  386\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8986850319151753\n",
      "train_loss :  1.346968060606712\n",
      "epoch_num :  391\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8967222337273966\n",
      "train_loss :  1.3423387845237995\n",
      "epoch_num :  396\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8949024299050123\n",
      "train_loss :  1.3378841793244207\n",
      "epoch_num :  401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies :  0\n",
      "test_loss :  1.8931973630109624\n",
      "train_loss :  1.333585860362403\n",
      "epoch_num :  406\n",
      "test_accuracies :  0\n",
      "test_loss :  1.891583436705198\n",
      "train_loss :  1.329431508386561\n",
      "epoch_num :  411\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8900421395430196\n",
      "train_loss :  1.3254128206616498\n",
      "epoch_num :  416\n",
      "test_accuracies :  0\n",
      "test_loss :  1.888559275138235\n",
      "train_loss :  1.3215239114833854\n",
      "epoch_num :  421\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8871239005480285\n",
      "train_loss :  1.3177602191736673\n",
      "epoch_num :  426\n",
      "test_accuracies :  0\n",
      "test_loss :  1.885727378701496\n",
      "train_loss :  1.3141178033937035\n",
      "epoch_num :  431\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8843626672436653\n",
      "train_loss :  1.3105929156771106\n",
      "epoch_num :  436\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8830238366994951\n",
      "train_loss :  1.3071817569927782\n",
      "epoch_num :  441\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8817057666442791\n",
      "train_loss :  1.303880362065515\n",
      "epoch_num :  446\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8804039638581882\n",
      "train_loss :  1.3006845673013607\n",
      "epoch_num :  451\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8791144565550586\n",
      "train_loss :  1.2975900305969998\n",
      "epoch_num :  456\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8778337320469085\n",
      "train_loss :  1.2945922797824847\n",
      "epoch_num :  461\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8765586968947698\n",
      "train_loss :  1.291686773257827\n",
      "epoch_num :  466\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8752866473186425\n",
      "train_loss :  1.288868961936212\n",
      "epoch_num :  471\n",
      "test_accuracies :  0\n",
      "test_loss :  1.874015243476646\n",
      "train_loss :  1.2861343459645327\n",
      "epoch_num :  476\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8727424847809557\n",
      "train_loss :  1.283478522900788\n",
      "epoch_num :  481\n",
      "test_accuracies :  0\n",
      "test_loss :  1.871466685376747\n",
      "train_loss :  1.2808972262119502\n",
      "epoch_num :  486\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8701864498495693\n",
      "train_loss :  1.2783863543071607\n",
      "epoch_num :  491\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8689006495711988\n",
      "train_loss :  1.2759419910541403\n",
      "epoch_num :  496\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8676084001281965\n",
      "train_loss :  1.27356041903955\n",
      "epoch_num :  501\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8663090401755928\n",
      "train_loss :  1.2712381268868311\n",
      "epoch_num :  506\n",
      "test_accuracies :  0\n",
      "test_loss :  1.865002111916127\n",
      "train_loss :  1.2689718118545807\n",
      "epoch_num :  511\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8636873432720171\n",
      "train_loss :  1.266758378782351\n",
      "epoch_num :  516\n",
      "test_accuracies :  0\n",
      "test_loss :  1.862364631710313\n",
      "train_loss :  1.2645949362763056\n",
      "epoch_num :  521\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8610340296076566\n",
      "train_loss :  1.2624787908600221\n",
      "epoch_num :  526\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8596957309935314\n",
      "train_loss :  1.2604074396681313\n",
      "epoch_num :  531\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8583500594857736\n",
      "train_loss :  1.258378562136045\n",
      "epoch_num :  536\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8569974572237657\n",
      "train_loss :  1.256390011037516\n",
      "epoch_num :  541\n",
      "test_accuracies :  0\n",
      "test_loss :  1.855638474607641\n",
      "train_loss :  1.2544398031404835\n",
      "epoch_num :  546\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8542737606627497\n",
      "train_loss :  1.2525261096876101\n",
      "epoch_num :  551\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8529040538650878\n",
      "train_loss :  1.2506472468579373\n",
      "epoch_num :  556\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8515301732832008\n",
      "train_loss :  1.248801666327421\n",
      "epoch_num :  561\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8501530099146721\n",
      "train_loss :  1.2469879460163786\n",
      "epoch_num :  566\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8487735181189415\n",
      "train_loss :  1.2452047810892237\n",
      "epoch_num :  571\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8473927070732283\n",
      "train_loss :  1.2434509752546086\n",
      "epoch_num :  576\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8460116322030642\n",
      "train_loss :  1.2417254324010787\n",
      "epoch_num :  581\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8446313865636663\n",
      "train_loss :  1.2400271485934928\n",
      "epoch_num :  586\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8432530921715549\n",
      "train_loss :  1.238355204447997\n",
      "epoch_num :  591\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8418778913079459\n",
      "train_loss :  1.2367087578975866\n",
      "epoch_num :  596\n",
      "test_accuracies :  0\n",
      "test_loss :  1.840506937834532\n",
      "train_loss :  1.2350870373558096\n",
      "epoch_num :  601\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8391413885789658\n",
      "train_loss :  1.2334893352824803\n",
      "epoch_num :  606\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8377823948606802\n",
      "train_loss :  1.231915002152178\n",
      "epoch_num :  611\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8364310942369357\n",
      "train_loss :  1.230363440823561\n",
      "epoch_num :  616\n",
      "test_accuracies :  0\n",
      "test_loss :  1.835088602554672\n",
      "train_loss :  1.2288341013049775\n",
      "epoch_num :  621\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8337560063955183\n",
      "train_loss :  1.2273264759094609\n",
      "epoch_num :  626\n",
      "test_accuracies :  0\n",
      "test_loss :  1.832434355998684\n",
      "train_loss :  1.2258400947899495\n",
      "epoch_num :  631\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8311246587411694\n",
      "train_loss :  1.224374521843345\n",
      "epoch_num :  636\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8298278732454423\n",
      "train_loss :  1.2229293509700532\n",
      "epoch_num :  641\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8285449041734336\n",
      "train_loss :  1.2215042026737706\n",
      "epoch_num :  646\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8272765977523484\n",
      "train_loss :  1.2200987209846663\n",
      "epoch_num :  651\n",
      "test_accuracies :  0\n",
      "test_loss :  1.826023738062838\n",
      "train_loss :  1.218712570687773\n",
      "epoch_num :  656\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8247870441050165\n",
      "train_loss :  1.2173454348373234\n",
      "epoch_num :  661\n",
      "test_accuracies :  0\n",
      "test_loss :  1.823567167642532\n",
      "train_loss :  1.215997012537073\n",
      "epoch_num :  666\n",
      "test_accuracies :  0\n",
      "test_loss :  1.822364691810155\n",
      "train_loss :  1.2146670169662555\n",
      "epoch_num :  671\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8211801304571569\n",
      "train_loss :  1.2133551736307464\n",
      "epoch_num :  676\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8200139281866476\n",
      "train_loss :  1.2120612188192958\n",
      "epoch_num :  681\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8188664610413068\n",
      "train_loss :  1.2107848982452294\n",
      "epoch_num :  686\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8177380377776788\n",
      "train_loss :  1.2095259658547817\n",
      "epoch_num :  691\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8166289016656962\n",
      "train_loss :  1.2082841827842636\n",
      "epoch_num :  696\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8155392327460163\n",
      "train_loss :  1.2070593164493961\n",
      "epoch_num :  701\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8144691504762114\n",
      "train_loss :  1.205851139751392\n",
      "epoch_num :  706\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8134187166969236\n",
      "train_loss :  1.2046594303857356\n",
      "epoch_num :  711\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8123879388505266\n",
      "train_loss :  1.2034839702409226\n",
      "epoch_num :  716\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8113767733880954\n",
      "train_loss :  1.2023245448757844\n",
      "epoch_num :  721\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8103851293044002\n",
      "train_loss :  1.2011809430653266\n",
      "epoch_num :  726\n",
      "test_accuracies :  0\n",
      "test_loss :  1.80941287174568\n",
      "train_loss :  1.20005295640621\n",
      "epoch_num :  731\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8084598256402207\n",
      "train_loss :  1.1989403789741921\n",
      "epoch_num :  736\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8075257793078165\n",
      "train_loss :  1.1978430070268744\n",
      "epoch_num :  741\n",
      "test_accuracies :  0\n",
      "test_loss :  1.80661048800989\n",
      "train_loss :  1.1967606387460477\n",
      "epoch_num :  746\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8057136774081235\n",
      "train_loss :  1.1956930740147824\n",
      "epoch_num :  751\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8048350469049406\n",
      "train_loss :  1.1946401142251457\n",
      "epoch_num :  756\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8039742728446717\n",
      "train_loss :  1.1936015621130587\n",
      "epoch_num :  761\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8031310115591834\n",
      "train_loss :  1.1925772216173638\n",
      "epoch_num :  766\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8023049022462219\n",
      "train_loss :  1.1915668977606073\n",
      "epoch_num :  771\n",
      "test_accuracies :  0\n",
      "test_loss :  1.8014955696727852\n",
      "train_loss :  1.1905703965494592\n",
      "epoch_num :  776\n",
      "test_accuracies :  0\n",
      "test_loss :  1.800702626699295\n",
      "train_loss :  1.1895875248929486\n",
      "epoch_num :  781\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7999256766235001\n",
      "train_loss :  1.1886180905370016\n",
      "epoch_num :  786\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7991643153454355\n",
      "train_loss :  1.1876619020139243\n",
      "epoch_num :  791\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7984181333569842\n",
      "train_loss :  1.1867187686056337\n",
      "epoch_num :  796\n",
      "test_accuracies :  0\n",
      "test_loss :  1.797686717561188\n",
      "train_loss :  1.1857885003195856\n",
      "epoch_num :  801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies :  0\n",
      "test_loss :  1.7969696529278212\n",
      "train_loss :  1.1848709078764084\n",
      "epoch_num :  806\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7962665239926436\n",
      "train_loss :  1.1839658027083186\n",
      "epoch_num :  811\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7955769162085453\n",
      "train_loss :  1.1830729969674643\n",
      "epoch_num :  816\n",
      "test_accuracies :  0\n",
      "test_loss :  1.794900417157055\n",
      "train_loss :  1.1821923035433608\n",
      "epoch_num :  821\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7942366176290885\n",
      "train_loss :  1.1813235360886079\n",
      "epoch_num :  826\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7935851125836648\n",
      "train_loss :  1.180466509052107\n",
      "epoch_num :  831\n",
      "test_accuracies :  0\n",
      "test_loss :  1.792945501993441\n",
      "train_loss :  1.1796210377190004\n",
      "epoch_num :  836\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7923173915854607\n",
      "train_loss :  1.1787869382565845\n",
      "epoch_num :  841\n",
      "test_accuracies :  0\n",
      "test_loss :  1.791700393485436\n",
      "train_loss :  1.1779640277654442\n",
      "epoch_num :  846\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7910941267733747\n",
      "train_loss :  1.1771521243350844\n",
      "epoch_num :  851\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7904982179580708\n",
      "train_loss :  1.1763510471033638\n",
      "epoch_num :  856\n",
      "test_accuracies :  0\n",
      "test_loss :  1.78991230137744\n",
      "train_loss :  1.1755606163190226\n",
      "epoch_num :  861\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7893360195314367\n",
      "train_loss :  1.1747806534066494\n",
      "epoch_num :  866\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7887690233535298\n",
      "train_loss :  1.1740109810334671\n",
      "epoch_num :  871\n",
      "test_accuracies :  0\n",
      "test_loss :  1.788210972426714\n",
      "train_loss :  1.173251423177311\n",
      "epoch_num :  876\n",
      "test_accuracies :  0\n",
      "test_loss :  1.787661535149176\n",
      "train_loss :  1.1725018051952407\n",
      "epoch_num :  881\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7871203888546734\n",
      "train_loss :  1.1717619538922543\n",
      "epoch_num :  886\n",
      "test_accuracies :  0\n",
      "test_loss :  1.786587219892117\n",
      "train_loss :  1.1710316975895991\n",
      "epoch_num :  891\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7860617236684873\n",
      "train_loss :  1.1703108661922403\n",
      "epoch_num :  896\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7855436046590665\n",
      "train_loss :  1.1695992912550566\n",
      "epoch_num :  901\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7850325763883754\n",
      "train_loss :  1.1688968060474056\n",
      "epoch_num :  906\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7845283613851215\n",
      "train_loss :  1.168203245615729\n",
      "epoch_num :  911\n",
      "test_accuracies :  0\n",
      "test_loss :  1.784030691114169\n",
      "train_loss :  1.1675184468438973\n",
      "epoch_num :  916\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7835393058881865\n",
      "train_loss :  1.1668422485110748\n",
      "epoch_num :  921\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7830539547615285\n",
      "train_loss :  1.1661744913468755\n",
      "epoch_num :  926\n",
      "test_accuracies :  0\n",
      "test_loss :  1.782574395408613\n",
      "train_loss :  1.1655150180836604\n",
      "epoch_num :  931\n",
      "test_accuracies :  0\n",
      "test_loss :  1.782100393988894\n",
      "train_loss :  1.1648636735058437\n",
      "epoch_num :  936\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7816317250003355\n",
      "train_loss :  1.1642203044961033\n",
      "epoch_num :  941\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7811681711232172\n",
      "train_loss :  1.1635847600784477\n",
      "epoch_num :  946\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7807095230557268\n",
      "train_loss :  1.162956891458092\n",
      "epoch_num :  951\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7802555793429315\n",
      "train_loss :  1.162336552058153\n",
      "epoch_num :  956\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7798061462003976\n",
      "train_loss :  1.1617235975531552\n",
      "epoch_num :  961\n",
      "test_accuracies :  0\n",
      "test_loss :  1.779361037333636\n",
      "train_loss :  1.1611178858994107\n",
      "epoch_num :  966\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7789200737544684\n",
      "train_loss :  1.1605192773623136\n",
      "epoch_num :  971\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7784830835953809\n",
      "train_loss :  1.1599276345406193\n",
      "epoch_num :  976\n",
      "test_accuracies :  0\n",
      "test_loss :  1.778049901922594\n",
      "train_loss :  1.159342822387801\n",
      "epoch_num :  981\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7776203705486897\n",
      "train_loss :  1.1587647082305637\n",
      "epoch_num :  986\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7771943378455446\n",
      "train_loss :  1.1581931617846226\n",
      "epoch_num :  991\n",
      "test_accuracies :  0\n",
      "test_loss :  1.7767716585580733\n",
      "train_loss :  1.1576280551678484\n",
      "epoch_num :  996\n"
     ]
    }
   ],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = [0]\n",
    "losses = [0]\n",
    "test_losses = [0]\n",
    "for epoch_num in range(num_epochs):\n",
    "    if epoch_num%5==1:\n",
    "        print(\"test_accuracies : \", accuracies_test[-1])\n",
    "        print(\"test_loss : \", test_losses[-1])\n",
    "        print(\"train_loss : \", losses[-1])\n",
    "        print(\"epoch_num : \", epoch_num)\n",
    "    for sample_num in range(x_train.shape[1]):\n",
    "#         if(sample_num%100==0):\n",
    "#         print(\"sample_num : \", sample_num)\n",
    "        #Forward propagation\n",
    "        out1_temp = np.matmul(weights1.T, x_train[:,sample_num])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        #Back-propogation\n",
    "        error_out = out2-y_train[:, sample_num]\n",
    "        err_2 = np.matrix(error_out)\n",
    "        grad_2 = np.matmul(out1, err_2.T)\n",
    "        err_1 = np.multiply(np.matmul(weights2, err_2), np.multiply(out1, np.ones(out1.shape)-out1))\n",
    "        grad_1 = np.multiply(x_train[:, sample_num], err_1.T)\n",
    "        #Updating the weights\n",
    "#         print(weights1)\n",
    "        biases1 = biases1 - learning_rate*err_1\n",
    "        biases2 = biases2 - learning_rate*err_2\n",
    "        weights1 = weights1-learning_rate*grad_1\n",
    "        weights2 = weights2-learning_rate*grad_2\n",
    "#         print(cross_entropy(out2, y_train[:, sample_num]))\n",
    "        #Updating the accuracy and loss\n",
    "        #End of for loop\n",
    "#     print(\"accuracy : \", np.mean(acc_arr))\n",
    "    test_losses.append(test_loss(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_test.append(acc_out(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_train.append(acc_out(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    losses.append(test_loss(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot(211)\n",
    "plt.plot(losses[1:], label = 'train_loss')\n",
    "plt.plot(test_losses[1:], label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.multiply(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test, pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36.02099539 14.57721285 14.46854963 15.5554453  12.86458578]\n",
      " [38.6340822  15.29903166 15.18006334 19.09571778 14.1094213 ]]\n",
      "[[35.56 14.34 14.6  15.19 12.97]\n",
      " [41.68 15.32 15.14 19.3  14.27]]\n"
     ]
    }
   ],
   "source": [
    "ind = 101\n",
    "print(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)[:,5:10])\n",
    "print(y_test[:,5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_losses)\n",
    "# print(losses)\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(weights1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
