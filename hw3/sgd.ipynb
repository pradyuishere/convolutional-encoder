{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def test_loss(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out2 = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "        loss = []\n",
    "        for iter in range(out2.shape[1]):\n",
    "            loss.append(MSE(out2[:, iter], y_in[:, iter]))\n",
    "        return np.mean(loss)\n",
    "\n",
    "def pred_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out1_temp = np.matmul(weights1.T, x_in[:,:])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        return out2\n",
    "\n",
    "def acc_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "    pred = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "    sum_acc = 0\n",
    "    for iter in range(pred.shape[1]):\n",
    "#         print(iter)\n",
    "        sum_acc+=(np.argmax(pred[:, iter])==np.argmax(y_in[:, iter]))\n",
    "    return sum_acc/y_in.shape[1]\n",
    "\n",
    "def MSE(yHat, y):\n",
    "    return np.sum(np.multiply(yHat - y, yHat - y)) / y.size\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-5):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.mean(np.multiply(np.log(predictions),targets)) \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 10)\n"
     ]
    }
   ],
   "source": [
    "##Load and segregate the data\n",
    "letters = genfromtxt('energy_eff2.csv', delimiter=',')\n",
    "# np.random.shuffle(letters)\n",
    "print(letters.shape)\n",
    "x_in = letters[:, 0:8]\n",
    "x_mean = np.mean(x_in, axis = 0)\n",
    "x_std = np.std(x_in, axis = 0)\n",
    "y_in = letters[:, 8:10]\n",
    "y_mean = np.mean(y_in, axis=0)\n",
    "y_std = np.std(y_in, axis = 0)\n",
    "# y_in = (y_in-y_mean)/y_std\n",
    "x_in = (x_in-x_mean)/x_std\n",
    "# print(np.std(y_in, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_in[0:int(x_in.shape[0]*0.7), :]\n",
    "y_train = y_in[0:int(x_in.shape[0]*0.7), :]\n",
    "\n",
    "x_test = x_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]\n",
    "y_test = y_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "weights1 = np.random.normal(size=(x_in.shape[1], hidden_size))\n",
    "weights2 = np.random.normal(size=(hidden_size, y_in.shape[1]))\n",
    "print(weights1.shape)\n",
    "print(weights2.shape)\n",
    "biases1 = np.random.normal(size=(hidden_size, 1))\n",
    "biases2 = np.random.normal(size=(y_in.shape[1], 1))\n",
    "\n",
    "x_train = np.matrix(x_train)\n",
    "x_train = x_train.T\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = np.matrix(x_test)\n",
    "x_test = x_test.T\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  17.908185547035934\n",
      "train_loss :  23.202966111005843\n",
      "epoch_num :  1\n",
      "test_loss :  9.521261048970693\n",
      "train_loss :  9.704199367335063\n",
      "epoch_num :  6\n",
      "test_loss :  7.760798810157399\n",
      "train_loss :  7.9144311584244225\n",
      "epoch_num :  11\n",
      "test_loss :  7.075297536033684\n",
      "train_loss :  7.145296236464155\n",
      "epoch_num :  16\n",
      "test_loss :  6.485466941845687\n",
      "train_loss :  6.466516593712417\n",
      "epoch_num :  21\n",
      "test_loss :  5.982518094598379\n",
      "train_loss :  5.908868828678322\n",
      "epoch_num :  26\n",
      "test_loss :  5.521820659009128\n",
      "train_loss :  5.435824457770141\n",
      "epoch_num :  31\n",
      "test_loss :  5.050591809167781\n",
      "train_loss :  4.978876207207795\n",
      "epoch_num :  36\n",
      "test_loss :  4.734559385886745\n",
      "train_loss :  4.617666365616537\n",
      "epoch_num :  41\n",
      "test_loss :  4.539854310307959\n",
      "train_loss :  4.350233794113743\n",
      "epoch_num :  46\n",
      "test_loss :  4.396947337765248\n",
      "train_loss :  4.1357850765521995\n",
      "epoch_num :  51\n",
      "test_loss :  4.277938463889775\n",
      "train_loss :  3.9573411989815046\n",
      "epoch_num :  56\n",
      "test_loss :  4.170884738078605\n",
      "train_loss :  3.8057355562608017\n",
      "epoch_num :  61\n",
      "test_loss :  4.0698943510863925\n",
      "train_loss :  3.674545276858234\n",
      "epoch_num :  66\n",
      "test_loss :  3.971749459848554\n",
      "train_loss :  3.558919941598961\n",
      "epoch_num :  71\n",
      "test_loss :  3.8749073660401274\n",
      "train_loss :  3.455226687328405\n",
      "epoch_num :  76\n",
      "test_loss :  3.7790529635205883\n",
      "train_loss :  3.360825949280231\n",
      "epoch_num :  81\n",
      "test_loss :  3.684633775450107\n",
      "train_loss :  3.2738542589890995\n",
      "epoch_num :  86\n",
      "test_loss :  3.5923915028484275\n",
      "train_loss :  3.193013209172152\n",
      "epoch_num :  91\n",
      "test_loss :  3.503020017669579\n",
      "train_loss :  3.1173752503181236\n",
      "epoch_num :  96\n",
      "test_loss :  3.4169829442658815\n",
      "train_loss :  3.0462067100645824\n",
      "epoch_num :  101\n",
      "test_loss :  3.3344412232378136\n",
      "train_loss :  2.9787923180505493\n",
      "epoch_num :  106\n",
      "test_loss :  3.255212587017778\n",
      "train_loss :  2.914200314332277\n",
      "epoch_num :  111\n",
      "test_loss :  3.1786251669859893\n",
      "train_loss :  2.850784667881081\n",
      "epoch_num :  116\n",
      "test_loss :  3.102811983301062\n",
      "train_loss :  2.7848465588896447\n",
      "epoch_num :  121\n",
      "test_loss :  3.0223227641762547\n",
      "train_loss :  2.709349241633143\n",
      "epoch_num :  126\n",
      "test_loss :  2.9310274783068233\n",
      "train_loss :  2.630085118112374\n",
      "epoch_num :  131\n",
      "test_loss :  2.838486815275416\n",
      "train_loss :  2.566665294151965\n",
      "epoch_num :  136\n",
      "test_loss :  2.7542549515040777\n",
      "train_loss :  2.5141040375745023\n",
      "epoch_num :  141\n",
      "test_loss :  2.679157778971332\n",
      "train_loss :  2.4665903886720466\n",
      "epoch_num :  146\n",
      "test_loss :  2.6120831189292497\n",
      "train_loss :  2.42243585255395\n",
      "epoch_num :  151\n",
      "test_loss :  2.5522367422673296\n",
      "train_loss :  2.381325948707493\n",
      "epoch_num :  156\n",
      "test_loss :  2.498760911499101\n",
      "train_loss :  2.343173956845934\n",
      "epoch_num :  161\n",
      "test_loss :  2.45062497622506\n",
      "train_loss :  2.3077854515530656\n",
      "epoch_num :  166\n",
      "test_loss :  2.4068557752766084\n",
      "train_loss :  2.274904361052198\n",
      "epoch_num :  171\n",
      "test_loss :  2.3666889827621853\n",
      "train_loss :  2.2442963609634616\n",
      "epoch_num :  176\n",
      "test_loss :  2.3295804229622665\n",
      "train_loss :  2.215773660548097\n",
      "epoch_num :  181\n",
      "test_loss :  2.295151845306495\n",
      "train_loss :  2.1891850393866608\n",
      "epoch_num :  186\n",
      "test_loss :  2.2631281484381796\n",
      "train_loss :  2.1643990791545\n",
      "epoch_num :  191\n",
      "test_loss :  2.2332892105238957\n",
      "train_loss :  2.141291433896325\n",
      "epoch_num :  196\n",
      "test_loss :  2.205439880828209\n",
      "train_loss :  2.119737465483585\n",
      "epoch_num :  201\n",
      "test_loss :  2.179394135262634\n",
      "train_loss :  2.0996088255662264\n",
      "epoch_num :  206\n",
      "test_loss :  2.1549681258303184\n",
      "train_loss :  2.0807723409532897\n",
      "epoch_num :  211\n",
      "test_loss :  2.1319778888318672\n",
      "train_loss :  2.0630898491091583\n",
      "epoch_num :  216\n",
      "test_loss :  2.1102389167641067\n",
      "train_loss :  2.0464179576422943\n",
      "epoch_num :  221\n",
      "test_loss :  2.0895660170526402\n",
      "train_loss :  2.0306070273348977\n",
      "epoch_num :  226\n",
      "test_loss :  2.069772810959882\n",
      "train_loss :  2.015499029208318\n",
      "epoch_num :  231\n",
      "test_loss :  2.05067100366755\n",
      "train_loss :  2.0009244042138823\n",
      "epoch_num :  236\n",
      "test_loss :  2.032070384163276\n",
      "train_loss :  1.9866989132184245\n",
      "epoch_num :  241\n",
      "test_loss :  2.013781552713582\n",
      "train_loss :  1.9726231034356922\n",
      "epoch_num :  246\n",
      "test_loss :  1.9956244616902612\n",
      "train_loss :  1.9584895946583154\n",
      "epoch_num :  251\n",
      "test_loss :  1.977445895876735\n",
      "train_loss :  1.9441054511931115\n",
      "epoch_num :  256\n",
      "test_loss :  1.9591455374223428\n",
      "train_loss :  1.9293332760969173\n",
      "epoch_num :  261\n",
      "test_loss :  1.9407014437811374\n",
      "train_loss :  1.9141388588889787\n",
      "epoch_num :  266\n",
      "test_loss :  1.922177017390147\n",
      "train_loss :  1.8986126704645092\n",
      "epoch_num :  271\n",
      "test_loss :  1.9036972512373593\n",
      "train_loss :  1.8829379976665956\n",
      "epoch_num :  276\n",
      "test_loss :  1.8854052444292597\n",
      "train_loss :  1.8673215327494064\n",
      "epoch_num :  281\n",
      "test_loss :  1.8674254330480806\n",
      "train_loss :  1.851933648653768\n",
      "epoch_num :  286\n",
      "test_loss :  1.84984872199987\n",
      "train_loss :  1.8368863234771395\n",
      "epoch_num :  291\n",
      "test_loss :  1.8327349420174255\n",
      "train_loss :  1.822240835767713\n",
      "epoch_num :  296\n",
      "test_loss :  1.8161213923759707\n",
      "train_loss :  1.8080252865461148\n",
      "epoch_num :  301\n",
      "test_loss :  1.8000304968260346\n",
      "train_loss :  1.7942496039431708\n",
      "epoch_num :  306\n",
      "test_loss :  1.7844747898445235\n",
      "train_loss :  1.7809148943330284\n",
      "epoch_num :  311\n",
      "test_loss :  1.7694598512143989\n",
      "train_loss :  1.7680182645055273\n",
      "epoch_num :  316\n",
      "test_loss :  1.7549862204577098\n",
      "train_loss :  1.7555550148188288\n",
      "epoch_num :  321\n",
      "test_loss :  1.7410510261367564\n",
      "train_loss :  1.7435196497765963\n",
      "epoch_num :  326\n",
      "test_loss :  1.7276496755415593\n",
      "train_loss :  1.7319065463239054\n",
      "epoch_num :  331\n",
      "test_loss :  1.714777625060839\n",
      "train_loss :  1.720710644140196\n",
      "epoch_num :  336\n",
      "test_loss :  1.7024319841700246\n",
      "train_loss :  1.7099281776507613\n",
      "epoch_num :  341\n",
      "test_loss :  1.690612490414638\n",
      "train_loss :  1.699557221811137\n",
      "epoch_num :  346\n",
      "test_loss :  1.679321298471265\n",
      "train_loss :  1.6895976919108975\n",
      "epoch_num :  351\n",
      "test_loss :  1.668561194066457\n",
      "train_loss :  1.680050499297566\n",
      "epoch_num :  356\n",
      "test_loss :  1.658332365826061\n",
      "train_loss :  1.6709158738133942\n",
      "epoch_num :  361\n",
      "test_loss :  1.6486285931928164\n",
      "train_loss :  1.662191313218817\n",
      "epoch_num :  366\n",
      "test_loss :  1.639434187375726\n",
      "train_loss :  1.6538699126954306\n",
      "epoch_num :  371\n",
      "test_loss :  1.6307228187974219\n",
      "train_loss :  1.64593969671098\n",
      "epoch_num :  376\n",
      "test_loss :  1.6224585195533907\n",
      "train_loss :  1.638384079999805\n",
      "epoch_num :  381\n",
      "test_loss :  1.6145982258423937\n",
      "train_loss :  1.6311830877374462\n",
      "epoch_num :  386\n",
      "test_loss :  1.6070947949097143\n",
      "train_loss :  1.624314777834313\n",
      "epoch_num :  391\n",
      "test_loss :  1.5998995991768048\n",
      "train_loss :  1.6177564364327717\n",
      "epoch_num :  396\n",
      "test_loss :  1.592964262270501\n",
      "train_loss :  1.6114853631603192\n",
      "epoch_num :  401\n",
      "test_loss :  1.5862415139409678\n",
      "train_loss :  1.605479253153407\n",
      "epoch_num :  406\n",
      "test_loss :  1.5796853541757148\n",
      "train_loss :  1.5997162672911216\n",
      "epoch_num :  411\n",
      "test_loss :  1.5732507572812167\n",
      "train_loss :  1.5941748890632323\n",
      "epoch_num :  416\n",
      "test_loss :  1.5668931004531428\n",
      "train_loss :  1.5888336393887472\n",
      "epoch_num :  421\n",
      "test_loss :  1.5605674377288183\n",
      "train_loss :  1.5836706875615862\n",
      "epoch_num :  426\n",
      "test_loss :  1.5542276945485125\n",
      "train_loss :  1.5786633688104437\n",
      "epoch_num :  431\n",
      "test_loss :  1.5478258440240096\n",
      "train_loss :  1.573787599361312\n",
      "epoch_num :  436\n",
      "test_loss :  1.5413111518854385\n",
      "train_loss :  1.5690171691049448\n",
      "epoch_num :  441\n",
      "test_loss :  1.5346296592818724\n",
      "train_loss :  1.564322893797237\n",
      "epoch_num :  446\n",
      "test_loss :  1.5277242456049842\n",
      "train_loss :  1.5596716353808264\n",
      "epoch_num :  451\n",
      "test_loss :  1.5205359381867556\n",
      "train_loss :  1.5550252799284423\n",
      "epoch_num :  456\n",
      "test_loss :  1.5130076962369212\n",
      "train_loss :  1.5503399589260127\n",
      "epoch_num :  461\n",
      "test_loss :  1.5050927435298818\n",
      "train_loss :  1.5455662185063554\n",
      "epoch_num :  466\n",
      "test_loss :  1.4967704450192674\n",
      "train_loss :  1.5406516114213906\n",
      "epoch_num :  471\n",
      "test_loss :  1.4880726078335587\n",
      "train_loss :  1.5355482498400672\n",
      "epoch_num :  476\n",
      "test_loss :  1.4791189002008012\n",
      "train_loss :  1.5302282867234123\n",
      "epoch_num :  481\n",
      "test_loss :  1.4701473532720737\n",
      "train_loss :  1.524707248846073\n",
      "epoch_num :  486\n",
      "test_loss :  1.4615069736548303\n",
      "train_loss :  1.5190652590644174\n",
      "epoch_num :  491\n",
      "test_loss :  1.4535797972541313\n",
      "train_loss :  1.5134449234941572\n",
      "epoch_num :  496\n",
      "test_loss :  1.4466525488804494\n",
      "train_loss :  1.5080131783272168\n",
      "epoch_num :  501\n",
      "test_loss :  1.4408245166091256\n",
      "train_loss :  1.5029080765874296\n",
      "epoch_num :  506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  1.4360157432209932\n",
      "train_loss :  1.498207615995193\n",
      "epoch_num :  511\n",
      "test_loss :  1.4320456077321926\n",
      "train_loss :  1.493931747684775\n",
      "epoch_num :  516\n",
      "test_loss :  1.428712599049066\n",
      "train_loss :  1.490061527008115\n",
      "epoch_num :  521\n",
      "test_loss :  1.425840438567677\n",
      "train_loss :  1.4865586435828817\n",
      "epoch_num :  526\n",
      "test_loss :  1.4232933658775124\n",
      "train_loss :  1.4833788458762804\n",
      "epoch_num :  531\n",
      "test_loss :  1.4209746239416223\n",
      "train_loss :  1.4804793139482502\n",
      "epoch_num :  536\n",
      "test_loss :  1.4188188271863098\n",
      "train_loss :  1.477821904121006\n",
      "epoch_num :  541\n",
      "test_loss :  1.4167835987006607\n",
      "train_loss :  1.4753740532009818\n",
      "epoch_num :  546\n",
      "test_loss :  1.4148424933120867\n",
      "train_loss :  1.4731085422026546\n",
      "epoch_num :  551\n",
      "test_loss :  1.412979635113624\n",
      "train_loss :  1.4710028075717718\n",
      "epoch_num :  556\n",
      "test_loss :  1.4111858957277679\n",
      "train_loss :  1.469038150428024\n",
      "epoch_num :  561\n",
      "test_loss :  1.4094562795288674\n",
      "train_loss :  1.4671990004703197\n",
      "epoch_num :  566\n",
      "test_loss :  1.407788191499415\n",
      "train_loss :  1.465472290133219\n",
      "epoch_num :  571\n",
      "test_loss :  1.4061803240256292\n",
      "train_loss :  1.4638469466917108\n",
      "epoch_num :  576\n",
      "test_loss :  1.4046319648332564\n",
      "train_loss :  1.4623134901766548\n",
      "epoch_num :  581\n",
      "test_loss :  1.4031425843168492\n",
      "train_loss :  1.4608637188879612\n",
      "epoch_num :  586\n",
      "test_loss :  1.4017116037297015\n",
      "train_loss :  1.459490464379742\n",
      "epoch_num :  591\n",
      "test_loss :  1.4003382773328734\n",
      "train_loss :  1.4581874002322035\n",
      "epoch_num :  596\n",
      "test_loss :  1.3990216440165688\n",
      "train_loss :  1.4569488919298403\n",
      "epoch_num :  601\n",
      "test_loss :  1.3977605194255833\n",
      "train_loss :  1.4557698779739223\n",
      "epoch_num :  606\n",
      "test_loss :  1.3965535101599333\n",
      "train_loss :  1.4546457747065478\n",
      "epoch_num :  611\n",
      "test_loss :  1.3953990386527546\n",
      "train_loss :  1.4535723991768876\n",
      "epoch_num :  616\n",
      "test_loss :  1.3942953719295017\n",
      "train_loss :  1.452545905791191\n",
      "epoch_num :  621\n",
      "test_loss :  1.3932406504003108\n",
      "train_loss :  1.4515627335375225\n",
      "epoch_num :  626\n",
      "test_loss :  1.3922329146744477\n",
      "train_loss :  1.450619561343476\n",
      "epoch_num :  631\n",
      "test_loss :  1.391270129489896\n",
      "train_loss :  1.4497132696789372\n",
      "epoch_num :  636\n",
      "test_loss :  1.3903502044811167\n",
      "train_loss :  1.448840906910581\n",
      "epoch_num :  641\n",
      "test_loss :  1.3894710118378995\n",
      "train_loss :  1.44799965919191\n",
      "epoch_num :  646\n",
      "test_loss :  1.3886304010559043\n",
      "train_loss :  1.4471868228628326\n",
      "epoch_num :  651\n",
      "test_loss :  1.3878262110202988\n",
      "train_loss :  1.4463997784579414\n",
      "epoch_num :  656\n",
      "test_loss :  1.3870562796477446\n",
      "train_loss :  1.4456359654978483\n",
      "epoch_num :  661\n",
      "test_loss :  1.386318451268671\n",
      "train_loss :  1.4448928572730546\n",
      "epoch_num :  666\n",
      "test_loss :  1.3856105818796873\n",
      "train_loss :  1.4441679348308838\n",
      "epoch_num :  671\n",
      "test_loss :  1.384930542344872\n",
      "train_loss :  1.443458659346082\n",
      "epoch_num :  676\n",
      "test_loss :  1.3842762195799787\n",
      "train_loss :  1.442762441996037\n",
      "epoch_num :  681\n",
      "test_loss :  1.3836455157172183\n",
      "train_loss :  1.4420766103722589\n",
      "epoch_num :  686\n",
      "test_loss :  1.3830363452221126\n",
      "train_loss :  1.4413983703408701\n",
      "epoch_num :  691\n",
      "test_loss :  1.382446629919114\n",
      "train_loss :  1.4407247621176542\n",
      "epoch_num :  696\n",
      "test_loss :  1.3818742918827216\n",
      "train_loss :  1.440052609152665\n",
      "epoch_num :  701\n",
      "test_loss :  1.3813172441713018\n",
      "train_loss :  1.439378458237184\n",
      "epoch_num :  706\n",
      "test_loss :  1.3807733794333936\n",
      "train_loss :  1.438698509076913\n",
      "epoch_num :  711\n",
      "test_loss :  1.3802405565187827\n",
      "train_loss :  1.4380085314681448\n",
      "epoch_num :  716\n",
      "test_loss :  1.3797165854129505\n",
      "train_loss :  1.4373037682585246\n",
      "epoch_num :  721\n",
      "test_loss :  1.3791992111417926\n",
      "train_loss :  1.4365788226367018\n",
      "epoch_num :  726\n",
      "test_loss :  1.3786860978731046\n",
      "train_loss :  1.4358275292770748\n",
      "epoch_num :  731\n",
      "test_loss :  1.3781748154846576\n",
      "train_loss :  1.4350428110168827\n",
      "epoch_num :  736\n",
      "test_loss :  1.377662832799477\n",
      "train_loss :  1.434216527068598\n",
      "epoch_num :  741\n",
      "test_loss :  1.3771475253550702\n",
      "train_loss :  1.4333393271014814\n",
      "epoch_num :  746\n",
      "test_loss :  1.3766262126244313\n",
      "train_loss :  1.4324005411248113\n",
      "epoch_num :  751\n",
      "test_loss :  1.3760962530227223\n",
      "train_loss :  1.4313881634712085\n",
      "epoch_num :  756\n",
      "test_loss :  1.3755552494508763\n",
      "train_loss :  1.4302890384939464\n",
      "epoch_num :  761\n",
      "test_loss :  1.3750014584766945\n",
      "train_loss :  1.4290894346521612\n",
      "epoch_num :  766\n",
      "test_loss :  1.3744345507212647\n",
      "train_loss :  1.4277763015364635\n",
      "epoch_num :  771\n",
      "test_loss :  1.3738569099321098\n",
      "train_loss :  1.426339601143187\n",
      "epoch_num :  776\n",
      "test_loss :  1.3732755902096736\n",
      "train_loss :  1.4247760554358968\n",
      "epoch_num :  781\n",
      "test_loss :  1.3727046886005976\n",
      "train_loss :  1.4230941791680054\n",
      "epoch_num :  786\n",
      "test_loss :  1.3721670550154907\n",
      "train_loss :  1.421319269003677\n",
      "epoch_num :  791\n",
      "test_loss :  1.3716932042386545\n",
      "train_loss :  1.419495329543355\n",
      "epoch_num :  796\n",
      "test_loss :  1.371315268380578\n",
      "train_loss :  1.4176803093106474\n",
      "epoch_num :  801\n",
      "test_loss :  1.371056504050524\n",
      "train_loss :  1.4159338273167335\n",
      "epoch_num :  806\n",
      "test_loss :  1.3709214355270105\n",
      "train_loss :  1.4143023263455172\n",
      "epoch_num :  811\n",
      "test_loss :  1.3708932835209666\n",
      "train_loss :  1.4128097488060167\n",
      "epoch_num :  816\n",
      "test_loss :  1.3709405358275801\n",
      "train_loss :  1.4114579080217642\n",
      "epoch_num :  821\n",
      "test_loss :  1.371027873069743\n",
      "train_loss :  1.4102337286106015\n",
      "epoch_num :  826\n",
      "test_loss :  1.3711250602760114\n",
      "train_loss :  1.4091176515246777\n",
      "epoch_num :  831\n",
      "test_loss :  1.3712108973837935\n",
      "train_loss :  1.408089590725975\n",
      "epoch_num :  836\n",
      "test_loss :  1.3712730701693154\n",
      "train_loss :  1.4071318838935625\n",
      "epoch_num :  841\n",
      "test_loss :  1.3713060529800583\n",
      "train_loss :  1.4062301709036868\n",
      "epoch_num :  846\n",
      "test_loss :  1.3713087201592569\n",
      "train_loss :  1.4053732382687993\n",
      "epoch_num :  851\n",
      "test_loss :  1.3712824274840267\n",
      "train_loss :  1.4045524947444914\n",
      "epoch_num :  856\n",
      "test_loss :  1.371229717271824\n",
      "train_loss :  1.4037614004698586\n",
      "epoch_num :  861\n",
      "test_loss :  1.3711535404980106\n",
      "train_loss :  1.402994971173545\n",
      "epoch_num :  866\n",
      "test_loss :  1.371056833908428\n",
      "train_loss :  1.402249385335277\n",
      "epoch_num :  871\n",
      "test_loss :  1.3709423157086276\n",
      "train_loss :  1.40152168618422\n",
      "epoch_num :  876\n",
      "test_loss :  1.3708124057452138\n",
      "train_loss :  1.4008095603338564\n",
      "epoch_num :  881\n",
      "test_loss :  1.3706692117740842\n",
      "train_loss :  1.400111174830965\n",
      "epoch_num :  886\n",
      "test_loss :  1.3705145479307632\n",
      "train_loss :  1.3994250574938727\n",
      "epoch_num :  891\n",
      "test_loss :  1.3703499667415067\n",
      "train_loss :  1.398750008921622\n",
      "epoch_num :  896\n",
      "test_loss :  1.370176794904358\n",
      "train_loss :  1.398085037573234\n",
      "epoch_num :  901\n",
      "test_loss :  1.3699961680445158\n",
      "train_loss :  1.3974293116618814\n",
      "epoch_num :  906\n",
      "test_loss :  1.36980906233573\n",
      "train_loss :  1.396782123347161\n",
      "epoch_num :  911\n",
      "test_loss :  1.3696163222769593\n",
      "train_loss :  1.39614286196698\n",
      "epoch_num :  916\n",
      "test_loss :  1.3694186846040832\n",
      "train_loss :  1.395510993951691\n",
      "epoch_num :  921\n",
      "test_loss :  1.3692167986283912\n",
      "train_loss :  1.3948860477063556\n",
      "epoch_num :  926\n",
      "test_loss :  1.369011243408638\n",
      "train_loss :  1.3942676022066356\n",
      "epoch_num :  931\n",
      "test_loss :  1.368802542180412\n",
      "train_loss :  1.3936552783837093\n",
      "epoch_num :  936\n",
      "test_loss :  1.3685911744376114\n",
      "train_loss :  1.393048732611494\n",
      "epoch_num :  941\n",
      "test_loss :  1.3683775860139231\n",
      "train_loss :  1.3924476517822495\n",
      "epoch_num :  946\n",
      "test_loss :  1.368162197460855\n",
      "train_loss :  1.3918517495828049\n",
      "epoch_num :  951\n",
      "test_loss :  1.3679454109699636\n",
      "train_loss :  1.39126076367638\n",
      "epoch_num :  956\n",
      "test_loss :  1.3677276160431424\n",
      "train_loss :  1.390674453563661\n",
      "epoch_num :  961\n",
      "test_loss :  1.3675091940771797\n",
      "train_loss :  1.3900925989478095\n",
      "epoch_num :  966\n",
      "test_loss :  1.3672905219972626\n",
      "train_loss :  1.3895149984664843\n",
      "epoch_num :  971\n",
      "test_loss :  1.3670719750483058\n",
      "train_loss :  1.3889414686829167\n",
      "epoch_num :  976\n",
      "test_loss :  1.366853928832326\n",
      "train_loss :  1.3883718432503858\n",
      "epoch_num :  981\n",
      "test_loss :  1.3666367606640566\n",
      "train_loss :  1.3878059721818135\n",
      "epoch_num :  986\n",
      "test_loss :  1.3664208503051183\n",
      "train_loss :  1.3872437211701045\n",
      "epoch_num :  991\n",
      "test_loss :  1.3662065801288366\n",
      "train_loss :  1.3866849709162896\n",
      "epoch_num :  996\n"
     ]
    }
   ],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = [0]\n",
    "losses = [0]\n",
    "test_losses = [0]\n",
    "for epoch_num in range(num_epochs):\n",
    "    if epoch_num%5==1:\n",
    "#         print(\"test_accuracies : \", accuracies_test[-1])\n",
    "        print(\"test_loss : \", test_losses[-1])\n",
    "        print(\"train_loss : \", losses[-1])\n",
    "        print(\"epoch_num : \", epoch_num)\n",
    "    for sample_num in range(x_train.shape[1]):\n",
    "#         if(sample_num%100==0):\n",
    "#         print(\"sample_num : \", sample_num)\n",
    "        #Forward propagation\n",
    "        out1_temp = np.matmul(weights1.T, x_train[:,sample_num])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        #Back-propogation\n",
    "        error_out = out2-y_train[:, sample_num]\n",
    "        err_2 = np.matrix(error_out)\n",
    "        grad_2 = np.matmul(out1, err_2.T)\n",
    "        err_1 = np.multiply(np.matmul(weights2, err_2), np.multiply(out1, np.ones(out1.shape)-out1))\n",
    "        grad_1 = np.multiply(x_train[:, sample_num], err_1.T)\n",
    "        #Updating the weights\n",
    "#         print(weights1)\n",
    "        biases1 = biases1 - learning_rate*err_1\n",
    "        biases2 = biases2 - learning_rate*err_2\n",
    "        weights1 = weights1-learning_rate*grad_1\n",
    "        weights2 = weights2-learning_rate*grad_2\n",
    "#         print(cross_entropy(out2, y_train[:, sample_num]))\n",
    "        #Updating the accuracy and loss\n",
    "        #End of for loop\n",
    "#     print(\"accuracy : \", np.mean(acc_arr))\n",
    "    test_losses.append(test_loss(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_test.append(acc_out(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_train.append(acc_out(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    losses.append(test_loss(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot(211)\n",
    "plt.plot(losses[1:], label = 'train_loss')\n",
    "plt.plot(test_losses[1:], label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.multiply(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test, pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26.66213416 12.42227846 10.48995195 27.81825115 29.56030529]\n",
      " [29.24353716 15.13314274 13.81414763 29.2364303  30.91331475]]\n",
      "[[26.48 12.16 10.35 29.71 29.47]\n",
      " [30.91 15.18 13.65 28.02 29.77]]\n"
     ]
    }
   ],
   "source": [
    "ind = 101\n",
    "print(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)[:,5:10])\n",
    "print(y_test[:,5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_losses)\n",
    "# print(losses)\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(weights1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
