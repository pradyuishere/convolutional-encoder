{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def test_loss(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out2 = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "        loss = []\n",
    "        for iter in range(out2.shape[1]):\n",
    "            loss.append(MSE(out2[:, iter], y_in[:, iter]))\n",
    "        return np.mean(loss)\n",
    "\n",
    "def pred_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out1_temp = np.matmul(weights1.T, x_in[:,:])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        return out2\n",
    "\n",
    "def acc_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "    pred = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "    sum_acc = 0\n",
    "    for iter in range(pred.shape[1]):\n",
    "#         print(iter)\n",
    "        sum_acc+=(np.argmax(pred[:, iter])==np.argmax(y_in[:, iter]))\n",
    "    return sum_acc/y_in.shape[1]\n",
    "\n",
    "def MSE(yHat, y):\n",
    "    return np.sum(np.multiply(yHat - y, yHat - y)) / y.size\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-5):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.mean(np.multiply(np.log(predictions),targets)) \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 10)\n"
     ]
    }
   ],
   "source": [
    "##Load and segregate the data\n",
    "letters = genfromtxt('energy_eff2.csv', delimiter=',')\n",
    "# np.random.shuffle(letters)\n",
    "print(letters.shape)\n",
    "x_in = letters[:, 0:8]\n",
    "x_mean = np.mean(x_in, axis = 0)\n",
    "x_std = np.std(x_in, axis = 0)\n",
    "y_in = letters[:, 8:10]\n",
    "y_mean = np.mean(y_in, axis=0)\n",
    "y_std = np.std(y_in, axis = 0)\n",
    "# y_in = (y_in-y_mean)/y_std\n",
    "x_in = (x_in-x_mean)/x_std\n",
    "# print(np.std(y_in, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_in[0:int(x_in.shape[0]*0.7), :]\n",
    "y_train = y_in[0:int(x_in.shape[0]*0.7), :]\n",
    "\n",
    "x_test = x_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]\n",
    "y_test = y_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1000\n",
    "alpha = 1\n",
    "epsilon = 1e-8\n",
    "\n",
    "weights1 = np.random.normal(size=(x_in.shape[1], hidden_size))\n",
    "weights2 = np.random.normal(size=(hidden_size, y_in.shape[1]))\n",
    "print(weights1.shape)\n",
    "print(weights2.shape)\n",
    "biases1 = np.random.normal(size=(hidden_size, 1))\n",
    "biases2 = np.random.normal(size=(y_in.shape[1], 1))\n",
    "\n",
    "x_train = np.matrix(x_train)\n",
    "x_train = x_train.T\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = np.matrix(x_test)\n",
    "x_test = x_test.T\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  18.152172480992615\n",
      "train_loss :  22.888300154496534\n",
      "epoch_num :  1\n",
      "test_loss :  9.4252571615526\n",
      "train_loss :  10.249366813292093\n",
      "epoch_num :  6\n",
      "test_loss :  8.92589768827612\n",
      "train_loss :  9.383721599092155\n",
      "epoch_num :  11\n",
      "test_loss :  8.576157001236986\n",
      "train_loss :  8.95503362885938\n",
      "epoch_num :  16\n",
      "test_loss :  8.160755785553118\n",
      "train_loss :  8.513042391327698\n",
      "epoch_num :  21\n",
      "test_loss :  7.862763889614786\n",
      "train_loss :  8.15509597081734\n",
      "epoch_num :  26\n",
      "test_loss :  7.619606272559105\n",
      "train_loss :  7.838361299468985\n",
      "epoch_num :  31\n",
      "test_loss :  7.407835294031027\n",
      "train_loss :  7.55988932366962\n",
      "epoch_num :  36\n",
      "test_loss :  7.2101842237343625\n",
      "train_loss :  7.309959322612241\n",
      "epoch_num :  41\n",
      "test_loss :  7.028452697798669\n",
      "train_loss :  7.082035363459208\n",
      "epoch_num :  46\n",
      "test_loss :  6.86045997814257\n",
      "train_loss :  6.874697945107208\n",
      "epoch_num :  51\n",
      "test_loss :  6.701130077929581\n",
      "train_loss :  6.683740934158644\n",
      "epoch_num :  56\n",
      "test_loss :  6.548744120719523\n",
      "train_loss :  6.504077143190574\n",
      "epoch_num :  61\n",
      "test_loss :  6.402479982011446\n",
      "train_loss :  6.332163517909524\n",
      "epoch_num :  66\n",
      "test_loss :  6.259092044561046\n",
      "train_loss :  6.164445209960783\n",
      "epoch_num :  71\n",
      "test_loss :  6.112855981313085\n",
      "train_loss :  5.996928756403799\n",
      "epoch_num :  76\n",
      "test_loss :  5.961600936302881\n",
      "train_loss :  5.828722128248803\n",
      "epoch_num :  81\n",
      "test_loss :  5.81123089616449\n",
      "train_loss :  5.665404750433945\n",
      "epoch_num :  86\n",
      "test_loss :  5.667393175596561\n",
      "train_loss :  5.513179921052112\n",
      "epoch_num :  91\n",
      "test_loss :  5.532805139770441\n",
      "train_loss :  5.374604636076789\n",
      "epoch_num :  96\n",
      "test_loss :  5.409491481903622\n",
      "train_loss :  5.249797124355878\n",
      "epoch_num :  101\n",
      "test_loss :  5.298147864859026\n",
      "train_loss :  5.137494875327475\n",
      "epoch_num :  106\n",
      "test_loss :  5.19815817101574\n",
      "train_loss :  5.035917252232938\n",
      "epoch_num :  111\n",
      "test_loss :  5.108270492470933\n",
      "train_loss :  4.9433321022112695\n",
      "epoch_num :  116\n",
      "test_loss :  5.027109779968489\n",
      "train_loss :  4.858267654185795\n",
      "epoch_num :  121\n",
      "test_loss :  4.953411514747476\n",
      "train_loss :  4.779538235030066\n",
      "epoch_num :  126\n",
      "test_loss :  4.886090707044406\n",
      "train_loss :  4.706207193170513\n",
      "epoch_num :  131\n",
      "test_loss :  4.824241526846104\n",
      "train_loss :  4.637538088940297\n",
      "epoch_num :  136\n",
      "test_loss :  4.767114725056104\n",
      "train_loss :  4.572950362206135\n",
      "epoch_num :  141\n",
      "test_loss :  4.714091175003718\n",
      "train_loss :  4.511983063478045\n",
      "epoch_num :  146\n",
      "test_loss :  4.664657445328577\n",
      "train_loss :  4.454266300860194\n",
      "epoch_num :  151\n",
      "test_loss :  4.618384918836205\n",
      "train_loss :  4.399499126287394\n",
      "epoch_num :  156\n",
      "test_loss :  4.574912602173974\n",
      "train_loss :  4.347432575124114\n",
      "epoch_num :  161\n",
      "test_loss :  4.533933354409482\n",
      "train_loss :  4.2978567783922745\n",
      "epoch_num :  166\n",
      "test_loss :  4.495183119136933\n",
      "train_loss :  4.250591280610773\n",
      "epoch_num :  171\n",
      "test_loss :  4.458432689696707\n",
      "train_loss :  4.2054778713963055\n",
      "epoch_num :  176\n",
      "test_loss :  4.423481526048835\n",
      "train_loss :  4.162375372769683\n",
      "epoch_num :  181\n",
      "test_loss :  4.390153163152519\n",
      "train_loss :  4.121155925577999\n",
      "epoch_num :  186\n",
      "test_loss :  4.358291797925497\n",
      "train_loss :  4.081702398843169\n",
      "epoch_num :  191\n",
      "test_loss :  4.327759707165884\n",
      "train_loss :  4.043906614189722\n",
      "epoch_num :  196\n",
      "test_loss :  4.298435222443168\n",
      "train_loss :  4.007668138651025\n",
      "epoch_num :  201\n",
      "test_loss :  4.2702110601254395\n",
      "train_loss :  3.9728934543214933\n",
      "epoch_num :  206\n",
      "test_loss :  4.2429928680489075\n",
      "train_loss :  3.939495361914425\n",
      "epoch_num :  211\n",
      "test_loss :  4.216697901210831\n",
      "train_loss :  3.9073925165928296\n",
      "epoch_num :  216\n",
      "test_loss :  4.191253777066247\n",
      "train_loss :  3.8765090284451764\n",
      "epoch_num :  221\n",
      "test_loss :  4.166597288414033\n",
      "train_loss :  3.846774087279969\n",
      "epoch_num :  226\n",
      "test_loss :  4.142673270944775\n",
      "train_loss :  3.818121592760533\n",
      "epoch_num :  231\n",
      "test_loss :  4.1194335353069675\n",
      "train_loss :  3.7904897867804985\n",
      "epoch_num :  236\n",
      "test_loss :  4.096835881172764\n",
      "train_loss :  3.763820895535353\n",
      "epoch_num :  241\n",
      "test_loss :  4.074843213672699\n",
      "train_loss :  3.738060794056787\n",
      "epoch_num :  246\n",
      "test_loss :  4.053422780886798\n",
      "train_loss :  3.7131587064981098\n",
      "epoch_num :  251\n",
      "test_loss :  4.032545545223886\n",
      "train_loss :  3.6890669522966966\n",
      "epoch_num :  256\n",
      "test_loss :  4.012185692372926\n",
      "train_loss :  3.6657407431255313\n",
      "epoch_num :  261\n",
      "test_loss :  3.992320270344367\n",
      "train_loss :  3.643138029943541\n",
      "epoch_num :  266\n",
      "test_loss :  3.972928939300395\n",
      "train_loss :  3.6212193945858537\n",
      "epoch_num :  271\n",
      "test_loss :  3.9539938015916185\n",
      "train_loss :  3.5999479764994367\n",
      "epoch_num :  276\n",
      "test_loss :  3.9354992717684496\n",
      "train_loss :  3.5792894220967906\n",
      "epoch_num :  281\n",
      "test_loss :  3.9174319397375603\n",
      "train_loss :  3.5592118413773766\n",
      "epoch_num :  286\n",
      "test_loss :  3.8997803789486944\n",
      "train_loss :  3.5396857541472353\n",
      "epoch_num :  291\n",
      "test_loss :  3.8825348586153923\n",
      "train_loss :  3.5206840074844727\n",
      "epoch_num :  296\n",
      "test_loss :  3.8656869372678004\n",
      "train_loss :  3.502181648841888\n",
      "epoch_num :  301\n",
      "test_loss :  3.8492289444805405\n",
      "train_loss :  3.484155746758591\n",
      "epoch_num :  306\n",
      "test_loss :  3.833153393256729\n",
      "train_loss :  3.466585163177659\n",
      "epoch_num :  311\n",
      "test_loss :  3.817452396466118\n",
      "train_loss :  3.4494502947946444\n",
      "epoch_num :  316\n",
      "test_loss :  3.8021171740813453\n",
      "train_loss :  3.4327328107676673\n",
      "epoch_num :  321\n",
      "test_loss :  3.787137725738338\n",
      "train_loss :  3.416415416175164\n",
      "epoch_num :  326\n",
      "test_loss :  3.7725027080022535\n",
      "train_loss :  3.400481663760632\n",
      "epoch_num :  331\n",
      "test_loss :  3.758199510698061\n",
      "train_loss :  3.38491582379801\n",
      "epoch_num :  336\n",
      "test_loss :  3.7442144883504827\n",
      "train_loss :  3.369702808666575\n",
      "epoch_num :  341\n",
      "test_loss :  3.7305332826429587\n",
      "train_loss :  3.354828139515141\n",
      "epoch_num :  346\n",
      "test_loss :  3.7171411717785596\n",
      "train_loss :  3.340277939028385\n",
      "epoch_num :  351\n",
      "test_loss :  3.7040233968016745\n",
      "train_loss :  3.326038935840995\n",
      "epoch_num :  356\n",
      "test_loss :  3.691165434647182\n",
      "train_loss :  3.312098470279164\n",
      "epoch_num :  361\n",
      "test_loss :  3.678553205897384\n",
      "train_loss :  3.298444495656148\n",
      "epoch_num :  366\n",
      "test_loss :  3.6661732183666116\n",
      "train_loss :  3.285065572988619\n",
      "epoch_num :  371\n",
      "test_loss :  3.654012655127338\n",
      "train_loss :  3.271950859309862\n",
      "epoch_num :  376\n",
      "test_loss :  3.642059418547589\n",
      "train_loss :  3.2590900908811737\n",
      "epoch_num :  381\n",
      "test_loss :  3.630302141927003\n",
      "train_loss :  3.2464735629105754\n",
      "epoch_num :  386\n",
      "test_loss :  3.6187301788037405\n",
      "train_loss :  3.234092107240565\n",
      "epoch_num :  391\n",
      "test_loss :  3.60733357793729\n",
      "train_loss :  3.2219370691291607\n",
      "epoch_num :  396\n",
      "test_loss :  3.5961030499206856\n",
      "train_loss :  3.2100002838801913\n",
      "epoch_num :  401\n",
      "test_loss :  3.5850299296105534\n",
      "train_loss :  3.1982740537565437\n",
      "epoch_num :  406\n",
      "test_loss :  3.5741061371675737\n",
      "train_loss :  3.1867511253611385\n",
      "epoch_num :  411\n",
      "test_loss :  3.563324139460094\n",
      "train_loss :  3.175424667494841\n",
      "epoch_num :  416\n",
      "test_loss :  3.5526769128444737\n",
      "train_loss :  3.1642882493885414\n",
      "epoch_num :  421\n",
      "test_loss :  3.5421579078317835\n",
      "train_loss :  3.1533358191452807\n",
      "epoch_num :  426\n",
      "test_loss :  3.5317610158209454\n",
      "train_loss :  3.1425616822065336\n",
      "epoch_num :  431\n",
      "test_loss :  3.521480537874841\n",
      "train_loss :  3.1319604796655436\n",
      "epoch_num :  436\n",
      "test_loss :  3.5113111553998233\n",
      "train_loss :  3.1215271662830117\n",
      "epoch_num :  441\n",
      "test_loss :  3.501247902533523\n",
      "train_loss :  3.1112569881108385\n",
      "epoch_num :  446\n",
      "test_loss :  3.4912861400313515\n",
      "train_loss :  3.1011454596930137\n",
      "epoch_num :  451\n",
      "test_loss :  3.481421530454495\n",
      "train_loss :  3.091188340884388\n",
      "epoch_num :  456\n",
      "test_loss :  3.471650014492642\n",
      "train_loss :  3.081381613402472\n",
      "epoch_num :  461\n",
      "test_loss :  3.4619677882948072\n",
      "train_loss :  3.071721457299565\n",
      "epoch_num :  466\n",
      "test_loss :  3.452371281726848\n",
      "train_loss :  3.0622042276068013\n",
      "epoch_num :  471\n",
      "test_loss :  3.442857137519225\n",
      "train_loss :  3.0528264314528664\n",
      "epoch_num :  476\n",
      "test_loss :  3.4334221913106973\n",
      "train_loss :  3.043584705994358\n",
      "epoch_num :  481\n",
      "test_loss :  3.4240634526290323\n",
      "train_loss :  3.034475797508358\n",
      "epoch_num :  486\n",
      "test_loss :  3.4147780868770687\n",
      "train_loss :  3.025496541989849\n",
      "epoch_num :  491\n",
      "test_loss :  3.4055633984106697\n",
      "train_loss :  3.0166438475671495\n",
      "epoch_num :  496\n",
      "test_loss :  3.3964168148034326\n",
      "train_loss :  3.0079146790001037\n",
      "epoch_num :  501\n",
      "test_loss :  3.3873358723928604\n",
      "train_loss :  2.9993060444619855\n",
      "epoch_num :  506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  3.3783182031948957\n",
      "train_loss :  2.990814984732217\n",
      "epoch_num :  511\n",
      "test_loss :  3.3693615232603857\n",
      "train_loss :  2.982438564848794\n",
      "epoch_num :  516\n",
      "test_loss :  3.360463622530903\n",
      "train_loss :  2.974173868192671\n",
      "epoch_num :  521\n",
      "test_loss :  3.3516223562330465\n",
      "train_loss :  2.966017992906437\n",
      "epoch_num :  526\n",
      "test_loss :  3.342835637833954\n",
      "train_loss :  2.957968050491133\n",
      "epoch_num :  531\n",
      "test_loss :  3.3341014335655195\n",
      "train_loss :  2.9500211663805853\n",
      "epoch_num :  536\n",
      "test_loss :  3.325417758513136\n",
      "train_loss :  2.9421744822637947\n",
      "epoch_num :  541\n",
      "test_loss :  3.316782674256281\n",
      "train_loss :  2.9344251599129487\n",
      "epoch_num :  546\n",
      "test_loss :  3.3081942880423965\n",
      "train_loss :  2.926770386275569\n",
      "epoch_num :  551\n",
      "test_loss :  3.299650753472156\n",
      "train_loss :  2.9192073796025078\n",
      "epoch_num :  556\n",
      "test_loss :  3.291150272671153\n",
      "train_loss :  2.9117333964052263\n",
      "epoch_num :  561\n",
      "test_loss :  3.2826910999195253\n",
      "train_loss :  2.904345739062812\n",
      "epoch_num :  566\n",
      "test_loss :  3.274271546704994\n",
      "train_loss :  2.897041763928056\n",
      "epoch_num :  571\n",
      "test_loss :  3.265889988154493\n",
      "train_loss :  2.8898188898089296\n",
      "epoch_num :  576\n",
      "test_loss :  3.2575448707834704\n",
      "train_loss :  2.8826746067243447\n",
      "epoch_num :  581\n",
      "test_loss :  3.24923472147847\n",
      "train_loss :  2.8756064848483907\n",
      "epoch_num :  586\n",
      "test_loss :  3.240958157596623\n",
      "train_loss :  2.8686121835636995\n",
      "epoch_num :  591\n",
      "test_loss :  3.232713898024189\n",
      "train_loss :  2.8616894605409353\n",
      "epoch_num :  596\n",
      "test_loss :  3.2245007749858963\n",
      "train_loss :  2.854836180747382\n",
      "epoch_num :  601\n",
      "test_loss :  3.21631774633718\n",
      "train_loss :  2.8480503252635807\n",
      "epoch_num :  606\n",
      "test_loss :  3.2081639080055115\n",
      "train_loss :  2.8413299997543375\n",
      "epoch_num :  611\n",
      "test_loss :  3.2000385061771017\n",
      "train_loss :  2.8346734424017552\n",
      "epoch_num :  616\n",
      "test_loss :  3.1919409487567774\n",
      "train_loss :  2.82807903106643\n",
      "epoch_num :  621\n",
      "test_loss :  3.1838708155670212\n",
      "train_loss :  2.8215452894031796\n",
      "epoch_num :  626\n",
      "test_loss :  3.17582786670591\n",
      "train_loss :  2.8150708916250324\n",
      "epoch_num :  631\n",
      "test_loss :  3.1678120484602674\n",
      "train_loss :  2.8086546655895064\n",
      "epoch_num :  636\n",
      "test_loss :  3.1598234961794645\n",
      "train_loss :  2.802295593880535\n",
      "epoch_num :  641\n",
      "test_loss :  3.1518625335646737\n",
      "train_loss :  2.795992812583031\n",
      "epoch_num :  646\n",
      "test_loss :  3.1439296679232354\n",
      "train_loss :  2.7897456074990603\n",
      "epoch_num :  651\n",
      "test_loss :  3.136025581080766\n",
      "train_loss :  2.7835534076361284\n",
      "epoch_num :  656\n",
      "test_loss :  3.128151115831973\n",
      "train_loss :  2.7774157759077696\n",
      "epoch_num :  661\n",
      "test_loss :  3.1203072580346043\n",
      "train_loss :  2.7713323971186625\n",
      "epoch_num :  666\n",
      "test_loss :  3.1124951146968636\n",
      "train_loss :  2.7653030634521247\n",
      "epoch_num :  671\n",
      "test_loss :  3.104715888655293\n",
      "train_loss :  2.759327657824446\n",
      "epoch_num :  676\n",
      "test_loss :  3.0969708506659757\n",
      "train_loss :  2.7534061356045987\n",
      "epoch_num :  681\n",
      "test_loss :  3.0892613099131476\n",
      "train_loss :  2.74753850530504\n",
      "epoch_num :  686\n",
      "test_loss :  3.081588584055813\n",
      "train_loss :  2.7417248089178177\n",
      "epoch_num :  691\n",
      "test_loss :  3.0739539699716976\n",
      "train_loss :  2.73596510259224\n",
      "epoch_num :  696\n",
      "test_loss :  3.066358716312243\n",
      "train_loss :  2.730259438322732\n",
      "epoch_num :  701\n",
      "test_loss :  3.0588039988574494\n",
      "train_loss :  2.724607847241683\n",
      "epoch_num :  706\n",
      "test_loss :  3.051290899468704\n",
      "train_loss :  2.719010324999977\n",
      "epoch_num :  711\n",
      "test_loss :  3.0438203892003313\n",
      "train_loss :  2.7134668195798337\n",
      "epoch_num :  716\n",
      "test_loss :  3.0363933158717646\n",
      "train_loss :  2.707977221734628\n",
      "epoch_num :  721\n",
      "test_loss :  3.0290103961463757\n",
      "train_loss :  2.7025413581029296\n",
      "epoch_num :  726\n",
      "test_loss :  3.0216722119309614\n",
      "train_loss :  2.697158986910959\n",
      "epoch_num :  731\n",
      "test_loss :  3.0143792107191887\n",
      "train_loss :  2.691829796068403\n",
      "epoch_num :  736\n",
      "test_loss :  3.007131709362171\n",
      "train_loss :  2.6865534033820753\n",
      "epoch_num :  741\n",
      "test_loss :  2.9999299006632\n",
      "train_loss :  2.681329358561892\n",
      "epoch_num :  746\n",
      "test_loss :  2.9927738621590145\n",
      "train_loss :  2.6761571466719234\n",
      "epoch_num :  751\n",
      "test_loss :  2.985663566459185\n",
      "train_loss :  2.671036192682509\n",
      "epoch_num :  756\n",
      "test_loss :  2.978598892559851\n",
      "train_loss :  2.665965866801182\n",
      "epoch_num :  761\n",
      "test_loss :  2.9715796376160295\n",
      "train_loss :  2.660945490295613\n",
      "epoch_num :  766\n",
      "test_loss :  2.964605528739177\n",
      "train_loss :  2.6559743415646255\n",
      "epoch_num :  771\n",
      "test_loss :  2.9576762344737264\n",
      "train_loss :  2.651051662259166\n",
      "epoch_num :  776\n",
      "test_loss :  2.9507913756917366\n",
      "train_loss :  2.646176663300076\n",
      "epoch_num :  781\n",
      "test_loss :  2.9439505357227356\n",
      "train_loss :  2.641348530680866\n",
      "epoch_num :  786\n",
      "test_loss :  2.937153269604078\n",
      "train_loss :  2.6365664309797823\n",
      "epoch_num :  791\n",
      "test_loss :  2.93039911239348\n",
      "train_loss :  2.6318295165355092\n",
      "epoch_num :  796\n",
      "test_loss :  2.923687586529958\n",
      "train_loss :  2.627136930264546\n",
      "epoch_num :  801\n",
      "test_loss :  2.917018208262863\n",
      "train_loss :  2.622487810115998\n",
      "epoch_num :  806\n",
      "test_loss :  2.9103904931920384\n",
      "train_loss :  2.6178812931719677\n",
      "epoch_num :  811\n",
      "test_loss :  2.9038039609776267\n",
      "train_loss :  2.613316519409346\n",
      "epoch_num :  816\n",
      "test_loss :  2.8972581392858223\n",
      "train_loss :  2.6087926351428012\n",
      "epoch_num :  821\n",
      "test_loss :  2.8907525670406193\n",
      "train_loss :  2.6043087961697338\n",
      "epoch_num :  826\n",
      "test_loss :  2.884286797050676\n",
      "train_loss :  2.5998641706367343\n",
      "epoch_num :  831\n",
      "test_loss :  2.8778603980774675\n",
      "train_loss :  2.595457941644425\n",
      "epoch_num :  836\n",
      "test_loss :  2.8714729564062296\n",
      "train_loss :  2.5910893096040093\n",
      "epoch_num :  841\n",
      "test_loss :  2.8651240769761275\n",
      "train_loss :  2.586757494354974\n",
      "epoch_num :  846\n",
      "test_loss :  2.858813384120431\n",
      "train_loss :  2.582461737049523\n",
      "epoch_num :  851\n",
      "test_loss :  2.852540521962818\n",
      "train_loss :  2.5782013018060113\n",
      "epoch_num :  856\n",
      "test_loss :  2.8463051545115117\n",
      "train_loss :  2.573975477130751\n",
      "epoch_num :  861\n",
      "test_loss :  2.840106965489367\n",
      "train_loss :  2.5697835771058544\n",
      "epoch_num :  866\n",
      "test_loss :  2.8339456579356255\n",
      "train_loss :  2.5656249423397255\n",
      "epoch_num :  871\n",
      "test_loss :  2.8278209536131174\n",
      "train_loss :  2.5614989406770157\n",
      "epoch_num :  876\n",
      "test_loss :  2.821732592253862\n",
      "train_loss :  2.5574049676658563\n",
      "epoch_num :  881\n",
      "test_loss :  2.815680330675286\n",
      "train_loss :  2.5533424467823576\n",
      "epoch_num :  886\n",
      "test_loss :  2.8096639417996805\n",
      "train_loss :  2.5493108294152047\n",
      "epoch_num :  891\n",
      "test_loss :  2.803683213609493\n",
      "train_loss :  2.545309594616715\n",
      "epoch_num :  896\n",
      "test_loss :  2.797737948071302\n",
      "train_loss :  2.541338248630888\n",
      "epoch_num :  901\n",
      "test_loss :  2.7918279600614038\n",
      "train_loss :  2.537396324213245\n",
      "epoch_num :  906\n",
      "test_loss :  2.7859530763257307\n",
      "train_loss :  2.53348337976166\n",
      "epoch_num :  911\n",
      "test_loss :  2.780113134505811\n",
      "train_loss :  2.5295989982815903\n",
      "epoch_num :  916\n",
      "test_loss :  2.7743079822610603\n",
      "train_loss :  2.525742786212846\n",
      "epoch_num :  921\n",
      "test_loss :  2.768537476515468\n",
      "train_loss :  2.521914372148168\n",
      "epoch_num :  926\n",
      "test_loss :  2.762801482853703\n",
      "train_loss :  2.518113405476153\n",
      "epoch_num :  931\n",
      "test_loss :  2.7570998750879863\n",
      "train_loss :  2.514339554982424\n",
      "epoch_num :  936\n",
      "test_loss :  2.75143253501228\n",
      "train_loss :  2.5105925074431696\n",
      "epoch_num :  941\n",
      "test_loss :  2.745799352355571\n",
      "train_loss :  2.5068719662443613\n",
      "epoch_num :  946\n",
      "test_loss :  2.740200224939905\n",
      "train_loss :  2.503177650057982\n",
      "epoch_num :  951\n",
      "test_loss :  2.734635059043133\n",
      "train_loss :  2.499509291603744\n",
      "epoch_num :  956\n",
      "test_loss :  2.7291037699595933\n",
      "train_loss :  2.4958666365207938\n",
      "epoch_num :  961\n",
      "test_loss :  2.723606282746044\n",
      "train_loss :  2.4922494423693715\n",
      "epoch_num :  966\n",
      "test_loss :  2.718142533133504\n",
      "train_loss :  2.48865747777713\n",
      "epoch_num :  971\n",
      "test_loss :  2.7127124685801713\n",
      "train_loss :  2.485090521739306\n",
      "epoch_num :  976\n",
      "test_loss :  2.707316049434831\n",
      "train_loss :  2.4815483630761492\n",
      "epoch_num :  981\n",
      "test_loss :  2.7019532501757237\n",
      "train_loss :  2.4780308000453797\n",
      "epoch_num :  986\n",
      "test_loss :  2.696624060685789\n",
      "train_loss :  2.4745376401019477\n",
      "epoch_num :  991\n",
      "test_loss :  2.69132848752214\n",
      "train_loss :  2.471068699792403\n",
      "epoch_num :  996\n"
     ]
    }
   ],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "biases1_squares = 0\n",
    "biases2_squares = 0\n",
    "weights1_squares = 0\n",
    "weights2_squares = 0\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "    if epoch_num%5==1:\n",
    "        print(\"test_loss : \", test_losses[-1])\n",
    "        print(\"train_loss : \", losses[-1])\n",
    "        print(\"epoch_num : \", epoch_num)\n",
    "    for sample_num in range(x_train.shape[1]):\n",
    "#         print(\"sample_num : \", sample_num)\n",
    "        #Forward propagation\n",
    "        out1_temp = np.matmul(weights1.T, x_train[:,sample_num])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        #Back-propogation\n",
    "        error_out = out2-y_train[:, sample_num]\n",
    "        err_2 = np.matrix(error_out)\n",
    "        grad_2 = np.matmul(out1, err_2.T)\n",
    "        err_1 = np.multiply(np.matmul(weights2, err_2), np.multiply(out1, np.ones(out1.shape)-out1))\n",
    "        grad_1 = np.multiply(x_train[:, sample_num], err_1.T)\n",
    "        #Updating the weights\n",
    "        \n",
    "        biases1_squares = alpha*biases1_squares+np.multiply(err_1, err_1)\n",
    "        biases2_squares = alpha*biases2_squares+np.multiply(err_2, err_2)\n",
    "        weights1_squares = alpha*weights1_squares+np.multiply(grad_1, grad_1)\n",
    "        weights2_squares = alpha*weights2_squares+np.multiply(grad_2, grad_2)\n",
    "        \n",
    "        biases1 = biases1 - np.multiply(learning_rate/(np.sqrt(biases1_squares)+epsilon), err_1)\n",
    "        biases2 = biases2 - np.multiply(learning_rate/(np.sqrt(biases2_squares)+epsilon), err_2)\n",
    "        weights1 = weights1 - np.multiply(learning_rate/(np.sqrt(weights1_squares)+epsilon), grad_1)\n",
    "        weights2 = weights2 - np.multiply(learning_rate/(np.sqrt(weights2_squares)+epsilon), grad_2)\n",
    "        #Updating the accuracy and loss\n",
    "        #End of for loop\n",
    "#     print(\"accuracy : \", np.mean(acc_arr))\n",
    "    test_losses.append(test_loss(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_test.append(acc_out(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_train.append(acc_out(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    losses.append(test_loss(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot(211)\n",
    "plt.plot(losses[1:], label = 'train_loss')\n",
    "plt.plot(test_losses[1:], label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.multiply(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test, pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24.32666128 12.79110975 10.10686117 27.07831844 31.31350649]\n",
      " [27.61061578 15.57771396 13.65199944 29.45614437 32.49936418]]\n",
      "[[26.48 12.16 10.35 29.71 29.47]\n",
      " [30.91 15.18 13.65 28.02 29.77]]\n"
     ]
    }
   ],
   "source": [
    "ind = 101\n",
    "print(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)[:,5:10])\n",
    "print(y_test[:,5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
