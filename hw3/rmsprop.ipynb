{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def test_loss(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out2 = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "        loss = []\n",
    "        for iter in range(out2.shape[1]):\n",
    "            loss.append(MSE(out2[:, iter], y_in[:, iter]))\n",
    "        return np.mean(loss)\n",
    "\n",
    "def pred_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out1_temp = np.matmul(weights1.T, x_in[:,:])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        return out2\n",
    "\n",
    "def acc_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "    pred = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "    sum_acc = 0\n",
    "    for iter in range(pred.shape[1]):\n",
    "#         print(iter)\n",
    "        sum_acc+=(np.argmax(pred[:, iter])==np.argmax(y_in[:, iter]))\n",
    "    return sum_acc/y_in.shape[1]\n",
    "\n",
    "def MSE(yHat, y):\n",
    "    return np.sum(np.multiply(yHat - y, yHat - y)) / y.size\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-5):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.mean(np.multiply(np.log(predictions),targets)) \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 10)\n"
     ]
    }
   ],
   "source": [
    "##Load and segregate the data\n",
    "letters = genfromtxt('energy_eff2.csv', delimiter=',')\n",
    "# np.random.shuffle(letters)\n",
    "print(letters.shape)\n",
    "x_in = letters[:, 0:8]\n",
    "x_mean = np.mean(x_in, axis = 0)\n",
    "x_std = np.std(x_in, axis = 0)\n",
    "y_in = letters[:, 8:10]\n",
    "y_mean = np.mean(y_in, axis=0)\n",
    "y_std = np.std(y_in, axis = 0)\n",
    "# y_in = (y_in-y_mean)/y_std\n",
    "x_in = (x_in-x_mean)/x_std\n",
    "# print(np.std(y_in, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_in[0:int(x_in.shape[0]*0.7), :]\n",
    "y_train = y_in[0:int(x_in.shape[0]*0.7), :]\n",
    "\n",
    "x_test = x_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]\n",
    "y_test = y_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "learning_rate = 0.005\n",
    "num_epochs = 1000\n",
    "alpha = 0.9\n",
    "epsilon = 1e-12\n",
    "\n",
    "weights1 = np.random.normal(size=(x_in.shape[1], hidden_size))\n",
    "weights2 = np.random.normal(size=(hidden_size, y_in.shape[1]))\n",
    "print(weights1.shape)\n",
    "print(weights2.shape)\n",
    "biases1 = np.random.normal(size=(hidden_size, 1))\n",
    "biases2 = np.random.normal(size=(y_in.shape[1], 1))\n",
    "\n",
    "x_train = np.matrix(x_train)\n",
    "x_train = x_train.T\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = np.matrix(x_test)\n",
    "x_test = x_test.T\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  166.81684165353306\n",
      "train_loss :  185.3848234521781\n",
      "epoch_num :  1\n",
      "test_loss :  8.640668477172644\n",
      "train_loss :  9.192988811989629\n",
      "epoch_num :  6\n",
      "test_loss :  7.173117209617851\n",
      "train_loss :  7.162389256271597\n",
      "epoch_num :  11\n",
      "test_loss :  5.73350162851825\n",
      "train_loss :  5.7431874400281115\n",
      "epoch_num :  16\n",
      "test_loss :  4.762742807632007\n",
      "train_loss :  4.809705547499166\n",
      "epoch_num :  21\n",
      "test_loss :  4.109854460878398\n",
      "train_loss :  4.107989584341822\n",
      "epoch_num :  26\n",
      "test_loss :  3.7546628587235364\n",
      "train_loss :  3.6199230081892075\n",
      "epoch_num :  31\n",
      "test_loss :  3.5858790143381247\n",
      "train_loss :  3.296036663466525\n",
      "epoch_num :  36\n",
      "test_loss :  3.484197953441186\n",
      "train_loss :  3.07049353175832\n",
      "epoch_num :  41\n",
      "test_loss :  3.325560942753908\n",
      "train_loss :  2.881667663789369\n",
      "epoch_num :  46\n",
      "test_loss :  3.141069593851535\n",
      "train_loss :  2.7277442631092352\n",
      "epoch_num :  51\n",
      "test_loss :  2.9650614375501467\n",
      "train_loss :  2.5996570461701616\n",
      "epoch_num :  56\n",
      "test_loss :  2.813461358157763\n",
      "train_loss :  2.491631375835173\n",
      "epoch_num :  61\n",
      "test_loss :  2.7009151829826954\n",
      "train_loss :  2.4054642995595374\n",
      "epoch_num :  66\n",
      "test_loss :  2.617829201720776\n",
      "train_loss :  2.341952827566106\n",
      "epoch_num :  71\n",
      "test_loss :  2.53842570511547\n",
      "train_loss :  2.2813927797415134\n",
      "epoch_num :  76\n",
      "test_loss :  2.4680667071667894\n",
      "train_loss :  2.225917520109541\n",
      "epoch_num :  81\n",
      "test_loss :  2.4024868860862862\n",
      "train_loss :  2.1782893870011293\n",
      "epoch_num :  86\n",
      "test_loss :  2.3429744254389147\n",
      "train_loss :  2.1399973781354626\n",
      "epoch_num :  91\n",
      "test_loss :  2.2946266842847804\n",
      "train_loss :  2.110869223343258\n",
      "epoch_num :  96\n",
      "test_loss :  2.258717887148134\n",
      "train_loss :  2.087537005389806\n",
      "epoch_num :  101\n",
      "test_loss :  2.2243716957322466\n",
      "train_loss :  2.062778442343548\n",
      "epoch_num :  106\n",
      "test_loss :  2.1853364005519715\n",
      "train_loss :  2.030782155422459\n",
      "epoch_num :  111\n",
      "test_loss :  2.14202925520042\n",
      "train_loss :  1.9925458900472244\n",
      "epoch_num :  116\n",
      "test_loss :  2.094999189397162\n",
      "train_loss :  1.9522053277646703\n",
      "epoch_num :  121\n",
      "test_loss :  2.0410678216793117\n",
      "train_loss :  1.911420904963363\n",
      "epoch_num :  126\n",
      "test_loss :  1.9865209420333272\n",
      "train_loss :  1.8730295424968844\n",
      "epoch_num :  131\n",
      "test_loss :  1.9355549118603954\n",
      "train_loss :  1.8378270695787038\n",
      "epoch_num :  136\n",
      "test_loss :  1.891204867492568\n",
      "train_loss :  1.8061556249893314\n",
      "epoch_num :  141\n",
      "test_loss :  1.8631186955512433\n",
      "train_loss :  1.7843753756205762\n",
      "epoch_num :  146\n",
      "test_loss :  1.8442980778446654\n",
      "train_loss :  1.768688946790291\n",
      "epoch_num :  151\n",
      "test_loss :  1.830655679315652\n",
      "train_loss :  1.7561174718296473\n",
      "epoch_num :  156\n",
      "test_loss :  1.8209492572429495\n",
      "train_loss :  1.7447259012433765\n",
      "epoch_num :  161\n",
      "test_loss :  1.8144044408041127\n",
      "train_loss :  1.7334315461572027\n",
      "epoch_num :  166\n",
      "test_loss :  1.8101266490307968\n",
      "train_loss :  1.7223924893560383\n",
      "epoch_num :  171\n",
      "test_loss :  1.8073890723593682\n",
      "train_loss :  1.7124333518531338\n",
      "epoch_num :  176\n",
      "test_loss :  1.8055805885803398\n",
      "train_loss :  1.7037764260734762\n",
      "epoch_num :  181\n",
      "test_loss :  1.804385819315432\n",
      "train_loss :  1.6963502312272352\n",
      "epoch_num :  186\n",
      "test_loss :  1.8037339393019418\n",
      "train_loss :  1.6900606324325476\n",
      "epoch_num :  191\n",
      "test_loss :  1.8035470158295699\n",
      "train_loss :  1.684725938737292\n",
      "epoch_num :  196\n",
      "test_loss :  1.8037621732813878\n",
      "train_loss :  1.680194638338806\n",
      "epoch_num :  201\n",
      "test_loss :  1.8043518030444903\n",
      "train_loss :  1.6763798706114446\n",
      "epoch_num :  206\n",
      "test_loss :  1.8053063241577223\n",
      "train_loss :  1.673214897378322\n",
      "epoch_num :  211\n",
      "test_loss :  1.8066216598068634\n",
      "train_loss :  1.6706106312070355\n",
      "epoch_num :  216\n",
      "test_loss :  1.8082915805706525\n",
      "train_loss :  1.6684678114580596\n",
      "epoch_num :  221\n",
      "test_loss :  1.8102911313634034\n",
      "train_loss :  1.6666900575050272\n",
      "epoch_num :  226\n",
      "test_loss :  1.812549523163905\n",
      "train_loss :  1.6651699195034089\n",
      "epoch_num :  231\n",
      "test_loss :  1.814920910783442\n",
      "train_loss :  1.6637827778105838\n",
      "epoch_num :  236\n",
      "test_loss :  1.8171937012584145\n",
      "train_loss :  1.6624245560404725\n",
      "epoch_num :  241\n",
      "test_loss :  1.8192079687227571\n",
      "train_loss :  1.6610697821343985\n",
      "epoch_num :  246\n",
      "test_loss :  1.8209783402663444\n",
      "train_loss :  1.65976521441933\n",
      "epoch_num :  251\n",
      "test_loss :  1.822603648770996\n",
      "train_loss :  1.6585455189656093\n",
      "epoch_num :  256\n",
      "test_loss :  1.8241396619850119\n",
      "train_loss :  1.6573657017238725\n",
      "epoch_num :  261\n",
      "test_loss :  1.8256437331488482\n",
      "train_loss :  1.6561621283932135\n",
      "epoch_num :  266\n",
      "test_loss :  1.8273107239023552\n",
      "train_loss :  1.6549366311380875\n",
      "epoch_num :  271\n",
      "test_loss :  1.8295338281603313\n",
      "train_loss :  1.65372442769893\n",
      "epoch_num :  276\n",
      "test_loss :  1.8331396910502045\n",
      "train_loss :  1.6525744551885735\n",
      "epoch_num :  281\n",
      "test_loss :  1.839095415224813\n",
      "train_loss :  1.6515150393153053\n",
      "epoch_num :  286\n",
      "test_loss :  1.845222388496719\n",
      "train_loss :  1.6504903792997927\n",
      "epoch_num :  291\n",
      "test_loss :  1.8526312997169712\n",
      "train_loss :  1.6495218812140275\n",
      "epoch_num :  296\n",
      "test_loss :  1.8592204250898137\n",
      "train_loss :  1.6478657449412408\n",
      "epoch_num :  301\n",
      "test_loss :  1.8648028850588996\n",
      "train_loss :  1.6456997244654556\n",
      "epoch_num :  306\n",
      "test_loss :  1.8696940964306252\n",
      "train_loss :  1.643377315373747\n",
      "epoch_num :  311\n",
      "test_loss :  1.8740470204448207\n",
      "train_loss :  1.6411352430525799\n",
      "epoch_num :  316\n",
      "test_loss :  1.8777463573534403\n",
      "train_loss :  1.6390476505258242\n",
      "epoch_num :  321\n",
      "test_loss :  1.880640877877\n",
      "train_loss :  1.6369513645387783\n",
      "epoch_num :  326\n",
      "test_loss :  1.8830190054183462\n",
      "train_loss :  1.6347716861213737\n",
      "epoch_num :  331\n",
      "test_loss :  1.885189614843296\n",
      "train_loss :  1.6328766800407983\n",
      "epoch_num :  336\n",
      "test_loss :  1.8871960228024072\n",
      "train_loss :  1.631268471258414\n",
      "epoch_num :  341\n",
      "test_loss :  1.8889795945633063\n",
      "train_loss :  1.629825483507021\n",
      "epoch_num :  346\n",
      "test_loss :  1.8904852683806617\n",
      "train_loss :  1.6284647380781323\n",
      "epoch_num :  351\n",
      "test_loss :  1.891697372213428\n",
      "train_loss :  1.6271401796874496\n",
      "epoch_num :  356\n",
      "test_loss :  1.8926410460030003\n",
      "train_loss :  1.625816848276515\n",
      "epoch_num :  361\n",
      "test_loss :  1.8933781072508846\n",
      "train_loss :  1.6244585585961115\n",
      "epoch_num :  366\n",
      "test_loss :  1.8940096460698974\n",
      "train_loss :  1.6230302553623934\n",
      "epoch_num :  371\n",
      "test_loss :  1.8946870110202905\n",
      "train_loss :  1.621518954424971\n",
      "epoch_num :  376\n",
      "test_loss :  1.8956113319209884\n",
      "train_loss :  1.6199650042518488\n",
      "epoch_num :  381\n",
      "test_loss :  1.8969456508113969\n",
      "train_loss :  1.6184776048024316\n",
      "epoch_num :  386\n",
      "test_loss :  1.898515986667252\n",
      "train_loss :  1.617131851208677\n",
      "epoch_num :  391\n",
      "test_loss :  1.8995912682816039\n",
      "train_loss :  1.6157481537561589\n",
      "epoch_num :  396\n",
      "test_loss :  1.899624217814823\n",
      "train_loss :  1.6140901088586177\n",
      "epoch_num :  401\n",
      "test_loss :  1.898795609653643\n",
      "train_loss :  1.6121171766170228\n",
      "epoch_num :  406\n",
      "test_loss :  1.8974609802976916\n",
      "train_loss :  1.6098666203049574\n",
      "epoch_num :  411\n",
      "test_loss :  1.895819510242959\n",
      "train_loss :  1.6073780386577323\n",
      "epoch_num :  416\n",
      "test_loss :  1.8939460011184663\n",
      "train_loss :  1.6046970315340894\n",
      "epoch_num :  421\n",
      "test_loss :  1.8918518796563484\n",
      "train_loss :  1.6018919004069947\n",
      "epoch_num :  426\n",
      "test_loss :  1.8895218355530112\n",
      "train_loss :  1.5990589673637843\n",
      "epoch_num :  431\n",
      "test_loss :  1.8869848258324435\n",
      "train_loss :  1.5962944822453018\n",
      "epoch_num :  436\n",
      "test_loss :  1.8843368026159737\n",
      "train_loss :  1.5936808283335753\n",
      "epoch_num :  441\n",
      "test_loss :  1.8817164270792088\n",
      "train_loss :  1.5913354876829573\n",
      "epoch_num :  446\n",
      "test_loss :  1.8793011060199196\n",
      "train_loss :  1.5894564758536678\n",
      "epoch_num :  451\n",
      "test_loss :  1.8769604044920494\n",
      "train_loss :  1.5881048693616449\n",
      "epoch_num :  456\n",
      "test_loss :  1.8736475042646694\n",
      "train_loss :  1.5868757553720498\n",
      "epoch_num :  461\n",
      "test_loss :  1.8685008039727897\n",
      "train_loss :  1.5853607238343488\n",
      "epoch_num :  466\n",
      "test_loss :  1.8653647665058062\n",
      "train_loss :  1.585271202232681\n",
      "epoch_num :  471\n",
      "test_loss :  1.8635955159338635\n",
      "train_loss :  1.5857440375993996\n",
      "epoch_num :  476\n",
      "test_loss :  1.8626637891127524\n",
      "train_loss :  1.5861107753873331\n",
      "epoch_num :  481\n",
      "test_loss :  1.862515192523394\n",
      "train_loss :  1.5862679071650267\n",
      "epoch_num :  486\n",
      "test_loss :  1.863160240486204\n",
      "train_loss :  1.5863762208580867\n",
      "epoch_num :  491\n",
      "test_loss :  1.8647770240844652\n",
      "train_loss :  1.5866925044813682\n",
      "epoch_num :  496\n",
      "test_loss :  1.8671322737539862\n",
      "train_loss :  1.587137253871548\n",
      "epoch_num :  501\n",
      "test_loss :  1.8675761794061307\n",
      "train_loss :  1.586837323682175\n",
      "epoch_num :  506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  1.8628621295286516\n",
      "train_loss :  1.585026607025363\n",
      "epoch_num :  511\n",
      "test_loss :  1.8572552908093984\n",
      "train_loss :  1.5829281699666031\n",
      "epoch_num :  516\n",
      "test_loss :  1.8547615446618269\n",
      "train_loss :  1.5810167638490136\n",
      "epoch_num :  521\n",
      "test_loss :  1.857469981365282\n",
      "train_loss :  1.5791095651290858\n",
      "epoch_num :  526\n",
      "test_loss :  1.861307770880345\n",
      "train_loss :  1.5780136368982762\n",
      "epoch_num :  531\n",
      "test_loss :  1.8645215492763942\n",
      "train_loss :  1.5772917999155782\n",
      "epoch_num :  536\n",
      "test_loss :  1.8670235179706296\n",
      "train_loss :  1.576838795592966\n",
      "epoch_num :  541\n",
      "test_loss :  1.868998341870171\n",
      "train_loss :  1.5765956011288695\n",
      "epoch_num :  546\n",
      "test_loss :  1.8708626665650308\n",
      "train_loss :  1.5766161642708725\n",
      "epoch_num :  551\n",
      "test_loss :  1.8732125576702443\n",
      "train_loss :  1.5771063242724255\n",
      "epoch_num :  556\n",
      "test_loss :  1.8771786239160972\n",
      "train_loss :  1.578446774679051\n",
      "epoch_num :  561\n",
      "test_loss :  1.884211516933094\n",
      "train_loss :  1.5813286586403605\n",
      "epoch_num :  566\n",
      "test_loss :  1.8860839701384917\n",
      "train_loss :  1.5832467625866264\n",
      "epoch_num :  571\n",
      "test_loss :  1.8883819017179706\n",
      "train_loss :  1.58528861992895\n",
      "epoch_num :  576\n",
      "test_loss :  1.8915575049418165\n",
      "train_loss :  1.5874016352866807\n",
      "epoch_num :  581\n",
      "test_loss :  1.8944412525022836\n",
      "train_loss :  1.5891674205244777\n",
      "epoch_num :  586\n",
      "test_loss :  1.8972280238400177\n",
      "train_loss :  1.5906749620735154\n",
      "epoch_num :  591\n",
      "test_loss :  1.899638614210426\n",
      "train_loss :  1.592027470729759\n",
      "epoch_num :  596\n",
      "test_loss :  1.9016757675823746\n",
      "train_loss :  1.5931610951421322\n",
      "epoch_num :  601\n",
      "test_loss :  1.9034758853131442\n",
      "train_loss :  1.5940653632911634\n",
      "epoch_num :  606\n",
      "test_loss :  1.9050806439472423\n",
      "train_loss :  1.5948628105915243\n",
      "epoch_num :  611\n",
      "test_loss :  1.9066852957481994\n",
      "train_loss :  1.5958096864755857\n",
      "epoch_num :  616\n",
      "test_loss :  1.9086231858874703\n",
      "train_loss :  1.5972099693547894\n",
      "epoch_num :  621\n",
      "test_loss :  1.9108847518433312\n",
      "train_loss :  1.5992824935217795\n",
      "epoch_num :  626\n",
      "test_loss :  1.9130828328391793\n",
      "train_loss :  1.6019976763996129\n",
      "epoch_num :  631\n",
      "test_loss :  1.9139369976722536\n",
      "train_loss :  1.6037216189681458\n",
      "epoch_num :  636\n",
      "test_loss :  1.915347511739517\n",
      "train_loss :  1.6048491697334597\n",
      "epoch_num :  641\n",
      "test_loss :  1.9178668268770653\n",
      "train_loss :  1.6060059387408974\n",
      "epoch_num :  646\n",
      "test_loss :  1.9210414512494793\n",
      "train_loss :  1.606966653529548\n",
      "epoch_num :  651\n",
      "test_loss :  1.924263408089645\n",
      "train_loss :  1.6072713056168177\n",
      "epoch_num :  656\n",
      "test_loss :  1.9260433907813916\n",
      "train_loss :  1.607370643199546\n",
      "epoch_num :  661\n",
      "test_loss :  1.9245222154742736\n",
      "train_loss :  1.6058729263953957\n",
      "epoch_num :  666\n",
      "test_loss :  1.92326847346352\n",
      "train_loss :  1.6041726832691017\n",
      "epoch_num :  671\n",
      "test_loss :  1.9218743993756788\n",
      "train_loss :  1.6024758136801318\n",
      "epoch_num :  676\n",
      "test_loss :  1.9204767997258412\n",
      "train_loss :  1.6007925130638785\n",
      "epoch_num :  681\n",
      "test_loss :  1.9192298758678097\n",
      "train_loss :  1.599126187620556\n",
      "epoch_num :  686\n",
      "test_loss :  1.9182422978890368\n",
      "train_loss :  1.5974523026297092\n",
      "epoch_num :  691\n",
      "test_loss :  1.9175442923344557\n",
      "train_loss :  1.5957497525155078\n",
      "epoch_num :  696\n",
      "test_loss :  1.9170861391314384\n",
      "train_loss :  1.5940205724981429\n",
      "epoch_num :  701\n",
      "test_loss :  1.9167613597290725\n",
      "train_loss :  1.592297158177719\n",
      "epoch_num :  706\n",
      "test_loss :  1.9164281573052175\n",
      "train_loss :  1.5906470986794592\n",
      "epoch_num :  711\n",
      "test_loss :  1.9159352591259953\n",
      "train_loss :  1.5891619696815584\n",
      "epoch_num :  716\n",
      "test_loss :  1.9151649598195313\n",
      "train_loss :  1.5879249678604368\n",
      "epoch_num :  721\n",
      "test_loss :  1.9140895375663596\n",
      "train_loss :  1.5869946855856008\n",
      "epoch_num :  726\n",
      "test_loss :  1.912761871875313\n",
      "train_loss :  1.5863886608883426\n",
      "epoch_num :  731\n",
      "test_loss :  1.9112268884568835\n",
      "train_loss :  1.5860618281471424\n",
      "epoch_num :  736\n",
      "test_loss :  1.909522173795206\n",
      "train_loss :  1.5859568617664612\n",
      "epoch_num :  741\n",
      "test_loss :  1.9077977436953435\n",
      "train_loss :  1.5860818637512004\n",
      "epoch_num :  746\n",
      "test_loss :  1.9063975166540896\n",
      "train_loss :  1.5865484181344796\n",
      "epoch_num :  751\n",
      "test_loss :  1.9055919648399184\n",
      "train_loss :  1.587254964075041\n",
      "epoch_num :  756\n",
      "test_loss :  1.9043247155936776\n",
      "train_loss :  1.586946834344911\n",
      "epoch_num :  761\n",
      "test_loss :  1.9006856505737717\n",
      "train_loss :  1.5853837248234228\n",
      "epoch_num :  766\n",
      "test_loss :  1.895535685607384\n",
      "train_loss :  1.5840916245036387\n",
      "epoch_num :  771\n",
      "test_loss :  1.8892636214703324\n",
      "train_loss :  1.5831324696550224\n",
      "epoch_num :  776\n",
      "test_loss :  1.8827197133988598\n",
      "train_loss :  1.5819682191402284\n",
      "epoch_num :  781\n",
      "test_loss :  1.8772089389212223\n",
      "train_loss :  1.5800850111690101\n",
      "epoch_num :  786\n",
      "test_loss :  1.873958018522531\n",
      "train_loss :  1.5774548174112004\n",
      "epoch_num :  791\n",
      "test_loss :  1.8720982369506196\n",
      "train_loss :  1.5730593318340382\n",
      "epoch_num :  796\n",
      "test_loss :  1.8681734453614152\n",
      "train_loss :  1.5672109538964962\n",
      "epoch_num :  801\n",
      "test_loss :  1.8654846672404566\n",
      "train_loss :  1.559358230852447\n",
      "epoch_num :  806\n",
      "test_loss :  1.8603633889782176\n",
      "train_loss :  1.5503149615018632\n",
      "epoch_num :  811\n",
      "test_loss :  1.8566568270764985\n",
      "train_loss :  1.5413220054781442\n",
      "epoch_num :  816\n",
      "test_loss :  1.8565751190538171\n",
      "train_loss :  1.5325595282180202\n",
      "epoch_num :  821\n",
      "test_loss :  1.8595755355480508\n",
      "train_loss :  1.530729902269229\n",
      "epoch_num :  826\n",
      "test_loss :  1.8615830815985948\n",
      "train_loss :  1.531358032628133\n",
      "epoch_num :  831\n",
      "test_loss :  1.8569441698168172\n",
      "train_loss :  1.5307384198208143\n",
      "epoch_num :  836\n",
      "test_loss :  1.8559224386209174\n",
      "train_loss :  1.527379139861039\n",
      "epoch_num :  841\n",
      "test_loss :  1.856314706235466\n",
      "train_loss :  1.5255857534609998\n",
      "epoch_num :  846\n",
      "test_loss :  1.8569789035652418\n",
      "train_loss :  1.5268202928035843\n",
      "epoch_num :  851\n",
      "test_loss :  1.8571520912851354\n",
      "train_loss :  1.5307238423507785\n",
      "epoch_num :  856\n",
      "test_loss :  1.8606442196994482\n",
      "train_loss :  1.5343778588550279\n",
      "epoch_num :  861\n",
      "test_loss :  1.86568532393342\n",
      "train_loss :  1.5358921072485785\n",
      "epoch_num :  866\n",
      "test_loss :  1.8712676339819148\n",
      "train_loss :  1.5370575031684095\n",
      "epoch_num :  871\n",
      "test_loss :  1.876594593484588\n",
      "train_loss :  1.5382846157703047\n",
      "epoch_num :  876\n",
      "test_loss :  1.8814050374858518\n",
      "train_loss :  1.539387773782595\n",
      "epoch_num :  881\n",
      "test_loss :  1.8857861370883309\n",
      "train_loss :  1.540277410737542\n",
      "epoch_num :  886\n",
      "test_loss :  1.889832677411594\n",
      "train_loss :  1.5409258310112863\n",
      "epoch_num :  891\n",
      "test_loss :  1.893610216406965\n",
      "train_loss :  1.541758340345448\n",
      "epoch_num :  896\n",
      "test_loss :  1.895771389097291\n",
      "train_loss :  1.5427075811619335\n",
      "epoch_num :  901\n",
      "test_loss :  1.8969109441756609\n",
      "train_loss :  1.5426696642330604\n",
      "epoch_num :  906\n",
      "test_loss :  1.8978257911766212\n",
      "train_loss :  1.5424813730046634\n",
      "epoch_num :  911\n",
      "test_loss :  1.8987896929065664\n",
      "train_loss :  1.542405465714024\n",
      "epoch_num :  916\n",
      "test_loss :  1.9000857577443986\n",
      "train_loss :  1.542594670942989\n",
      "epoch_num :  921\n",
      "test_loss :  1.9020783197827722\n",
      "train_loss :  1.5435577444248907\n",
      "epoch_num :  926\n",
      "test_loss :  1.9048653146415608\n",
      "train_loss :  1.545579216242304\n",
      "epoch_num :  931\n",
      "test_loss :  1.9080906919122678\n",
      "train_loss :  1.54825189510394\n",
      "epoch_num :  936\n",
      "test_loss :  1.9112043940937127\n",
      "train_loss :  1.5511337516832227\n",
      "epoch_num :  941\n",
      "test_loss :  1.9138151717666059\n",
      "train_loss :  1.5539492799651569\n",
      "epoch_num :  946\n",
      "test_loss :  1.9156155931889634\n",
      "train_loss :  1.5565935903123234\n",
      "epoch_num :  951\n",
      "test_loss :  1.9164142824139911\n",
      "train_loss :  1.559093577868107\n",
      "epoch_num :  956\n",
      "test_loss :  1.9162383184182812\n",
      "train_loss :  1.561523157133251\n",
      "epoch_num :  961\n",
      "test_loss :  1.9152024177853233\n",
      "train_loss :  1.5639432075773518\n",
      "epoch_num :  966\n",
      "test_loss :  1.9133833876042827\n",
      "train_loss :  1.5663897333793457\n",
      "epoch_num :  971\n",
      "test_loss :  1.9107790058890342\n",
      "train_loss :  1.568882569655193\n",
      "epoch_num :  976\n",
      "test_loss :  1.9073268011610545\n",
      "train_loss :  1.571460125267351\n",
      "epoch_num :  981\n",
      "test_loss :  1.9030519106423718\n",
      "train_loss :  1.5742860814061976\n",
      "epoch_num :  986\n",
      "test_loss :  1.89827685150206\n",
      "train_loss :  1.5776938332816146\n",
      "epoch_num :  991\n",
      "test_loss :  1.8933662678745857\n",
      "train_loss :  1.5818327832839372\n",
      "epoch_num :  996\n"
     ]
    }
   ],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "biases1_squares = 0\n",
    "biases2_squares = 0\n",
    "weights1_squares = 0\n",
    "weights2_squares = 0\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "    if epoch_num%5==1:\n",
    "        print(\"test_loss : \", test_losses[-1])\n",
    "        print(\"train_loss : \", losses[-1])\n",
    "        print(\"epoch_num : \", epoch_num)\n",
    "    for sample_num in range(x_train.shape[1]):\n",
    "#         print(\"sample_num : \", sample_num)\n",
    "        #Forward propagation\n",
    "        out1_temp = np.matmul(weights1.T, x_train[:,sample_num])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        #Back-propogation\n",
    "        error_out = out2-y_train[:, sample_num]\n",
    "        err_2 = np.matrix(error_out)\n",
    "        grad_2 = np.matmul(out1, err_2.T)\n",
    "        err_1 = np.multiply(np.matmul(weights2, err_2), np.multiply(out1, np.ones(out1.shape)-out1))\n",
    "        grad_1 = np.multiply(x_train[:, sample_num], err_1.T)\n",
    "        #Updating the weights\n",
    "        \n",
    "        biases1_squares = alpha*biases1_squares+(1-alpha)*np.multiply(err_1, err_1)\n",
    "        biases2_squares = alpha*biases2_squares+(1-alpha)*np.multiply(err_2, err_2)\n",
    "        weights1_squares = alpha*weights1_squares+(1-alpha)*np.multiply(grad_1, grad_1)\n",
    "        weights2_squares = alpha*weights2_squares+(1-alpha)*np.multiply(grad_2, grad_2)\n",
    "        \n",
    "        biases1 = biases1 - np.multiply(learning_rate/(np.sqrt(biases1_squares)+epsilon), err_1)\n",
    "        biases2 = biases2 - np.multiply(learning_rate/(np.sqrt(biases2_squares)+epsilon), err_2)\n",
    "        weights1 = weights1 - np.multiply(learning_rate/(np.sqrt(weights1_squares)+epsilon), grad_1)\n",
    "        weights2 = weights2 - np.multiply(learning_rate/(np.sqrt(weights2_squares)+epsilon), grad_2)\n",
    "        #Updating the accuracy and loss\n",
    "        #End of for loop\n",
    "#     print(\"accuracy : \", np.mean(acc_arr))\n",
    "    test_losses.append(test_loss(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_test.append(acc_out(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_train.append(acc_out(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    losses.append(test_loss(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot(211)\n",
    "plt.plot(losses[1:], label = 'train_loss')\n",
    "plt.plot(test_losses[1:], label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.multiply(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test, pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25.77571768 12.32670325 10.47579336 28.21683429 29.16366175]\n",
      " [29.12482345 15.18571226 13.70949541 31.20425776 29.76592732]]\n",
      "[[26.48 12.16 10.35 29.71 29.47]\n",
      " [30.91 15.18 13.65 28.02 29.77]]\n"
     ]
    }
   ],
   "source": [
    "ind = 101\n",
    "print(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)[:,5:10])\n",
    "print(y_test[:,5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
