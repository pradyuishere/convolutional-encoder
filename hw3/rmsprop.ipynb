{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def test_loss(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out2 = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "        loss = []\n",
    "        for iter in range(out2.shape[1]):\n",
    "            loss.append(MSE(out2[:, iter], y_in[:, iter]))\n",
    "        return np.mean(loss)\n",
    "\n",
    "def pred_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out1_temp = np.matmul(weights1.T, x_in[:,:])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        return out2\n",
    "\n",
    "def acc_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "    pred = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "    sum_acc = 0\n",
    "    for iter in range(pred.shape[1]):\n",
    "#         print(iter)\n",
    "        sum_acc+=(np.argmax(pred[:, iter])==np.argmax(y_in[:, iter]))\n",
    "    return sum_acc/y_in.shape[1]\n",
    "\n",
    "def MSE(yHat, y):\n",
    "    return np.sum(np.multiply(yHat - y, yHat - y)) / y.size\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-5):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.mean(np.multiply(np.log(predictions),targets)) \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 10)\n"
     ]
    }
   ],
   "source": [
    "##Load and segregate the data\n",
    "letters = genfromtxt('energy_eff2.csv', delimiter=',')\n",
    "# np.random.shuffle(letters)\n",
    "print(letters.shape)\n",
    "x_in = letters[:, 0:8]\n",
    "x_mean = np.mean(x_in, axis = 0)\n",
    "x_std = np.std(x_in, axis = 0)\n",
    "y_in = letters[:, 8:10]\n",
    "y_mean = np.mean(y_in, axis=0)\n",
    "y_std = np.std(y_in, axis = 0)\n",
    "# y_in = (y_in-y_mean)/y_std\n",
    "x_in = (x_in-x_mean)/x_std\n",
    "# print(np.std(y_in, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_in[0:int(x_in.shape[0]*0.7), :]\n",
    "y_train = y_in[0:int(x_in.shape[0]*0.7), :]\n",
    "\n",
    "x_test = x_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]\n",
    "y_test = y_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "learning_rate = 0.005\n",
    "num_epochs = 1000\n",
    "alpha = 0.9\n",
    "epsilon = 1e-12\n",
    "\n",
    "weights1 = np.random.normal(size=(x_in.shape[1], hidden_size))\n",
    "weights2 = np.random.normal(size=(hidden_size, y_in.shape[1]))\n",
    "print(weights1.shape)\n",
    "print(weights2.shape)\n",
    "biases1 = np.random.normal(size=(hidden_size, 1))\n",
    "biases2 = np.random.normal(size=(y_in.shape[1], 1))\n",
    "\n",
    "x_train = np.matrix(x_train)\n",
    "x_train = x_train.T\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = np.matrix(x_test)\n",
    "x_test = x_test.T\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  166.81684165353306\n",
      "train_loss :  185.3848234521781\n",
      "epoch_num :  1\n",
      "test_loss :  8.640668477172644\n",
      "train_loss :  9.192988811989629\n",
      "epoch_num :  6\n",
      "test_loss :  7.173117209617851\n",
      "train_loss :  7.162389256271597\n",
      "epoch_num :  11\n",
      "test_loss :  5.73350162851825\n",
      "train_loss :  5.7431874400281115\n",
      "epoch_num :  16\n",
      "test_loss :  4.762742807632007\n",
      "train_loss :  4.809705547499166\n",
      "epoch_num :  21\n",
      "test_loss :  4.109854460878398\n",
      "train_loss :  4.107989584341822\n",
      "epoch_num :  26\n",
      "test_loss :  3.7546628587235364\n",
      "train_loss :  3.6199230081892075\n",
      "epoch_num :  31\n",
      "test_loss :  3.5858790143381247\n",
      "train_loss :  3.296036663466525\n",
      "epoch_num :  36\n",
      "test_loss :  3.484197953441186\n",
      "train_loss :  3.07049353175832\n",
      "epoch_num :  41\n",
      "test_loss :  3.325560942753908\n",
      "train_loss :  2.881667663789369\n",
      "epoch_num :  46\n",
      "test_loss :  3.141069593851535\n",
      "train_loss :  2.7277442631092352\n",
      "epoch_num :  51\n",
      "test_loss :  2.9650614375501467\n",
      "train_loss :  2.5996570461701616\n",
      "epoch_num :  56\n",
      "test_loss :  2.813461358157763\n",
      "train_loss :  2.491631375835173\n",
      "epoch_num :  61\n",
      "test_loss :  2.7009151829826954\n",
      "train_loss :  2.4054642995595374\n",
      "epoch_num :  66\n",
      "test_loss :  2.617829201720776\n",
      "train_loss :  2.341952827566106\n",
      "epoch_num :  71\n",
      "test_loss :  2.53842570511547\n",
      "train_loss :  2.2813927797415134\n",
      "epoch_num :  76\n",
      "test_loss :  2.4680667071667894\n",
      "train_loss :  2.225917520109541\n",
      "epoch_num :  81\n",
      "test_loss :  2.4024868860862862\n",
      "train_loss :  2.1782893870011293\n",
      "epoch_num :  86\n",
      "test_loss :  2.3429744254389147\n",
      "train_loss :  2.1399973781354626\n",
      "epoch_num :  91\n",
      "test_loss :  2.2946266842847804\n",
      "train_loss :  2.110869223343258\n",
      "epoch_num :  96\n",
      "test_loss :  2.258717887148134\n",
      "train_loss :  2.087537005389806\n",
      "epoch_num :  101\n",
      "test_loss :  2.2243716957322466\n",
      "train_loss :  2.062778442343548\n",
      "epoch_num :  106\n",
      "test_loss :  2.1853364005519715\n",
      "train_loss :  2.030782155422459\n",
      "epoch_num :  111\n",
      "test_loss :  2.14202925520042\n",
      "train_loss :  1.9925458900472244\n",
      "epoch_num :  116\n",
      "test_loss :  2.094999189397162\n",
      "train_loss :  1.9522053277646703\n",
      "epoch_num :  121\n",
      "test_loss :  2.0410678216793117\n",
      "train_loss :  1.911420904963363\n",
      "epoch_num :  126\n",
      "test_loss :  1.9865209420333272\n",
      "train_loss :  1.8730295424968844\n",
      "epoch_num :  131\n",
      "test_loss :  1.9355549118603954\n",
      "train_loss :  1.8378270695787038\n",
      "epoch_num :  136\n",
      "test_loss :  1.891204867492568\n",
      "train_loss :  1.8061556249893314\n",
      "epoch_num :  141\n",
      "test_loss :  1.8631186955512433\n",
      "train_loss :  1.7843753756205762\n",
      "epoch_num :  146\n",
      "test_loss :  1.8442980778446654\n",
      "train_loss :  1.768688946790291\n",
      "epoch_num :  151\n",
      "test_loss :  1.830655679315652\n",
      "train_loss :  1.7561174718296473\n",
      "epoch_num :  156\n",
      "test_loss :  1.8209492572429495\n",
      "train_loss :  1.7447259012433765\n",
      "epoch_num :  161\n",
      "test_loss :  1.8144044408041127\n",
      "train_loss :  1.7334315461572027\n",
      "epoch_num :  166\n",
      "test_loss :  1.8101266490307968\n",
      "train_loss :  1.7223924893560383\n",
      "epoch_num :  171\n",
      "test_loss :  1.8073890723593682\n",
      "train_loss :  1.7124333518531338\n",
      "epoch_num :  176\n",
      "test_loss :  1.8055805885803398\n",
      "train_loss :  1.7037764260734762\n",
      "epoch_num :  181\n",
      "test_loss :  1.804385819315432\n",
      "train_loss :  1.6963502312272352\n",
      "epoch_num :  186\n",
      "test_loss :  1.8037339393019418\n",
      "train_loss :  1.6900606324325476\n",
      "epoch_num :  191\n",
      "test_loss :  1.8035470158295699\n",
      "train_loss :  1.684725938737292\n",
      "epoch_num :  196\n",
      "test_loss :  1.8037621732813878\n",
      "train_loss :  1.680194638338806\n",
      "epoch_num :  201\n",
      "test_loss :  1.8043518030444903\n",
      "train_loss :  1.6763798706114446\n",
      "epoch_num :  206\n",
      "test_loss :  1.8053063241577223\n",
      "train_loss :  1.673214897378322\n",
      "epoch_num :  211\n",
      "test_loss :  1.8066216598068634\n",
      "train_loss :  1.6706106312070355\n",
      "epoch_num :  216\n",
      "test_loss :  1.8082915805706525\n",
      "train_loss :  1.6684678114580596\n",
      "epoch_num :  221\n",
      "test_loss :  1.8102911313634034\n",
      "train_loss :  1.6666900575050272\n",
      "epoch_num :  226\n",
      "test_loss :  1.812549523163905\n",
      "train_loss :  1.6651699195034089\n",
      "epoch_num :  231\n",
      "test_loss :  1.814920910783442\n",
      "train_loss :  1.6637827778105838\n",
      "epoch_num :  236\n",
      "test_loss :  1.8171937012584145\n",
      "train_loss :  1.6624245560404725\n",
      "epoch_num :  241\n",
      "test_loss :  1.8192079687227571\n",
      "train_loss :  1.6610697821343985\n",
      "epoch_num :  246\n",
      "test_loss :  1.8209783402663444\n",
      "train_loss :  1.65976521441933\n",
      "epoch_num :  251\n",
      "test_loss :  1.822603648770996\n",
      "train_loss :  1.6585455189656093\n",
      "epoch_num :  256\n",
      "test_loss :  1.8241396619850119\n",
      "train_loss :  1.6573657017238725\n",
      "epoch_num :  261\n",
      "test_loss :  1.8256437331488482\n",
      "train_loss :  1.6561621283932135\n",
      "epoch_num :  266\n",
      "test_loss :  1.8273107239023552\n",
      "train_loss :  1.6549366311380875\n",
      "epoch_num :  271\n",
      "test_loss :  1.8295338281603313\n",
      "train_loss :  1.65372442769893\n",
      "epoch_num :  276\n",
      "test_loss :  1.8331396910502045\n",
      "train_loss :  1.6525744551885735\n",
      "epoch_num :  281\n",
      "test_loss :  1.839095415224813\n",
      "train_loss :  1.6515150393153053\n",
      "epoch_num :  286\n",
      "test_loss :  1.845222388496719\n",
      "train_loss :  1.6504903792997927\n",
      "epoch_num :  291\n",
      "test_loss :  1.8526312997169712\n",
      "train_loss :  1.6495218812140275\n",
      "epoch_num :  296\n",
      "test_loss :  1.8592204250898137\n",
      "train_loss :  1.6478657449412408\n",
      "epoch_num :  301\n",
      "test_loss :  1.8648028850588996\n",
      "train_loss :  1.6456997244654556\n",
      "epoch_num :  306\n",
      "test_loss :  1.8696940964306252\n",
      "train_loss :  1.643377315373747\n",
      "epoch_num :  311\n",
      "test_loss :  1.8740470204448207\n",
      "train_loss :  1.6411352430525799\n",
      "epoch_num :  316\n",
      "test_loss :  1.8777463573534403\n",
      "train_loss :  1.6390476505258242\n",
      "epoch_num :  321\n",
      "test_loss :  1.880640877877\n",
      "train_loss :  1.6369513645387783\n",
      "epoch_num :  326\n",
      "test_loss :  1.8830190054183462\n",
      "train_loss :  1.6347716861213737\n",
      "epoch_num :  331\n",
      "test_loss :  1.885189614843296\n",
      "train_loss :  1.6328766800407983\n",
      "epoch_num :  336\n",
      "test_loss :  1.8871960228024072\n",
      "train_loss :  1.631268471258414\n",
      "epoch_num :  341\n",
      "test_loss :  1.8889795945633063\n",
      "train_loss :  1.629825483507021\n",
      "epoch_num :  346\n",
      "test_loss :  1.8904852683806617\n",
      "train_loss :  1.6284647380781323\n",
      "epoch_num :  351\n",
      "test_loss :  1.891697372213428\n",
      "train_loss :  1.6271401796874496\n",
      "epoch_num :  356\n",
      "test_loss :  1.8926410460030003\n",
      "train_loss :  1.625816848276515\n",
      "epoch_num :  361\n",
      "test_loss :  1.8933781072508846\n",
      "train_loss :  1.6244585585961115\n",
      "epoch_num :  366\n",
      "test_loss :  1.8940096460698974\n",
      "train_loss :  1.6230302553623934\n",
      "epoch_num :  371\n",
      "test_loss :  1.8946870110202905\n",
      "train_loss :  1.621518954424971\n",
      "epoch_num :  376\n",
      "test_loss :  1.8956113319209884\n",
      "train_loss :  1.6199650042518488\n",
      "epoch_num :  381\n",
      "test_loss :  1.8969456508113969\n",
      "train_loss :  1.6184776048024316\n",
      "epoch_num :  386\n",
      "test_loss :  1.898515986667252\n",
      "train_loss :  1.617131851208677\n",
      "epoch_num :  391\n",
      "test_loss :  1.8995912682816039\n",
      "train_loss :  1.6157481537561589\n",
      "epoch_num :  396\n",
      "test_loss :  1.899624217814823\n",
      "train_loss :  1.6140901088586177\n",
      "epoch_num :  401\n",
      "test_loss :  1.898795609653643\n",
      "train_loss :  1.6121171766170228\n",
      "epoch_num :  406\n",
      "test_loss :  1.8974609802976916\n",
      "train_loss :  1.6098666203049574\n",
      "epoch_num :  411\n",
      "test_loss :  1.895819510242959\n",
      "train_loss :  1.6073780386577323\n",
      "epoch_num :  416\n",
      "test_loss :  1.8939460011184663\n",
      "train_loss :  1.6046970315340894\n",
      "epoch_num :  421\n",
      "test_loss :  1.8918518796563484\n",
      "train_loss :  1.6018919004069947\n",
      "epoch_num :  426\n",
      "test_loss :  1.8895218355530112\n",
      "train_loss :  1.5990589673637843\n",
      "epoch_num :  431\n",
      "test_loss :  1.8869848258324435\n",
      "train_loss :  1.5962944822453018\n",
      "epoch_num :  436\n",
      "test_loss :  1.8843368026159737\n",
      "train_loss :  1.5936808283335753\n",
      "epoch_num :  441\n",
      "test_loss :  1.8817164270792088\n",
      "train_loss :  1.5913354876829573\n",
      "epoch_num :  446\n",
      "test_loss :  1.8793011060199196\n",
      "train_loss :  1.5894564758536678\n",
      "epoch_num :  451\n",
      "test_loss :  1.8769604044920494\n",
      "train_loss :  1.5881048693616449\n",
      "epoch_num :  456\n",
      "test_loss :  1.8736475042646694\n",
      "train_loss :  1.5868757553720498\n",
      "epoch_num :  461\n",
      "test_loss :  1.8685008039727897\n",
      "train_loss :  1.5853607238343488\n",
      "epoch_num :  466\n",
      "test_loss :  1.8653647665058062\n",
      "train_loss :  1.585271202232681\n",
      "epoch_num :  471\n",
      "test_loss :  1.8635955159338635\n",
      "train_loss :  1.5857440375993996\n",
      "epoch_num :  476\n",
      "test_loss :  1.8626637891127524\n",
      "train_loss :  1.5861107753873331\n",
      "epoch_num :  481\n",
      "test_loss :  1.862515192523394\n",
      "train_loss :  1.5862679071650267\n",
      "epoch_num :  486\n",
      "test_loss :  1.863160240486204\n",
      "train_loss :  1.5863762208580867\n",
      "epoch_num :  491\n",
      "test_loss :  1.8647770240844652\n",
      "train_loss :  1.5866925044813682\n",
      "epoch_num :  496\n",
      "test_loss :  1.8671322737539862\n",
      "train_loss :  1.587137253871548\n",
      "epoch_num :  501\n",
      "test_loss :  1.8675761794061307\n",
      "train_loss :  1.586837323682175\n",
      "epoch_num :  506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  1.8628621295286516\n",
      "train_loss :  1.585026607025363\n",
      "epoch_num :  511\n",
      "test_loss :  1.8572552908093984\n",
      "train_loss :  1.5829281699666031\n",
      "epoch_num :  516\n",
      "test_loss :  1.8547615446618269\n",
      "train_loss :  1.5810167638490136\n",
      "epoch_num :  521\n",
      "test_loss :  1.857469981365282\n",
      "train_loss :  1.5791095651290858\n",
      "epoch_num :  526\n",
      "test_loss :  1.861307770880345\n",
      "train_loss :  1.5780136368982762\n",
      "epoch_num :  531\n",
      "test_loss :  1.8645215492763942\n",
      "train_loss :  1.5772917999155782\n",
      "epoch_num :  536\n",
      "test_loss :  1.8670235179706296\n",
      "train_loss :  1.576838795592966\n",
      "epoch_num :  541\n",
      "test_loss :  1.868998341870171\n",
      "train_loss :  1.5765956011288695\n",
      "epoch_num :  546\n",
      "test_loss :  1.8708626665650308\n",
      "train_loss :  1.5766161642708725\n",
      "epoch_num :  551\n",
      "test_loss :  1.8732125576702443\n",
      "train_loss :  1.5771063242724255\n",
      "epoch_num :  556\n",
      "test_loss :  1.8771786239160972\n",
      "train_loss :  1.578446774679051\n",
      "epoch_num :  561\n",
      "test_loss :  1.884211516933094\n",
      "train_loss :  1.5813286586403605\n",
      "epoch_num :  566\n",
      "test_loss :  1.8860839701384917\n",
      "train_loss :  1.5832467625866264\n",
      "epoch_num :  571\n",
      "test_loss :  1.8883819017179706\n",
      "train_loss :  1.58528861992895\n",
      "epoch_num :  576\n",
      "test_loss :  1.8915575049418165\n",
      "train_loss :  1.5874016352866807\n",
      "epoch_num :  581\n",
      "test_loss :  1.8944412525022836\n",
      "train_loss :  1.5891674205244777\n",
      "epoch_num :  586\n",
      "test_loss :  1.8972280238400177\n",
      "train_loss :  1.5906749620735154\n",
      "epoch_num :  591\n",
      "test_loss :  1.899638614210426\n",
      "train_loss :  1.592027470729759\n",
      "epoch_num :  596\n",
      "test_loss :  1.9016757675823746\n",
      "train_loss :  1.5931610951421322\n",
      "epoch_num :  601\n",
      "test_loss :  1.9034758853131442\n",
      "train_loss :  1.5940653632911634\n",
      "epoch_num :  606\n",
      "test_loss :  1.9050806439472423\n",
      "train_loss :  1.5948628105915243\n",
      "epoch_num :  611\n",
      "test_loss :  1.9066852957481994\n",
      "train_loss :  1.5958096864755857\n",
      "epoch_num :  616\n",
      "test_loss :  1.9086231858874703\n",
      "train_loss :  1.5972099693547894\n",
      "epoch_num :  621\n",
      "test_loss :  1.9108847518433312\n",
      "train_loss :  1.5992824935217795\n",
      "epoch_num :  626\n",
      "test_loss :  1.9130828328391793\n",
      "train_loss :  1.6019976763996129\n",
      "epoch_num :  631\n",
      "test_loss :  1.9139369976722536\n",
      "train_loss :  1.6037216189681458\n",
      "epoch_num :  636\n",
      "test_loss :  1.915347511739517\n",
      "train_loss :  1.6048491697334597\n",
      "epoch_num :  641\n",
      "test_loss :  1.9178668268770653\n",
      "train_loss :  1.6060059387408974\n",
      "epoch_num :  646\n",
      "test_loss :  1.9210414512494793\n",
      "train_loss :  1.606966653529548\n",
      "epoch_num :  651\n",
      "test_loss :  1.924263408089645\n",
      "train_loss :  1.6072713056168177\n",
      "epoch_num :  656\n",
      "test_loss :  1.9260433907813916\n",
      "train_loss :  1.607370643199546\n",
      "epoch_num :  661\n",
      "test_loss :  1.9245222154742736\n",
      "train_loss :  1.6058729263953957\n",
      "epoch_num :  666\n",
      "test_loss :  1.92326847346352\n",
      "train_loss :  1.6041726832691017\n",
      "epoch_num :  671\n",
      "test_loss :  1.9218743993756788\n",
      "train_loss :  1.6024758136801318\n",
      "epoch_num :  676\n",
      "test_loss :  1.9204767997258412\n",
      "train_loss :  1.6007925130638785\n",
      "epoch_num :  681\n",
      "test_loss :  1.9192298758678097\n",
      "train_loss :  1.599126187620556\n",
      "epoch_num :  686\n",
      "test_loss :  1.9182422978890368\n",
      "train_loss :  1.5974523026297092\n",
      "epoch_num :  691\n",
      "test_loss :  1.9175442923344557\n",
      "train_loss :  1.5957497525155078\n",
      "epoch_num :  696\n",
      "test_loss :  1.9170861391314384\n",
      "train_loss :  1.5940205724981429\n",
      "epoch_num :  701\n",
      "test_loss :  1.9167613597290725\n",
      "train_loss :  1.592297158177719\n",
      "epoch_num :  706\n",
      "test_loss :  1.9164281573052175\n",
      "train_loss :  1.5906470986794592\n",
      "epoch_num :  711\n"
     ]
    }
   ],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "biases1_squares = 0\n",
    "biases2_squares = 0\n",
    "weights1_squares = 0\n",
    "weights2_squares = 0\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "    if epoch_num%5==1:\n",
    "        print(\"test_loss : \", test_losses[-1])\n",
    "        print(\"train_loss : \", losses[-1])\n",
    "        print(\"epoch_num : \", epoch_num)\n",
    "    for sample_num in range(x_train.shape[1]):\n",
    "#         print(\"sample_num : \", sample_num)\n",
    "        #Forward propagation\n",
    "        out1_temp = np.matmul(weights1.T, x_train[:,sample_num])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        #Back-propogation\n",
    "        error_out = out2-y_train[:, sample_num]\n",
    "        err_2 = np.matrix(error_out)\n",
    "        grad_2 = np.matmul(out1, err_2.T)\n",
    "        err_1 = np.multiply(np.matmul(weights2, err_2), np.multiply(out1, np.ones(out1.shape)-out1))\n",
    "        grad_1 = np.multiply(x_train[:, sample_num], err_1.T)\n",
    "        #Updating the weights\n",
    "        \n",
    "        biases1_squares = alpha*biases1_squares+(1-alpha)*np.multiply(err_1, err_1)\n",
    "        biases2_squares = alpha*biases2_squares+(1-alpha)*np.multiply(err_2, err_2)\n",
    "        weights1_squares = alpha*weights1_squares+(1-alpha)*np.multiply(grad_1, grad_1)\n",
    "        weights2_squares = alpha*weights2_squares+(1-alpha)*np.multiply(grad_2, grad_2)\n",
    "        \n",
    "        biases1 = biases1 - np.multiply(learning_rate/(np.sqrt(biases1_squares)+epsilon), err_1)\n",
    "        biases2 = biases2 - np.multiply(learning_rate/(np.sqrt(biases2_squares)+epsilon), err_2)\n",
    "        weights1 = weights1 - np.multiply(learning_rate/(np.sqrt(weights1_squares)+epsilon), grad_1)\n",
    "        weights2 = weights2 - np.multiply(learning_rate/(np.sqrt(weights2_squares)+epsilon), grad_2)\n",
    "        #Updating the accuracy and loss\n",
    "        #End of for loop\n",
    "#     print(\"accuracy : \", np.mean(acc_arr))\n",
    "    test_losses.append(test_loss(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_test.append(acc_out(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_train.append(acc_out(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    losses.append(test_loss(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot(211)\n",
    "plt.plot(losses[1:], label = 'train_loss')\n",
    "plt.plot(test_losses[1:], label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.multiply(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test, pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 101\n",
    "print(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)[:,5:10])\n",
    "print(y_test[:,5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
