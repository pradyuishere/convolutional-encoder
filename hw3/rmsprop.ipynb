{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = x\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def test_loss(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out2 = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "        loss = []\n",
    "        for iter in range(out2.shape[1]):\n",
    "            loss.append(MSE(out2[:, iter], y_in[:, iter]))\n",
    "        return np.mean(loss)\n",
    "\n",
    "def pred_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "        out1_temp = np.matmul(weights1.T, x_in[:,:])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        return out2\n",
    "\n",
    "def acc_out(x_in, y_in, weights1, weights2, biases1, biases2):\n",
    "    pred = pred_out(x_in, y_in, weights1, weights2, biases1, biases2)\n",
    "    sum_acc = 0\n",
    "    for iter in range(pred.shape[1]):\n",
    "#         print(iter)\n",
    "        sum_acc+=(np.argmax(pred[:, iter])==np.argmax(y_in[:, iter]))\n",
    "    return sum_acc/y_in.shape[1]\n",
    "\n",
    "def MSE(yHat, y):\n",
    "    return np.sum(np.multiply(yHat - y, yHat - y)) / y.size\n",
    "\n",
    "def cross_entropy(predictions, targets, epsilon=1e-5):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = - np.mean(np.multiply(np.log(predictions),targets)) \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 10)\n"
     ]
    }
   ],
   "source": [
    "##Load and segregate the data\n",
    "letters = genfromtxt('energy_eff2.csv', delimiter=',')\n",
    "# np.random.shuffle(letters)\n",
    "print(letters.shape)\n",
    "x_in = letters[:, 0:8]\n",
    "x_mean = np.mean(x_in, axis = 0)\n",
    "x_std = np.std(x_in, axis = 0)\n",
    "y_in = letters[:, 8:10]\n",
    "y_mean = np.mean(y_in, axis=0)\n",
    "y_std = np.std(y_in, axis = 0)\n",
    "# y_in = (y_in-y_mean)/y_std\n",
    "x_in = (x_in-x_mean)/x_std\n",
    "# print(np.std(y_in, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_in[0:int(x_in.shape[0]*0.7), :]\n",
    "y_train = y_in[0:int(x_in.shape[0]*0.7), :]\n",
    "\n",
    "x_test = x_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]\n",
    "y_test = y_in[int(x_in.shape[0]*0.7):x_in.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "learning_rate = 0.005\n",
    "num_epochs = 1000\n",
    "alpha = 0.9\n",
    "epsilon = 1e-12\n",
    "\n",
    "weights1 = np.random.normal(size=(x_in.shape[1], hidden_size))\n",
    "weights2 = np.random.normal(size=(hidden_size, y_in.shape[1]))\n",
    "print(weights1.shape)\n",
    "print(weights2.shape)\n",
    "biases1 = np.random.normal(size=(hidden_size, 1))\n",
    "biases2 = np.random.normal(size=(y_in.shape[1], 1))\n",
    "\n",
    "x_train = np.matrix(x_train)\n",
    "x_train = x_train.T\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = np.matrix(x_test)\n",
    "x_test = x_test.T\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  175.67174528892693\n",
      "train_loss :  173.75912370566425\n",
      "epoch_num :  1\n",
      "test_loss :  11.967110291156782\n",
      "train_loss :  9.95433418484508\n",
      "epoch_num :  6\n",
      "test_loss :  8.820672372858501\n",
      "train_loss :  7.824726104402361\n",
      "epoch_num :  11\n",
      "test_loss :  7.556668100408728\n",
      "train_loss :  6.771737905879429\n",
      "epoch_num :  16\n",
      "test_loss :  6.65722195967123\n",
      "train_loss :  5.917331167137911\n",
      "epoch_num :  21\n",
      "test_loss :  6.077287201866922\n",
      "train_loss :  5.2304814185441755\n",
      "epoch_num :  26\n",
      "test_loss :  5.305784265641386\n",
      "train_loss :  4.444463252141871\n",
      "epoch_num :  31\n",
      "test_loss :  4.640901138864503\n",
      "train_loss :  3.8228361421770787\n",
      "epoch_num :  36\n",
      "test_loss :  4.196877320033241\n",
      "train_loss :  3.373240466848007\n",
      "epoch_num :  41\n",
      "test_loss :  3.7961473274454978\n",
      "train_loss :  2.9993172513790944\n",
      "epoch_num :  46\n",
      "test_loss :  3.4829724995872415\n",
      "train_loss :  2.7233576827089916\n",
      "epoch_num :  51\n",
      "test_loss :  3.386336281129401\n",
      "train_loss :  2.565346383953335\n",
      "epoch_num :  56\n",
      "test_loss :  3.312155376267079\n",
      "train_loss :  2.457889887370681\n",
      "epoch_num :  61\n",
      "test_loss :  3.2185411481955493\n",
      "train_loss :  2.381304361707128\n",
      "epoch_num :  66\n",
      "test_loss :  3.147965453625666\n",
      "train_loss :  2.325082273913585\n",
      "epoch_num :  71\n",
      "test_loss :  3.0960403946654576\n",
      "train_loss :  2.279681706504013\n",
      "epoch_num :  76\n",
      "test_loss :  3.051112303323148\n",
      "train_loss :  2.2382475088348617\n",
      "epoch_num :  81\n",
      "test_loss :  3.008653793896915\n",
      "train_loss :  2.1976397278945172\n",
      "epoch_num :  86\n",
      "test_loss :  2.9713174045679898\n",
      "train_loss :  2.157827955582325\n",
      "epoch_num :  91\n",
      "test_loss :  2.9351655262242313\n",
      "train_loss :  2.117158345716816\n",
      "epoch_num :  96\n",
      "test_loss :  2.8954774005524273\n",
      "train_loss :  2.0701277949570693\n",
      "epoch_num :  101\n",
      "test_loss :  2.855029533927532\n",
      "train_loss :  2.0205818960835087\n",
      "epoch_num :  106\n",
      "test_loss :  2.8114398688586864\n",
      "train_loss :  1.9748119369211667\n",
      "epoch_num :  111\n",
      "test_loss :  2.767855329039961\n",
      "train_loss :  1.9334150874958431\n",
      "epoch_num :  116\n",
      "test_loss :  2.725374543062702\n",
      "train_loss :  1.9000444820959999\n",
      "epoch_num :  121\n",
      "test_loss :  2.714778702530813\n",
      "train_loss :  1.8751984743766834\n",
      "epoch_num :  126\n",
      "test_loss :  2.7116282900793087\n",
      "train_loss :  1.8469594927425563\n",
      "epoch_num :  131\n",
      "test_loss :  2.7118546058160384\n",
      "train_loss :  1.8241525120827389\n",
      "epoch_num :  136\n",
      "test_loss :  2.7139414954942382\n",
      "train_loss :  1.8078740145458074\n",
      "epoch_num :  141\n",
      "test_loss :  2.710638410624045\n",
      "train_loss :  1.7939648415230076\n",
      "epoch_num :  146\n",
      "test_loss :  2.7094837509750977\n",
      "train_loss :  1.7839104823593748\n",
      "epoch_num :  151\n",
      "test_loss :  2.7095004132550518\n",
      "train_loss :  1.775682079432503\n",
      "epoch_num :  156\n",
      "test_loss :  2.703524295923681\n",
      "train_loss :  1.7660677868336776\n",
      "epoch_num :  161\n",
      "test_loss :  2.693147884429519\n",
      "train_loss :  1.756385732098367\n",
      "epoch_num :  166\n",
      "test_loss :  2.6804353783465236\n",
      "train_loss :  1.7475669248412262\n",
      "epoch_num :  171\n",
      "test_loss :  2.663538669315042\n",
      "train_loss :  1.7388859475090408\n",
      "epoch_num :  176\n",
      "test_loss :  2.6475841968452896\n",
      "train_loss :  1.732398595930733\n",
      "epoch_num :  181\n",
      "test_loss :  2.6320315325876953\n",
      "train_loss :  1.7271465669975565\n",
      "epoch_num :  186\n",
      "test_loss :  2.6162772255618307\n",
      "train_loss :  1.7222737503087562\n",
      "epoch_num :  191\n",
      "test_loss :  2.6011922520296245\n",
      "train_loss :  1.717959092601452\n",
      "epoch_num :  196\n",
      "test_loss :  2.5902174306978254\n",
      "train_loss :  1.7154716756478545\n",
      "epoch_num :  201\n",
      "test_loss :  2.5848065227862573\n",
      "train_loss :  1.7150189532710565\n",
      "epoch_num :  206\n",
      "test_loss :  2.5836327058861053\n",
      "train_loss :  1.7158096248089145\n",
      "epoch_num :  211\n",
      "test_loss :  2.5837963686905834\n",
      "train_loss :  1.7168244771466779\n",
      "epoch_num :  216\n",
      "test_loss :  2.5813135224652024\n",
      "train_loss :  1.7169272520353753\n",
      "epoch_num :  221\n",
      "test_loss :  2.5743040227269285\n",
      "train_loss :  1.7156651930320397\n",
      "epoch_num :  226\n",
      "test_loss :  2.56414543111032\n",
      "train_loss :  1.714055510903442\n",
      "epoch_num :  231\n",
      "test_loss :  2.555432560415778\n",
      "train_loss :  1.7131851726634593\n",
      "epoch_num :  236\n",
      "test_loss :  2.54791102666416\n",
      "train_loss :  1.7117964077164964\n",
      "epoch_num :  241\n",
      "test_loss :  2.5407070976770076\n",
      "train_loss :  1.710057111943212\n",
      "epoch_num :  246\n",
      "test_loss :  2.536181086651626\n",
      "train_loss :  1.7085326327663983\n",
      "epoch_num :  251\n",
      "test_loss :  2.534191353390959\n",
      "train_loss :  1.7073011707087966\n",
      "epoch_num :  256\n",
      "test_loss :  2.532080688937821\n",
      "train_loss :  1.7059448711190572\n",
      "epoch_num :  261\n",
      "test_loss :  2.5306059079684657\n",
      "train_loss :  1.70497423849879\n",
      "epoch_num :  266\n",
      "test_loss :  2.530279631497154\n",
      "train_loss :  1.7042753270968964\n",
      "epoch_num :  271\n",
      "test_loss :  2.530690106761968\n",
      "train_loss :  1.7035180454894243\n",
      "epoch_num :  276\n",
      "test_loss :  2.5312682400575763\n",
      "train_loss :  1.7025286181909784\n",
      "epoch_num :  281\n",
      "test_loss :  2.5318058104069094\n",
      "train_loss :  1.7013519061650937\n",
      "epoch_num :  286\n",
      "test_loss :  2.532217612433422\n",
      "train_loss :  1.7000678652878856\n",
      "epoch_num :  291\n",
      "test_loss :  2.532308679524534\n",
      "train_loss :  1.6986844051787993\n",
      "epoch_num :  296\n",
      "test_loss :  2.5318984754343896\n",
      "train_loss :  1.6971852646438361\n",
      "epoch_num :  301\n",
      "test_loss :  2.5309017557048428\n",
      "train_loss :  1.695574067698778\n",
      "epoch_num :  306\n",
      "test_loss :  2.529224489717025\n",
      "train_loss :  1.6938558674207262\n",
      "epoch_num :  311\n",
      "test_loss :  2.526762458928587\n",
      "train_loss :  1.6919850392319271\n",
      "epoch_num :  316\n",
      "test_loss :  2.523561059346429\n",
      "train_loss :  1.6898754667326354\n",
      "epoch_num :  321\n",
      "test_loss :  2.5196435488545283\n",
      "train_loss :  1.6874415542725372\n",
      "epoch_num :  326\n",
      "test_loss :  2.514893122878894\n",
      "train_loss :  1.6846030828243452\n",
      "epoch_num :  331\n",
      "test_loss :  2.509213000489313\n",
      "train_loss :  1.6813190390630026\n",
      "epoch_num :  336\n",
      "test_loss :  2.5026605084564966\n",
      "train_loss :  1.6776010547374816\n",
      "epoch_num :  341\n",
      "test_loss :  2.495422045596921\n",
      "train_loss :  1.673508444790215\n",
      "epoch_num :  346\n",
      "test_loss :  2.487587789773425\n",
      "train_loss :  1.6690808174282747\n",
      "epoch_num :  351\n",
      "test_loss :  2.4789755690913724\n",
      "train_loss :  1.6642529150580967\n",
      "epoch_num :  356\n",
      "test_loss :  2.469259947409195\n",
      "train_loss :  1.6589136448115693\n",
      "epoch_num :  361\n",
      "test_loss :  2.4582004233048425\n",
      "train_loss :  1.652984844053082\n",
      "epoch_num :  366\n",
      "test_loss :  2.4458536700869664\n",
      "train_loss :  1.6464816741173207\n",
      "epoch_num :  371\n",
      "test_loss :  2.432826294441785\n",
      "train_loss :  1.6395880583958504\n",
      "epoch_num :  376\n",
      "test_loss :  2.420488825227896\n",
      "train_loss :  1.6327206718152907\n",
      "epoch_num :  381\n",
      "test_loss :  2.410059093260662\n",
      "train_loss :  1.6262489027540001\n",
      "epoch_num :  386\n",
      "test_loss :  2.401723368803784\n",
      "train_loss :  1.6202148063518693\n",
      "epoch_num :  391\n",
      "test_loss :  2.395011858829409\n",
      "train_loss :  1.6144653773544035\n",
      "epoch_num :  396\n",
      "test_loss :  2.3893248890609087\n",
      "train_loss :  1.6088288431481672\n",
      "epoch_num :  401\n",
      "test_loss :  2.384204872031269\n",
      "train_loss :  1.6031937930573088\n",
      "epoch_num :  406\n",
      "test_loss :  2.3794292959337278\n",
      "train_loss :  1.5975298300329739\n",
      "epoch_num :  411\n",
      "test_loss :  2.3749809077832897\n",
      "train_loss :  1.5918730046188672\n",
      "epoch_num :  416\n",
      "test_loss :  2.3709525885007574\n",
      "train_loss :  1.5862955771859528\n",
      "epoch_num :  421\n",
      "test_loss :  2.367444894820901\n",
      "train_loss :  1.5808750922770567\n",
      "epoch_num :  426\n",
      "test_loss :  2.364502738209266\n",
      "train_loss :  1.5756744598730452\n",
      "epoch_num :  431\n",
      "test_loss :  2.362104848859223\n",
      "train_loss :  1.5707366555239668\n",
      "epoch_num :  436\n",
      "test_loss :  2.360186658015431\n",
      "train_loss :  1.5660884342236348\n",
      "epoch_num :  441\n",
      "test_loss :  2.3586672964091058\n",
      "train_loss :  1.5617449102847338\n",
      "epoch_num :  446\n",
      "test_loss :  2.3574632003579783\n",
      "train_loss :  1.557710711416671\n",
      "epoch_num :  451\n",
      "test_loss :  2.356487435296137\n",
      "train_loss :  1.5539777016576757\n",
      "epoch_num :  456\n",
      "test_loss :  2.355643833501799\n",
      "train_loss :  1.550521178860174\n",
      "epoch_num :  461\n",
      "test_loss :  2.3548263872731168\n",
      "train_loss :  1.547296984292956\n",
      "epoch_num :  466\n",
      "test_loss :  2.353929123897064\n",
      "train_loss :  1.544241910182636\n",
      "epoch_num :  471\n",
      "test_loss :  2.352863997833515\n",
      "train_loss :  1.5412792948332992\n",
      "epoch_num :  476\n",
      "test_loss :  2.351576502009901\n",
      "train_loss :  1.5383322816043579\n",
      "epoch_num :  481\n",
      "test_loss :  2.350048681046979\n",
      "train_loss :  1.535347069556427\n",
      "epoch_num :  486\n",
      "test_loss :  2.348292669614469\n",
      "train_loss :  1.5323130766115098\n",
      "epoch_num :  491\n",
      "test_loss :  2.346327923360705\n",
      "train_loss :  1.5292588761648458\n",
      "epoch_num :  496\n",
      "test_loss :  2.34413813993845\n",
      "train_loss :  1.5262391895243927\n",
      "epoch_num :  501\n",
      "test_loss :  2.3416295746633256\n",
      "train_loss :  1.523341463059424\n",
      "epoch_num :  506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  2.3386504482314185\n",
      "train_loss :  1.5207186257835468\n",
      "epoch_num :  511\n",
      "test_loss :  2.335251260510737\n",
      "train_loss :  1.5184687903017016\n",
      "epoch_num :  516\n",
      "test_loss :  2.331815897875849\n",
      "train_loss :  1.5165381501059598\n",
      "epoch_num :  521\n",
      "test_loss :  2.3285892610968415\n",
      "train_loss :  1.5149095428709658\n",
      "epoch_num :  526\n",
      "test_loss :  2.3253609266712165\n",
      "train_loss :  1.513221522374636\n",
      "epoch_num :  531\n",
      "test_loss :  2.3205428522645497\n",
      "train_loss :  1.5103534875903444\n",
      "epoch_num :  536\n",
      "test_loss :  2.3169244116758456\n",
      "train_loss :  1.508185399525571\n",
      "epoch_num :  541\n",
      "test_loss :  2.3141275636172196\n",
      "train_loss :  1.5073599558590187\n",
      "epoch_num :  546\n",
      "test_loss :  2.3101979884206916\n",
      "train_loss :  1.504393797209343\n",
      "epoch_num :  551\n",
      "test_loss :  2.308378715090237\n",
      "train_loss :  1.50430009479707\n",
      "epoch_num :  556\n",
      "test_loss :  2.304150752954831\n",
      "train_loss :  1.5012628894996953\n",
      "epoch_num :  561\n",
      "test_loss :  2.3016755588907434\n",
      "train_loss :  1.5008149739177734\n",
      "epoch_num :  566\n",
      "test_loss :  2.2968095748079667\n",
      "train_loss :  1.4979712866326338\n",
      "epoch_num :  571\n",
      "test_loss :  2.2936275246020355\n",
      "train_loss :  1.4972967766753238\n",
      "epoch_num :  576\n",
      "test_loss :  2.2883646262668664\n",
      "train_loss :  1.4947457211158888\n",
      "epoch_num :  581\n",
      "test_loss :  2.284686626161933\n",
      "train_loss :  1.4939724413253834\n",
      "epoch_num :  586\n",
      "test_loss :  2.2792084100307384\n",
      "train_loss :  1.4917254551067152\n",
      "epoch_num :  591\n",
      "test_loss :  2.2754913243900803\n",
      "train_loss :  1.4911608331419202\n",
      "epoch_num :  596\n",
      "test_loss :  2.2695993005972617\n",
      "train_loss :  1.489032762527654\n",
      "epoch_num :  601\n",
      "test_loss :  2.2643950274585536\n",
      "train_loss :  1.4875375658733474\n",
      "epoch_num :  606\n",
      "test_loss :  2.259645450492944\n",
      "train_loss :  1.486559394180377\n",
      "epoch_num :  611\n",
      "test_loss :  2.254626828425197\n",
      "train_loss :  1.485462220157255\n",
      "epoch_num :  616\n",
      "test_loss :  2.2498509781424363\n",
      "train_loss :  1.4845776678637725\n",
      "epoch_num :  621\n",
      "test_loss :  2.245587742174604\n",
      "train_loss :  1.4839808892318274\n",
      "epoch_num :  626\n",
      "test_loss :  2.242628353128758\n",
      "train_loss :  1.4839415140495433\n",
      "epoch_num :  631\n",
      "test_loss :  2.242554351661621\n",
      "train_loss :  1.4853959462971909\n",
      "epoch_num :  636\n",
      "test_loss :  2.2405995992579797\n",
      "train_loss :  1.48566454279037\n",
      "epoch_num :  641\n",
      "test_loss :  2.2303230907969165\n",
      "train_loss :  1.4818244046131326\n",
      "epoch_num :  646\n",
      "test_loss :  2.2240062532538167\n",
      "train_loss :  1.4800046018693929\n",
      "epoch_num :  651\n",
      "test_loss :  2.2155604644849443\n",
      "train_loss :  1.476688861852778\n",
      "epoch_num :  656\n",
      "test_loss :  2.210138828159355\n",
      "train_loss :  1.4746990204868005\n",
      "epoch_num :  661\n",
      "test_loss :  2.205133870793099\n",
      "train_loss :  1.4725955350973072\n",
      "epoch_num :  666\n",
      "test_loss :  2.2015693495414452\n",
      "train_loss :  1.471053122730268\n",
      "epoch_num :  671\n",
      "test_loss :  2.1988234294279945\n",
      "train_loss :  1.4697881996317923\n",
      "epoch_num :  676\n",
      "test_loss :  2.1966788972722933\n",
      "train_loss :  1.468757260303177\n",
      "epoch_num :  681\n",
      "test_loss :  2.1948979734650473\n",
      "train_loss :  1.4678694538999593\n",
      "epoch_num :  686\n",
      "test_loss :  2.193369564764153\n",
      "train_loss :  1.4670744099175206\n",
      "epoch_num :  691\n",
      "test_loss :  2.1920749227337226\n",
      "train_loss :  1.4663588162264334\n",
      "epoch_num :  696\n",
      "test_loss :  2.1910443264082153\n",
      "train_loss :  1.4657380371346065\n",
      "epoch_num :  701\n",
      "test_loss :  2.1903200274844385\n",
      "train_loss :  1.465246180679377\n",
      "epoch_num :  706\n",
      "test_loss :  2.1899262550476615\n",
      "train_loss :  1.4649214636130985\n",
      "epoch_num :  711\n",
      "test_loss :  2.1898468251286527\n",
      "train_loss :  1.464779819495251\n",
      "epoch_num :  716\n",
      "test_loss :  2.1900236550371517\n",
      "train_loss :  1.464795932638842\n",
      "epoch_num :  721\n",
      "test_loss :  2.1903814575429226\n",
      "train_loss :  1.4649253806807692\n",
      "epoch_num :  726\n",
      "test_loss :  2.1908414495600637\n",
      "train_loss :  1.4651281415503357\n",
      "epoch_num :  731\n",
      "test_loss :  2.1913229553440505\n",
      "train_loss :  1.4653474729438798\n",
      "epoch_num :  736\n",
      "test_loss :  2.191779429831552\n",
      "train_loss :  1.4655414725920861\n",
      "epoch_num :  741\n",
      "test_loss :  2.1921994350112906\n",
      "train_loss :  1.4657069804823248\n",
      "epoch_num :  746\n",
      "test_loss :  2.1925839609181086\n",
      "train_loss :  1.4658704534384828\n",
      "epoch_num :  751\n",
      "test_loss :  2.1929574385429254\n",
      "train_loss :  1.4660982017389081\n",
      "epoch_num :  756\n",
      "test_loss :  2.193246768433046\n",
      "train_loss :  1.4664029984289366\n",
      "epoch_num :  761\n",
      "test_loss :  2.193175332238507\n",
      "train_loss :  1.466485569064227\n",
      "epoch_num :  766\n",
      "test_loss :  2.192845045905445\n",
      "train_loss :  1.466280250381996\n",
      "epoch_num :  771\n",
      "test_loss :  2.1925026849344613\n",
      "train_loss :  1.4659252020843951\n",
      "epoch_num :  776\n",
      "test_loss :  2.1923202670854414\n",
      "train_loss :  1.4655275038215532\n",
      "epoch_num :  781\n",
      "test_loss :  2.192408071300333\n",
      "train_loss :  1.4651930109448128\n",
      "epoch_num :  786\n",
      "test_loss :  2.1927290356489237\n",
      "train_loss :  1.4650505674939212\n",
      "epoch_num :  791\n",
      "test_loss :  2.1937479866325713\n",
      "train_loss :  1.4653068390618575\n",
      "epoch_num :  796\n",
      "test_loss :  2.195477807855891\n",
      "train_loss :  1.4656905570516643\n",
      "epoch_num :  801\n",
      "test_loss :  2.197620322380808\n",
      "train_loss :  1.4663095360771392\n",
      "epoch_num :  806\n",
      "test_loss :  2.199338639109187\n",
      "train_loss :  1.4667773998117455\n",
      "epoch_num :  811\n",
      "test_loss :  2.2004289412177487\n",
      "train_loss :  1.4667080034803643\n",
      "epoch_num :  816\n",
      "test_loss :  2.203286875755325\n",
      "train_loss :  1.4676704970630514\n",
      "epoch_num :  821\n",
      "test_loss :  2.205720690695729\n",
      "train_loss :  1.4681599194794046\n",
      "epoch_num :  826\n",
      "test_loss :  2.210250185905894\n",
      "train_loss :  1.4699672316503163\n",
      "epoch_num :  831\n",
      "test_loss :  2.2147811468863297\n",
      "train_loss :  1.4718568938863061\n",
      "epoch_num :  836\n",
      "test_loss :  2.219891634879783\n",
      "train_loss :  1.4738732638129228\n",
      "epoch_num :  841\n",
      "test_loss :  2.2239535531756327\n",
      "train_loss :  1.474804840474482\n",
      "epoch_num :  846\n",
      "test_loss :  2.228014084063288\n",
      "train_loss :  1.4755864159246752\n",
      "epoch_num :  851\n",
      "test_loss :  2.2320594900043926\n",
      "train_loss :  1.4766405661697128\n",
      "epoch_num :  856\n",
      "test_loss :  2.233819982776864\n",
      "train_loss :  1.4766811987748332\n",
      "epoch_num :  861\n",
      "test_loss :  2.2372150606168986\n",
      "train_loss :  1.477389772061859\n",
      "epoch_num :  866\n",
      "test_loss :  2.2419175960693116\n",
      "train_loss :  1.478620072301873\n",
      "epoch_num :  871\n",
      "test_loss :  2.246818371022853\n",
      "train_loss :  1.4799904638141546\n",
      "epoch_num :  876\n",
      "test_loss :  2.2505092390729406\n",
      "train_loss :  1.4810815157296227\n",
      "epoch_num :  881\n",
      "test_loss :  2.2518738591823637\n",
      "train_loss :  1.48170572972642\n",
      "epoch_num :  886\n",
      "test_loss :  2.2522213283294548\n",
      "train_loss :  1.482651919999781\n",
      "epoch_num :  891\n",
      "test_loss :  2.24775938126706\n",
      "train_loss :  1.4819773994083938\n",
      "epoch_num :  896\n",
      "test_loss :  2.2428897486653216\n",
      "train_loss :  1.4822235748182304\n",
      "epoch_num :  901\n",
      "test_loss :  2.239342541489851\n",
      "train_loss :  1.482954612219485\n",
      "epoch_num :  906\n",
      "test_loss :  2.2356996951282\n",
      "train_loss :  1.4826472489980937\n",
      "epoch_num :  911\n",
      "test_loss :  2.2316863705145638\n",
      "train_loss :  1.4824550001983807\n",
      "epoch_num :  916\n",
      "test_loss :  2.230428333259414\n",
      "train_loss :  1.483208205383285\n",
      "epoch_num :  921\n",
      "test_loss :  2.2293781140792457\n",
      "train_loss :  1.4837390146546028\n",
      "epoch_num :  926\n",
      "test_loss :  2.227484537623864\n",
      "train_loss :  1.4836100318907923\n",
      "epoch_num :  931\n",
      "test_loss :  2.226822648882466\n",
      "train_loss :  1.4849928914042392\n",
      "epoch_num :  936\n",
      "test_loss :  2.224341974830908\n",
      "train_loss :  1.4851688722948255\n",
      "epoch_num :  941\n",
      "test_loss :  2.221695947689871\n",
      "train_loss :  1.4853601442623368\n",
      "epoch_num :  946\n",
      "test_loss :  2.219957381596356\n",
      "train_loss :  1.4860677443372554\n",
      "epoch_num :  951\n",
      "test_loss :  2.2177136860104114\n",
      "train_loss :  1.48642956604053\n",
      "epoch_num :  956\n",
      "test_loss :  2.2157809336182126\n",
      "train_loss :  1.486916851979671\n",
      "epoch_num :  961\n",
      "test_loss :  2.2141951382223897\n",
      "train_loss :  1.4876353175653891\n",
      "epoch_num :  966\n",
      "test_loss :  2.2131923465212777\n",
      "train_loss :  1.4888020581128347\n",
      "epoch_num :  971\n",
      "test_loss :  2.2131977897778565\n",
      "train_loss :  1.4906933000826017\n",
      "epoch_num :  976\n",
      "test_loss :  2.2146373506997965\n",
      "train_loss :  1.4934886960395297\n",
      "epoch_num :  981\n",
      "test_loss :  2.217584096381848\n",
      "train_loss :  1.4970062686970609\n",
      "epoch_num :  986\n",
      "test_loss :  2.221487609287743\n",
      "train_loss :  1.5006038958435206\n",
      "epoch_num :  991\n",
      "test_loss :  2.225541953107068\n",
      "train_loss :  1.5036292643558682\n",
      "epoch_num :  996\n"
     ]
    }
   ],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "biases1_squares = 0\n",
    "biases2_squares = 0\n",
    "weights1_squares = 0\n",
    "weights2_squares = 0\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "    if epoch_num%5==1:\n",
    "        print(\"test_loss : \", test_losses[-1])\n",
    "        print(\"train_loss : \", losses[-1])\n",
    "        print(\"epoch_num : \", epoch_num)\n",
    "    for sample_num in range(x_train.shape[1]):\n",
    "#         print(\"sample_num : \", sample_num)\n",
    "        #Forward propagation\n",
    "        out1_temp = np.matmul(weights1.T, x_train[:,sample_num])+biases1\n",
    "        out1 = sigmoid(out1_temp)\n",
    "        out2_temp = np.matmul(weights2.T, out1)+biases2\n",
    "        out2 = (out2_temp)\n",
    "        #Back-propogation\n",
    "        error_out = out2-y_train[:, sample_num]\n",
    "        err_2 = np.matrix(error_out)\n",
    "        grad_2 = np.matmul(out1, err_2.T)\n",
    "        err_1 = np.multiply(np.matmul(weights2, err_2), np.multiply(out1, np.ones(out1.shape)-out1))\n",
    "        grad_1 = np.multiply(x_train[:, sample_num], err_1.T)\n",
    "        #Updating the weights\n",
    "        \n",
    "        biases1_squares = alpha*biases1_squares+(1-alpha)*np.multiply(err_1, err_1)\n",
    "        biases2_squares = alpha*biases2_squares+(1-alpha)*np.multiply(err_2, err_2)\n",
    "        weights1_squares = alpha*weights1_squares+(1-alpha)*np.multiply(grad_1, grad_1)\n",
    "        weights2_squares = alpha*weights2_squares+(1-alpha)*np.multiply(grad_2, grad_2)\n",
    "        \n",
    "        biases1 = biases1 - np.multiply(learning_rate/(np.sqrt(biases1_squares)+epsilon), err_1)\n",
    "        biases2 = biases2 - np.multiply(learning_rate/(np.sqrt(biases2_squares)+epsilon), err_2)\n",
    "        weights1 = weights1 - np.multiply(learning_rate/(np.sqrt(weights1_squares)+epsilon), grad_1)\n",
    "        weights2 = weights2 - np.multiply(learning_rate/(np.sqrt(weights2_squares)+epsilon), grad_2)\n",
    "        #Updating the accuracy and loss\n",
    "        #End of for loop\n",
    "#     print(\"accuracy : \", np.mean(acc_arr))\n",
    "    test_losses.append(test_loss(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_test.append(acc_out(x_test, y_test, weights1, weights2, biases1, biases2))\n",
    "#     accuracies_train.append(acc_out(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    losses.append(test_loss(x_train, y_train, weights1, weights2, biases1, biases2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.subplot(211)\n",
    "plt.plot(losses[1:], label = 'train_loss')\n",
    "plt.plot(test_losses[1:], label = 'test_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.multiply(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test, pred_out(x_test, y_test, weights1, weights2, biases1, biases2)-y_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.62460829 20.09772482 39.69617401 19.06377592 24.56413151]\n",
      " [14.03875298 26.66509027 41.73016864 22.08024681 27.26021202]]\n",
      "[[11.7  19.95 38.57 19.36 24.29]\n",
      " [13.88 21.97 43.86 22.73 25.95]]\n"
     ]
    }
   ],
   "source": [
    "ind = 101\n",
    "print(pred_out(x_test, y_test, weights1, weights2, biases1, biases2)[:,5:10])\n",
    "print(y_test[:,5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5054608873599062\n"
     ]
    }
   ],
   "source": [
    "print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
